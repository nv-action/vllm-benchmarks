# Chinese translations for PROJECT.
# Copyright (C) 2025 ORGANIZATION
# This file is distributed under the same license as the PROJECT project.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
#
msgid ""
msgstr ""
"Project-Id-Version: PROJECT VERSION\n"
"Report-Msgid-Bugs-To: EMAIL@ADDRESS\n"
"POT-Creation-Date: 2026-02-11 03:12+0000\n"
"PO-Revision-Date: 2025-07-18 10:11+0800\n"
"Last-Translator: \n"
"Language: zh\n"
"Language-Team: \n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.18.0\n"

#: ../../source/user_guide/release_notes.md:1
msgid "Release Notes"
msgstr "版本说明"

#: ../../source/user_guide/release_notes.md:3
msgid "v0.14.0rc1 - 2026.01.26"
msgstr "v0.14.0rc1 - 2026.01.26"

#: ../../source/user_guide/release_notes.md:5
msgid ""
"This is the first release candidate of v0.14.0 for vLLM Ascend. Please "
"follow the [official doc](https://docs.vllm.ai/projects/ascend/en/latest)"
" to get started. This release includes all the changes in v0.13.0rc2. So "
"We just list the differences from v0.13.0rc2. If you are upgrading from "
"v0.13.0rc1, please read both v0.14.0rc1 and v0.13.0rc2 release notes."
msgstr ""
"这是 vLLM Ascend v0.14.0 的第一个候选版本。请按照[官方文档](https://docs.vllm.ai/projects/ascend/en/latest)开始使用。此版本包含了 v0.13.0rc2 中的所有更改。因此我们仅列出与 v0.13.0rc2 的差异。如果您是从 v0.13.0rc1 升级，请同时阅读 v0.14.0rc1 和 v0.13.0rc2 的版本说明。"

#: ../../source/user_guide/release_notes.md:7
#: ../../source/user_guide/release_notes.md:56
#: ../../source/user_guide/release_notes.md:111
#: ../../source/user_guide/release_notes.md:168
#: ../../source/user_guide/release_notes.md:206
#: ../../source/user_guide/release_notes.md:257
#: ../../source/user_guide/release_notes.md:276
#: ../../source/user_guide/release_notes.md:310
#: ../../source/user_guide/release_notes.md:345
#: ../../source/user_guide/release_notes.md:369
#: ../../source/user_guide/release_notes.md:412
#: ../../source/user_guide/release_notes.md:468
#: ../../source/user_guide/release_notes.md:542
#: ../../source/user_guide/release_notes.md:601
#: ../../source/user_guide/release_notes.md:713
#: ../../source/user_guide/release_notes.md:817
#: ../../source/user_guide/release_notes.md:825
#: ../../source/user_guide/release_notes.md:863
#: ../../source/user_guide/release_notes.md:887
#: ../../source/user_guide/release_notes.md:914
#: ../../source/user_guide/release_notes.md:940
#: ../../source/user_guide/release_notes.md:963
#: ../../source/user_guide/release_notes.md:989
#: ../../source/user_guide/release_notes.md:1015
#: ../../source/user_guide/release_notes.md:1052
msgid "Highlights"
msgstr "亮点"

#: ../../source/user_guide/release_notes.md:9
msgid ""
"310P support is back now. In this release, only basic dense and vl models"
" are supported with eager mode. We'll keep improving and maintaining the "
"support for 310P. [#5776](https://github.com/vllm-project/vllm-"
"ascend/pull/5776)"
msgstr ""
"现已恢复对 310P 的支持。在此版本中，仅支持基础密集模型和视觉语言模型，并使用 Eager 模式。我们将持续改进和维护对 310P 的支持。[#5776](https://github.com/vllm-project/vllm-ascend/pull/5776)"

#: ../../source/user_guide/release_notes.md:10
msgid ""
"Support compressed tensors moe w8a8-int8 quantization. "
"[#5718](https://github.com/vllm-project/vllm-ascend/pull/5718)"
msgstr ""
"支持压缩张量的 MoE w8a8-int8 量化。[#5718](https://github.com/vllm-project/vllm-ascend/pull/5718)"

#: ../../source/user_guide/release_notes.md:11
msgid ""
"Support Medusa speculative decoding. [#5668](https://github.com/vllm-"
"project/vllm-ascend/pull/5668)"
msgstr ""
"支持 Medusa 推测式解码。[#5668](https://github.com/vllm-project/vllm-ascend/pull/5668)"

#: ../../source/user_guide/release_notes.md:12
msgid ""
"Support Eagle3 speculative decoding for Qwen3vl. "
"[#4848](https://github.com/vllm-project/vllm-ascend/pull/4848)"
msgstr ""
"支持 Qwen3vl 的 Eagle3 推测式解码。[#4848](https://github.com/vllm-project/vllm-ascend/pull/4848)"

#: ../../source/user_guide/release_notes.md:14
#: ../../source/user_guide/release_notes.md:60
#: ../../source/user_guide/release_notes.md:117
msgid "Features"
msgstr "新功能"

#: ../../source/user_guide/release_notes.md:16
msgid ""
"Xlite Backend supports Qwen3 MoE now. [#5951](https://github.com/vllm-"
"project/vllm-ascend/pull/5951)"
msgstr ""
"Xlite 后端现已支持 Qwen3 MoE 模型。[#5951](https://github.com/vllm-project/vllm-ascend/pull/5951)"

#: ../../source/user_guide/release_notes.md:17
msgid ""
"Support DSA-CP for PD-mix deployment case. [#5702](https://github.com"
"/vllm-project/vllm-ascend/pull/5702)"
msgstr ""
"支持 PD-mix 部署场景下的 DSA-CP。[#5702](https://github.com/vllm-project/vllm-ascend/pull/5702)"

#: ../../source/user_guide/release_notes.md:18
msgid ""
"Add support of new W4A4_LAOS_DYNAMIC quantization method. "
"[#5143](https://github.com/vllm-project/vllm-ascend/pull/5143)"
msgstr ""
"新增对 W4A4_LAOS_DYNAMIC 量化方法的支持。[#5143](https://github.com/vllm-project/vllm-ascend/pull/5143)"

#: ../../source/user_guide/release_notes.md:20
#: ../../source/user_guide/release_notes.md:75
#: ../../source/user_guide/release_notes.md:127
msgid "Performance"
msgstr "性能"

#: ../../source/user_guide/release_notes.md:22
msgid ""
"The performance of Qwen3-next has been improved. "
"[#5664](https://github.com/vllm-project/vllm-ascend/pull/5664) "
"[#5984](https://github.com/vllm-project/vllm-ascend/pull/5984) "
"[#5765](https://github.com/vllm-project/vllm-ascend/pull/5765)"
msgstr ""
"Qwen3-next 的性能已得到提升。[#5664](https://github.com/vllm-project/vllm-ascend/pull/5664) [#5984](https://github.com/vllm-project/vllm-ascend/pull/5984) [#5765](https://github.com/vllm-project/vllm-ascend/pull/5765)"

#: ../../source/user_guide/release_notes.md:23
msgid ""
"The CPU bind logic and performance has been improved. "
"[#5555](https://github.com/vllm-project/vllm-ascend/pull/5555)"
msgstr ""
"CPU 绑定逻辑和性能已得到改进。[#5555](https://github.com/vllm-project/vllm-ascend/pull/5555)"

#: ../../source/user_guide/release_notes.md:24
msgid ""
"Merge Q/K split to simplify AscendApplyRotaryEmb for better performance. "
"[#5799](https://github.com/vllm-project/vllm-ascend/pull/5799)"
msgstr ""
"合并 Q/K 拆分以简化 AscendApplyRotaryEmb，从而获得更好的性能。[#5799](https://github.com/vllm-project/vllm-ascend/pull/5799)"

#: ../../source/user_guide/release_notes.md:25
msgid ""
"Add Matmul Allreduce Rmsnorm fusion Pass. It's disabled by default. Set "
"`fuse_allreduce_rms=True` in `--additional_config` to enable it. "
"[#5034](https://github.com/vllm-project/vllm-ascend/pull/5034)"
msgstr ""
"新增 Matmul Allreduce Rmsnorm 融合 Pass。默认禁用。在 `--additional_config` 中设置 `fuse_allreduce_rms=True` 以启用它。[#5034](https://github.com/vllm-project/vllm-ascend/pull/5034)"

#: ../../source/user_guide/release_notes.md:26
msgid ""
"Optimize rope embedding with triton kernel for huge performance gain. "
"[#5918](https://github.com/vllm-project/vllm-ascend/pull/5918)"
msgstr ""
"使用 Triton 内核优化 rope embedding，以获得巨大的性能提升。[#5918](https://github.com/vllm-project/vllm-ascend/pull/5918)"

#: ../../source/user_guide/release_notes.md:27
msgid ""
"support advanced apply_top_k_top_p without top_k constraint. "
"[#6098](https://github.com/vllm-project/vllm-ascend/pull/6098)"
msgstr ""
"支持无 top_k 约束的高级 apply_top_k_top_p。[#6098](https://github.com/vllm-project/vllm-ascend/pull/6098)"

#: ../../source/user_guide/release_notes.md:28
msgid ""
"Parallelize Q/K/V padding in AscendMMEncoderAttention for better "
"performance. [#6204](https://github.com/vllm-project/vllm-"
"ascend/pull/6204)"
msgstr ""
"在 AscendMMEncoderAttention 中并行化 Q/K/V 填充，以获得更好的性能。[#6204](https://github.com/vllm-project/vllm-ascend/pull/6204)"

#: ../../source/user_guide/release_notes.md:30
#: ../../source/user_guide/release_notes.md:80
#: ../../source/user_guide/release_notes.md:358
#: ../../source/user_guide/release_notes.md:382
#: ../../source/user_guide/release_notes.md:426
#: ../../source/user_guide/release_notes.md:557
#: ../../source/user_guide/release_notes.md:727
#: ../../source/user_guide/release_notes.md:846
#: ../../source/user_guide/release_notes.md:903
#: ../../source/user_guide/release_notes.md:930
#: ../../source/user_guide/release_notes.md:951
#: ../../source/user_guide/release_notes.md:976
#: ../../source/user_guide/release_notes.md:1003
#: ../../source/user_guide/release_notes.md:1031
#: ../../source/user_guide/release_notes.md:1063
msgid "Others"
msgstr "其他"

#: ../../source/user_guide/release_notes.md:32
msgid ""
"model runner v2 support triton of penalty. [#5854](https://github.com"
"/vllm-project/vllm-ascend/pull/5854)"
msgstr ""
"model runner v2 支持 Triton 惩罚项。[#5854](https://github.com/vllm-project/vllm-ascend/pull/5854)"

#: ../../source/user_guide/release_notes.md:33
msgid ""
"model runner v2 support eagle spec decoding. [#5840](https://github.com"
"/vllm-project/vllm-ascend/pull/5840)"
msgstr ""
"model runner v2 支持 eagle 推测式解码。[#5840](https://github.com/vllm-project/vllm-ascend/pull/5840)"

#: ../../source/user_guide/release_notes.md:34
msgid ""
"Fix multi-modal inference OOM issues by setting "
"`expandable_segments:True` by default. [#5855](https://github.com/vllm-"
"project/vllm-ascend/pull/5855)"
msgstr ""
"通过默认设置 `expandable_segments:True` 修复多模态推理 OOM 问题。[#5855](https://github.com/vllm-project/vllm-ascend/pull/5855)"

#: ../../source/user_guide/release_notes.md:35
msgid ""
"`VLLM_ASCEND_ENABLE_MLAPO` is set to `True` by default. It's enabled "
"automatically on decode node in PD deployment case. Please note that this"
" feature will cost more memory. If you are memory sensitive, please set "
"it to False. [#5952](https://github.com/vllm-project/vllm-"
"ascend/pull/5952)"
msgstr ""
"`VLLM_ASCEND_ENABLE_MLAPO` 默认设置为 `True`。在 PD 部署场景下，解码节点会自动启用此功能。请注意，此功能会消耗更多内存。如果您对内存敏感，请将其设置为 False。[#5952](https://github.com/vllm-project/vllm-ascend/pull/5952)"

#: ../../source/user_guide/release_notes.md:36
msgid ""
"SSL config can be set to kv_extra_config for PD deployment with mooncake "
"layerwise connector. [#5875](https://github.com/vllm-project/vllm-"
"ascend/pull/5875)"
msgstr ""
"对于使用 mooncake 分层连接器的 PD 部署，SSL 配置可以设置在 kv_extra_config 中。[#5875](https://github.com/vllm-project/vllm-ascend/pull/5875)"

#: ../../source/user_guide/release_notes.md:37
msgid ""
"support `--max_model_len=auto`. [#6193](https://github.com/vllm-project"
"/vllm-ascend/pull/6193)"
msgstr ""
"支持 `--max_model_len=auto`。[#6193](https://github.com/vllm-project/vllm-ascend/pull/6193)"

#: ../../source/user_guide/release_notes.md:39
#: ../../source/user_guide/release_notes.md:94
#: ../../source/user_guide/release_notes.md:153
msgid "Dependencies"
msgstr "依赖项"

#: ../../source/user_guide/release_notes.md:41
msgid ""
"torch-npu is upgraded to 2.9.0 [#6112](https://github.com/vllm-project"
"/vllm-ascend/pull/6112)"
msgstr ""
"torch-npu 已升级至 2.9.0 [#6112](https://github.com/vllm-project/vllm-ascend/pull/6112)"

#: ../../source/user_guide/release_notes.md:43
#: ../../source/user_guide/release_notes.md:100
#: ../../source/user_guide/release_notes.md:145
msgid "Deprecation & Breaking Changes"
msgstr "弃用与破坏性变更"

#: ../../source/user_guide/release_notes.md:45
msgid ""
"EPLB config options is moved to `eplb_config` in [additional "
"config](https://docs.vllm.ai/projects/ascend/en/latest/user_guide/configuration/additional_config.html)."
" The old ones are removed in this release."
msgstr ""
"EPLB 配置选项已移至 [额外配置](https://docs.vllm.ai/projects/ascend/en/latest/user_guide/configuration/additional_config.html) 中的 `eplb_config`。旧选项已在此版本中移除。"

#: ../../source/user_guide/release_notes.md:46
msgid ""
"The profiler envs, such as `VLLM_TORCH_PROFILER_DIR` and "
"`VLLM_TORCH_PROFILER_WITH_PROFILE_MEMORY` do not work with vLLM Ascend "
"now. Please use vLLM `--profiler-config` parameters instead. "
"[#5928](https://github.com/vllm-project/vllm-ascend/pull/5928)"
msgstr ""
"分析器环境变量（如 `VLLM_TORCH_PROFILER_DIR` 和 `VLLM_TORCH_PROFILER_WITH_PROFILE_MEMORY`）现已不适用于 vLLM Ascend。请改用 vLLM 的 `--profiler-config` 参数。"
"[#5928](https://github.com/vllm-project/vllm-ascend/pull/5928)"

#: ../../source/user_guide/release_notes.md:48
#: ../../source/user_guide/release_notes.md:158
#: ../../source/user_guide/release_notes.md:195
#: ../../source/user_guide/release_notes.md:244
#: ../../source/user_guide/release_notes.md:299
#: ../../source/user_guide/release_notes.md:401
#: ../../source/user_guide/release_notes.md:455
#: ../../source/user_guide/release_notes.md:500
#: ../../source/user_guide/release_notes.md:534
#: ../../source/user_guide/release_notes.md:589
#: ../../source/user_guide/release_notes.md:703
#: ../../source/user_guide/release_notes.md:739
#: ../../source/user_guide/release_notes.md:793
#: ../../source/user_guide/release_notes.md:1037
#: ../../source/user_guide/release_notes.md:1069
msgid "Known Issues"
msgstr "已知问题"

#: ../../source/user_guide/release_notes.md:50
msgid ""
"If you hit the pickle error from `EngineCore` process sometimes, please "
"cherry-pick the [PR](https://github.com/vllm-project/vllm/pull/32022) "
"into your local vLLM code. This known issue will be fixed in vLLM in the "
"next release."
msgstr ""
"如果您偶尔遇到来自 `EngineCore` 进程的 pickle 错误，请将 [PR](https://github.com/vllm-project/vllm/pull/32022) 拣选到您的本地 vLLM 代码中。此已知问题将在 vLLM 的下一个版本中修复。"

#: ../../source/user_guide/release_notes.md:52
msgid "v0.13.0rc2 - 2026.01.24"
msgstr "v0.13.0rc2 - 2026.01.24"

#: ../../source/user_guide/release_notes.md:54
msgid ""
"This is the second release candidate of v0.13.0 for vLLM Ascend. In this "
"rc release, we fixed lots of bugs and improved the performance of many "
"models. Please follow the [official "
"doc](https://docs.vllm.ai/projects/ascend/en/v0.13.0/) to get started. "
"Any feedback is welcome to help us to improve the final version of "
"v0.13.0."
msgstr ""
"这是 vLLM Ascend v0.13.0 的第二个候选版本。在此 rc 版本中，我们修复了大量错误并提升了许多模型的性能。请按照 [官方文档](https://docs.vllm.ai/projects/ascend/en/v0.13.0/) 开始使用。欢迎任何反馈以帮助我们改进 v0.13.0 的最终版本。"

#: ../../source/user_guide/release_notes.md:58
msgid ""
"We mainly focus on quality and performance improvement in this release. "
"The spec decode, graph mode, context parallel and EPLB have been improved"
" significantly. A lot of bugs have been fixed and the performance has "
"been improved for DeepSeek3.1/3.2, Qwen3 Dense/MOE models."
msgstr ""
"在此版本中，我们主要关注质量和性能的提升。推测解码、图模式、上下文并行和 EPLB 都得到了显著改进。修复了大量错误，并提升了 DeepSeek3.1/3.2、Qwen3 Dense/MOE 模型的性能。"

#: ../../source/user_guide/release_notes.md:62
msgid ""
"implement basic framework for batch invariant [#5517](https://github.com"
"/vllm-project/vllm-ascend/pull/5517)"
msgstr "实现批处理不变性的基础框架 [#5517](https://github.com/vllm-project/vllm-ascend/pull/5517)"

#: ../../source/user_guide/release_notes.md:63
msgid ""
"Eagle spec decode feature now works with full graph mode. "
"[#5118](https://github.com/vllm-project/vllm-ascend/pull/5118)"
msgstr "Eagle 推测解码功能现在可与全图模式协同工作。[#5118](https://github.com/vllm-project/vllm-ascend/pull/5118)"

#: ../../source/user_guide/release_notes.md:64
msgid ""
"Context Parallel(PCP&DCP) feature is more stable now. And it works for "
"most case. Please try it out."
msgstr "上下文并行（PCP&DCP）功能现已更加稳定，适用于大多数场景。请尝试使用。"

#: ../../source/user_guide/release_notes.md:65
msgid ""
"MTP and eagle spec decode feature now works in most cases. And it's "
"suggested to use them in most cases."
msgstr "MTP 和 eagle 推测解码功能现在在大多数情况下都能正常工作。建议在大多数情况下使用它们。"

#: ../../source/user_guide/release_notes.md:66
msgid ""
"EPLB feature more stable now. Many bugs have been fixed. Mix placement "
"works now [#6086](https://github.com/vllm-project/vllm-ascend/pull/6086)"
msgstr "EPLB 功能现在更加稳定。修复了许多错误。混合放置现已可用 [#6086](https://github.com/vllm-project/vllm-ascend/pull/6086)"

#: ../../source/user_guide/release_notes.md:67
msgid ""
"Support kv nz feature for DeepSeek decode node in disagg-prefill scenario"
" [#3072](https://github.com/vllm-project/vllm-ascend/pull/3072)"
msgstr "在 disaggregated-prefill 场景中为 DeepSeek 解码节点支持 kv nz 特性 [#3072](https://github.com/vllm-project/vllm-ascend/pull/3072)"

#: ../../source/user_guide/release_notes.md:69
msgid "Model Support"
msgstr "模型支持"

#: ../../source/user_guide/release_notes.md:71
msgid ""
"LongCat-Flash is supported now.[#3833](https://github.com/vllm-project"
"/vllm-ascend/pull/3833)"
msgstr "现已支持 LongCat-Flash。[#3833](https://github.com/vllm-project/vllm-ascend/pull/3833)"

#: ../../source/user_guide/release_notes.md:72
msgid ""
"minimax_m2 is supported now. [#5624](https://github.com/vllm-project"
"/vllm-ascend/pull/5624)"
msgstr "现已支持 minimax_m2。[#5624](https://github.com/vllm-project/vllm-ascend/pull/5624)"

#: ../../source/user_guide/release_notes.md:73
msgid ""
"Support for cross-attention and whisper models [#5592](https://github.com"
"/vllm-project/vllm-ascend/pull/5592)"
msgstr "支持 cross-attention 和 whisper 模型 [#5592](https://github.com/vllm-project/vllm-ascend/pull/5592)"

#: ../../source/user_guide/release_notes.md:77
msgid ""
"Many custom ops and triton kernels are added in this release to speed up "
"the performance of models. Such as `RejectSampler`, "
"`MoeInitRoutingCustom`, `DispatchFFNCombine` and so on."
msgstr "此版本添加了许多自定义算子和 triton 内核以加速模型性能。例如 `RejectSampler`、`MoeInitRoutingCustom`、`DispatchFFNCombine` 等。"

#: ../../source/user_guide/release_notes.md:78
msgid ""
"Improved the performance of Layerwise Connector "
"[#5303](https://github.com/vllm-project/vllm-ascend/pull/5303)"
msgstr "提升了 Layerwise Connector 的性能 [#5303](https://github.com/vllm-project/vllm-ascend/pull/5303)"

#: ../../source/user_guide/release_notes.md:82
msgid ""
"Basic support Model Runner v2. Model Runner V2 is the next generation of "
"vLLM. It will be used by default in the future release. "
"[#5210](https://github.com/vllm-project/vllm-ascend/pull/5210)"
msgstr "基础支持 Model Runner v2。Model Runner V2 是 vLLM 的下一代引擎，将在未来版本中默认使用。[#5210](https://github.com/vllm-project/vllm-ascend/pull/5210)"

#: ../../source/user_guide/release_notes.md:83
msgid ""
"Fixed a bug that the zmq send/receive may failed "
"[#5503](https://github.com/vllm-project/vllm-ascend/pull/5503)"
msgstr "修复了 zmq 发送/接收可能失败的 bug [#5503](https://github.com/vllm-project/vllm-ascend/pull/5503)"

#: ../../source/user_guide/release_notes.md:84
msgid ""
"Supported to use full-graph with Qwen3-Next-MTP "
"[#5477](https://github.com/vllm-project/vllm-ascend/pull/5477)"
msgstr "支持 Qwen3-Next-MTP 使用全图模式 [#5477](https://github.com/vllm-project/vllm-ascend/pull/5477)"

#: ../../source/user_guide/release_notes.md:85
msgid ""
"Fix weight transpose in RL scenarios [#5567](https://github.com/vllm-"
"project/vllm-ascend/pull/5567)"
msgstr "修复 RL 场景中的权重转置问题 [#5567](https://github.com/vllm-project/vllm-ascend/pull/5567)"

#: ../../source/user_guide/release_notes.md:86
msgid ""
"Adapted SP to eagle3 [#5562](https://github.com/vllm-project/vllm-"
"ascend/pull/5562)"
msgstr "使 SP 适配 eagle3 [#5562](https://github.com/vllm-project/vllm-ascend/pull/5562)"

#: ../../source/user_guide/release_notes.md:87
msgid ""
"Context Parallel(PCP&DCP) support mlapo [#5672](https://github.com/vllm-"
"project/vllm-ascend/pull/5672)"
msgstr "上下文并行（PCP&DCP）支持 mlapo [#5672](https://github.com/vllm-project/vllm-ascend/pull/5672)"

#: ../../source/user_guide/release_notes.md:88
msgid ""
"GLM4.6 support mtp with fullgraph [#5460](https://github.com/vllm-project"
"/vllm-ascend/pull/5460)"
msgstr "GLM4.6 支持 mtp 与全图模式 [#5460](https://github.com/vllm-project/vllm-ascend/pull/5460)"

#: ../../source/user_guide/release_notes.md:89
msgid ""
"Flashcomm2 now works with oshard generalized feature "
"[#4723](https://github.com/vllm-project/vllm-ascend/pull/4723)"
msgstr "Flashcomm2 现在可与 oshard generalized 特性协同工作 [#4723](https://github.com/vllm-project/vllm-ascend/pull/4723)"

#: ../../source/user_guide/release_notes.md:90
msgid ""
"Support setting tp=1 for the Eagle draft model [#5804](https://github.com"
"/vllm-project/vllm-ascend/pull/5804)"
msgstr "支持为 Eagle 草稿模型设置 tp=1 [#5804](https://github.com/vllm-project/vllm-ascend/pull/5804)"

#: ../../source/user_guide/release_notes.md:91
msgid ""
"Flashcomm1 feature now works with qwen3-vl [#5848](https://github.com"
"/vllm-project/vllm-ascend/pull/5848)"
msgstr "Flashcomm1 功能现在可与 qwen3-vl 协同工作 [#5848](https://github.com/vllm-project/vllm-ascend/pull/5848)"

#: ../../source/user_guide/release_notes.md:92
msgid ""
"Support fine-grained shared expert overlap [#5962](https://github.com"
"/vllm-project/vllm-ascend/pull/5962)"
msgstr "支持细粒度共享专家重叠 [#5962](https://github.com/vllm-project/vllm-ascend/pull/5962)"

#: ../../source/user_guide/release_notes.md:96
msgid "CANN is upgraded to 8.5.0"
msgstr "CANN 已升级至 8.5.0"

#: ../../source/user_guide/release_notes.md:97
msgid ""
"torch-npu is upgraded to 2.8.0.post1. Please note that the post version "
"will not be installed by default. Please install it by hand from [pypi "
"mirror](https://mirrors.huaweicloud.com/ascend/repos/pypi/torch-npu/)."
msgstr ""
"torch-npu 已升级至 2.8.0.post1。请注意，此 post 版本不会默认安装。请从 [pypi 镜像](https://mirrors.huaweicloud.com/ascend/repos/pypi/torch-npu/) 手动安装。"

#: ../../source/user_guide/release_notes.md:98
msgid "triton-ascend is upgraded to 3.2.0"
msgstr "triton-ascend 已升级至 3.2.0"

#: ../../source/user_guide/release_notes.md:102
msgid ""
"`CPUOffloadingConnector` is deprecated. We'll remove it in the next "
"release. It'll be replaced by CPUOffload feature from vLLM in the future."
msgstr "`CPUOffloadingConnector` 已被弃用。我们将在下一个版本中移除它。未来将由 vLLM 的 CPUOffload 功能替代。"

#: ../../source/user_guide/release_notes.md:103
msgid ""
"eplb config options is moved to `eplb_config` in [additional "
"config](https://docs.vllm.ai/projects/ascend/en/latest/user_guide/configuration/additional_config.html)."
" The old ones will be removed in the next release."
msgstr "eplb 配置选项已移至 [额外配置](https://docs.vllm.ai/projects/ascend/en/latest/user_guide/configuration/additional_config.html) 中的 `eplb_config`。旧选项将在下一个版本中移除。"

#: ../../source/user_guide/release_notes.md:104
msgid ""
"`ProfileExecuteDuration` "
"[feature](https://docs.vllm.ai/projects/ascend/en/latest/developer_guide/performance_and_debug/profile_execute_duration.html)"
" is deprecated. It's replaced by `ObservabilityConfig` from vLLM."
msgstr ""
"`ProfileExecuteDuration` "
"[功能](https://docs.vllm.ai/projects/ascend/en/latest/developer_guide/performance_and_debug/profile_execute_duration.html) 已被弃用。它已被 vLLM 的 `ObservabilityConfig` 取代。"

#: ../../source/user_guide/release_notes.md:105
msgid ""
"The value of `VLLM_ASCEND_ENABLE_MLAPO` env will be set to True by "
"default in the next release. It'll be enabled in decode node by default. "
"Please note that this feature will cost more memory. If you are memory "
"sensitive, please set it to False."
msgstr "`VLLM_ASCEND_ENABLE_MLAPO` 环境变量的值将在下一个版本中默认设置为 True。它将在解码节点中默认启用。请注意，此功能将消耗更多内存。如果您对内存敏感，请将其设置为 False。"

#: ../../source/user_guide/release_notes.md:107
msgid "v0.13.0rc1 - 2025.12.27"
msgstr "v0.13.0rc1 - 2025.12.27"

#: ../../source/user_guide/release_notes.md:109
msgid ""
"This is the first release candidate of v0.13.0 for vLLM Ascend. We landed"
" lots of bug fix, performance improvement and feature support in this "
"release. Any feedback is welcome to help us to improve vLLM Ascend. "
"Please follow the [official "
"doc](https://docs.vllm.ai/projects/ascend/en/latest) to get started."
msgstr ""
"这是 vLLM Ascend v0.13.0 的第一个候选发布版本。本次发布包含了大量错误修复、性能改进和功能支持。欢迎提供任何反馈以帮助我们改进 vLLM Ascend。请按照[官方文档](https://docs.vllm.ai/projects/ascend/en/latest)开始使用。"

#: ../../source/user_guide/release_notes.md:113
msgid ""
"Improved the performance of DeepSeek V3.2, please refer to "
"[tutorials](https://docs.vllm.ai/projects/ascend/en/latest/tutorials/DeepSeek-V3.2.html)"
msgstr ""
"提升了 DeepSeek V3.2 的性能，请参阅[教程](https://docs.vllm.ai/projects/ascend/en/latest/tutorials/DeepSeek-V3.2.html)"

#: ../../source/user_guide/release_notes.md:114
msgid ""
"Qwen3-Next MTP with chunked prefill is supported now "
"[#4770](https://github.com/vllm-project/vllm-ascend/pull/4770), please "
"refer to "
"[tutorials](https://docs.vllm.ai/projects/ascend/en/latest/tutorials/Qwen3-Next.html)"
msgstr ""
"现已支持带分块预填充的 Qwen3-Next MTP [#4770](https://github.com/vllm-project/vllm-ascend/pull/4770)，请参阅[教程](https://docs.vllm.ai/projects/ascend/en/latest/tutorials/Qwen3-Next.html)"

#: ../../source/user_guide/release_notes.md:115
msgid ""
"[Experimental] Prefill Context Parallel and Decode Context Parallel are "
"supported, but notice that it is an experimental feature now, welcome any"
" feedback. please refer to [context parallel feature "
"guide](https://docs.vllm.ai/projects/ascend/en/latest/user_guide/feature_guide/context_parallel.html)"
msgstr ""
"[实验性] 现已支持预填充上下文并行和解码上下文并行，但请注意这目前是实验性功能，欢迎任何反馈。请参阅[上下文并行功能指南](https://docs.vllm.ai/projects/ascend/en/latest/user_guide/feature_guide/context_parallel.html)"

#: ../../source/user_guide/release_notes.md:119
msgid ""
"Support openPangu Ultra MoE [4615](https://github.com/vllm-project/vllm-"
"ascend/pull/4615)"
msgstr ""
"支持 openPangu Ultra MoE [4615](https://github.com/vllm-project/vllm-ascend/pull/4615)"

#: ../../source/user_guide/release_notes.md:120
msgid ""
"A new quantization method W8A16 is supported now. "
"[#4541](https://github.com/vllm-project/vllm-ascend/pull/4541)"
msgstr ""
"现已支持新的量化方法 W8A16。[#4541](https://github.com/vllm-project/vllm-ascend/pull/4541)"

#: ../../source/user_guide/release_notes.md:121
msgid ""
"Cross-machine Disaggregated Prefill is supported now. "
"[#5008](https://github.com/vllm-project/vllm-ascend/pull/5008)"
msgstr ""
"现已支持跨机解耦预填充。[#5008](https://github.com/vllm-project/vllm-ascend/pull/5008)"

#: ../../source/user_guide/release_notes.md:122
msgid ""
"Add UCMConnector for KV Cache Offloading. [#4411](https://github.com"
"/vllm-project/vllm-ascend/pull/4411)"
msgstr ""
"为 KV Cache 卸载添加 UCMConnector。[#4411](https://github.com/vllm-project/vllm-ascend/pull/4411)"

#: ../../source/user_guide/release_notes.md:123
msgid ""
"Support async_scheduler and disable_padded_drafter_batch in eagle. "
"[#4893](https://github.com/vllm-project/vllm-ascend/pull/4893)"
msgstr ""
"在 eagle 中支持 async_scheduler 和 disable_padded_drafter_batch。[#4893](https://github.com/vllm-project/vllm-ascend/pull/4893)"

#: ../../source/user_guide/release_notes.md:124
msgid ""
"Support pcp + mtp in full graph mode. [#4572](https://github.com/vllm-"
"project/vllm-ascend/pull/4572)"
msgstr ""
"在全图模式下支持 pcp + mtp。[#4572](https://github.com/vllm-project/vllm-ascend/pull/4572)"

#: ../../source/user_guide/release_notes.md:125
msgid ""
"Enhance all-reduce skipping logic for MoE models in NPUModelRunner "
"[#5329](https://github.com/vllm-project/vllm-ascend/pull/5329)"
msgstr ""
"增强 NPUModelRunner 中 MoE 模型的 all-reduce 跳过逻辑 [#5329](https://github.com/vllm-project/vllm-ascend/pull/5329)"

#: ../../source/user_guide/release_notes.md:129
msgid "Some general performance improvement:"
msgstr "一些通用性能改进："

#: ../../source/user_guide/release_notes.md:131
msgid ""
"Add l2norm triton kernel [#4595](https://github.com/vllm-project/vllm-"
"ascend/pull/4595)"
msgstr ""
"添加 l2norm triton 内核 [#4595](https://github.com/vllm-project/vllm-ascend/pull/4595)"

#: ../../source/user_guide/release_notes.md:132
msgid ""
"Add new pattern for AddRmsnormQuant with SP, which could only take effect"
" in graph mode. [#5077](https://github.com/vllm-project/vllm-"
"ascend/pull/5077)"
msgstr ""
"为 AddRmsnormQuant 添加新的 SP 模式，该模式仅在图模式下生效。[#5077](https://github.com/vllm-project/vllm-ascend/pull/5077)"

#: ../../source/user_guide/release_notes.md:133
msgid ""
"Add async exponential while model executing. [#4501](https://github.com"
"/vllm-project/vllm-ascend/pull/4501)"
msgstr ""
"在模型执行时添加异步指数等待。[#4501](https://github.com/vllm-project/vllm-ascend/pull/4501)"

#: ../../source/user_guide/release_notes.md:134
msgid ""
"Remove the transpose step after attention and switch to "
"transpose_batchmatmul [#5390](https://github.com/vllm-project/vllm-"
"ascend/pull/5390)"
msgstr ""
"移除注意力后的转置步骤，并切换到 transpose_batchmatmul [#5390](https://github.com/vllm-project/vllm-ascend/pull/5390)"

#: ../../source/user_guide/release_notes.md:135
msgid ""
"To optimize the performance in small batch size scenario, an attention "
"operator with flash decoding function is offered, please refer to item 22"
" in [FAQs](https://docs.vllm.ai/projects/ascend/en/latest/faqs.html) to "
"enable it."
msgstr ""
"为优化小批量场景下的性能，提供了一个带有 flash decoding 功能的注意力算子，请参阅[常见问题解答](https://docs.vllm.ai/projects/ascend/en/latest/faqs.html)中的第22项以启用它。"

#: ../../source/user_guide/release_notes.md:137
#: ../../source/user_guide/release_notes.md:175
#: ../../source/user_guide/release_notes.md:225
#: ../../source/user_guide/release_notes.md:263
#: ../../source/user_guide/release_notes.md:288
#: ../../source/user_guide/release_notes.md:324
msgid "Other"
msgstr "其它"

#: ../../source/user_guide/release_notes.md:139
msgid ""
"OOM error on VL models is fixed now. We're keeping observing it, if you "
"hit OOM problem again, please submit an issue. [#5136](https://github.com"
"/vllm-project/vllm-ascend/pull/5136)"
msgstr ""
"VL 模型的 OOM 错误现已修复。我们将持续观察，如果您再次遇到 OOM 问题，请提交 issue。[#5136](https://github.com/vllm-project/vllm-ascend/pull/5136)"

#: ../../source/user_guide/release_notes.md:140
msgid ""
"Fixed an accuracy bug of Qwen3-Next-MTP when batched inferring. "
"[#4932](https://github.com/vllm-project/vllm-ascend/pull/4932)"
msgstr ""
"修复了 Qwen3-Next-MTP 在批量推理时的精度错误。[#4932](https://github.com/vllm-project/vllm-ascend/pull/4932)"

#: ../../source/user_guide/release_notes.md:141
msgid ""
"Fix npu-cpu offloading interface change bug. [#5290](https://github.com"
"/vllm-project/vllm-ascend/pull/5290)"
msgstr ""
"修复 npu-cpu 卸载接口变更导致的错误。[#5290](https://github.com/vllm-project/vllm-ascend/pull/5290)"

#: ../../source/user_guide/release_notes.md:142
msgid ""
"Fix MHA model runtime error in aclgraph mode [#5397](https://github.com"
"/vllm-project/vllm-ascend/pull/5397)"
msgstr ""
"修复 MHA 模型在 aclgraph 模式下的运行时错误 [#5397](https://github.com/vllm-project/vllm-ascend/pull/5397)"

#: ../../source/user_guide/release_notes.md:143
msgid ""
"Fix unsuitable moe_comm_type under ep=1 scenario "
"[#5388](https://github.com/vllm-project/vllm-ascend/pull/5388)"
msgstr ""
"修复在 ep=1 场景下使用不合适的 moe_comm_type 的问题 [#5388](https://github.com/vllm-project/vllm-ascend/pull/5388)"

#: ../../source/user_guide/release_notes.md:147
msgid ""
"`VLLM_ASCEND_ENABLE_DENSE_OPTIMIZE` is removed and "
"`VLLM_ASCEND_ENABLE_PREFETCH_MLP` is recommend to replace as they always "
"be enabled together. [#5272](https://github.com/vllm-project/vllm-"
"ascend/pull/5272)"
msgstr ""
"`VLLM_ASCEND_ENABLE_DENSE_OPTIMIZE` 已被移除，建议使用 `VLLM_ASCEND_ENABLE_PREFETCH_MLP` 替代，因为它们总是一起启用。[#5272](https://github.com/vllm-project/vllm-ascend/pull/5272)"

#: ../../source/user_guide/release_notes.md:148
msgid ""
"`VLLM_ENABLE_FUSED_EXPERTS_ALLGATHER_EP` is dropped now. "
"[#5270](https://github.com/vllm-project/vllm-ascend/pull/5270)"
msgstr ""
"`VLLM_ENABLE_FUSED_EXPERTS_ALLGATHER_EP` 现已弃用。[#5270](https://github.com/vllm-project/vllm-ascend/pull/5270)"

#: ../../source/user_guide/release_notes.md:149
msgid ""
"`VLLM_ASCEND_ENABLE_NZ` is disabled for float weight case, since we "
"notice that the performance is not good in some float case. Feel free to "
"set it to 2 if you make sure it works for your case. "
"[#4878](https://github.com/vllm-project/vllm-ascend/pull/4878)"
msgstr ""
"对于浮点权重的情况，`VLLM_ASCEND_ENABLE_NZ` 已被禁用，因为我们注意到在某些浮点情况下性能不佳。如果您确定它适用于您的情况，可以将其设置为 2。[#4878](https://github.com/vllm-project/vllm-ascend/pull/4878)"

#: ../../source/user_guide/release_notes.md:150
msgid ""
"`chunked_prefill_for_mla` in `additional_config` is dropped now. "
"[#5296](https://github.com/vllm-project/vllm-ascend/pull/5296)"
msgstr ""
"`additional_config` 中的 `chunked_prefill_for_mla` 现已弃用。[#5296](https://github.com/vllm-project/vllm-ascend/pull/5296)"

#: ../../source/user_guide/release_notes.md:151
msgid ""
"`dump_config` in `additional_config` is renamed to `dump_config_path` and"
" the type is change from `dict` to `string`. [#5296](https://github.com"
"/vllm-project/vllm-ascend/pull/5296)"
msgstr ""
"`additional_config` 中的 `dump_config` 已重命名为 `dump_config_path`，类型从 `dict` 更改为 `string`。[#5296](https://github.com/vllm-project/vllm-ascend/pull/5296)"

#: ../../source/user_guide/release_notes.md:155
msgid ""
"vLLM version has been upgraded to 0.13.0 and drop 0.12.0 support. "
"[#5146](https://github.com/vllm-project/vllm-ascend/pull/5146)"
msgstr ""
"vLLM 版本已升级至 0.13.0 并放弃对 0.12.0 的支持。[#5146](https://github.com/vllm-project/vllm-ascend/pull/5146)"

#: ../../source/user_guide/release_notes.md:156
msgid ""
"Transformer version has been upgraded >= 4.57.3 "
"[#5250](https://github.com/vllm-project/vllm-ascend/pull/5250)"
msgstr ""
"Transformer 版本已升级至 >= 4.57.3 [#5250](https://github.com/vllm-project/vllm-ascend/pull/5250)"

#: ../../source/user_guide/release_notes.md:160
msgid ""
"Qwen3-Next doesn't support long sequence scenario, and we should limit "
"`gpu-memory-utilization` according to the doc to run Qwen3-Next. We'll "
"improve it in the next release"
msgstr ""
"Qwen3-Next 不支持长序列场景，我们需要根据文档限制 `gpu-memory-utilization` 来运行 Qwen3-Next。我们将在下一个版本中改进此问题。"

#: ../../source/user_guide/release_notes.md:161
msgid ""
"The functional break on Qwen3-Next when the input/output is around "
"3.5k/1.5k is fixed, but it introduces a regression on performance. We'll "
"fix it in next release. [#5357](https://github.com/vllm-project/vllm-"
"ascend/issues/5357)"
msgstr ""
"Qwen3-Next 在输入/输出约为 3.5k/1.5k 时的功能中断问题已修复，但这导致了性能回退。我们将在下一个版本中修复此问题。[#5357](https://github.com/vllm-project/vllm-ascend/issues/5357)"

#: ../../source/user_guide/release_notes.md:162
msgid ""
"There is a precision issue with curl on ultra-short sequences in "
"DeepSeek-V3.2. We'll fix it in next release. [#5370](https://github.com"
"/vllm-project/vllm-ascend/issues/5370)"
msgstr ""
"DeepSeek-V3.2 在超短序列上存在 curl 精度问题。我们将在下一个版本中修复此问题。[#5370](https://github.com/vllm-project/vllm-ascend/issues/5370)"

#: ../../source/user_guide/release_notes.md:164
msgid "v0.11.0 - 2025.12.16"
msgstr "v0.11.0 - 2025年12月16日"

#: ../../source/user_guide/release_notes.md:166
msgid ""
"We're excited to announce the release of v0.11.0 for vLLM Ascend. This is"
" the official release for v0.11.0. Please follow the [official "
"doc](https://docs.vllm.ai/projects/ascend/en/v0.11.0) to get started. "
"We'll consider to release post version in the future if needed. This "
"release note will only contain the important change and note from "
"v0.11.0rc3."
msgstr ""
"我们很高兴地宣布 vLLM Ascend v0.11.0 版本发布。这是 v0.11.0 的正式发布版本。请按照[官方文档](https://docs.vllm.ai/projects/ascend/en/v0.11.0)开始使用。未来如有需要，我们会考虑发布后续版本。本发布说明仅包含自 v0.11.0rc3 以来的重要变更和注意事项。"

#: ../../source/user_guide/release_notes.md:170
msgid ""
"Improved the performance for deepseek 3/3.1. [#3995](https://github.com"
"/vllm-project/vllm-ascend/pull/3995)"
msgstr ""
"提升了 deepseek 3/3.1 的性能。[#3995](https://github.com/vllm-project/vllm-ascend/pull/3995)"

#: ../../source/user_guide/release_notes.md:171
msgid ""
"Fixed the accuracy bug for qwen3-vl. [#4811](https://github.com/vllm-"
"project/vllm-ascend/pull/4811)"
msgstr ""
"修复了 qwen3-vl 的精度错误。[#4811](https://github.com/vllm-project/vllm-ascend/pull/4811)"

#: ../../source/user_guide/release_notes.md:172
msgid ""
"Improved the performance of sample. [#4153](https://github.com/vllm-"
"project/vllm-ascend/pull/4153)"
msgstr ""
"改进了 sample 的性能。[#4153](https://github.com/vllm-project/vllm-"
"ascend/pull/4153)"

#: ../../source/user_guide/release_notes.md:173
msgid ""
"Eagle3 is back now. [#4721](https://github.com/vllm-project/vllm-"
"ascend/pull/4721)"
msgstr ""
"Eagle3 现已恢复可用。[#4721](https://github.com/vllm-project/vllm-"
"ascend/pull/4721)"

#: ../../source/user_guide/release_notes.md:177
msgid ""
"Improved the performance for kimi-k2.  [#4555](https://github.com/vllm-"
"project/vllm-ascend/pull/4555)"
msgstr ""
"提升了 kimi-k2 的性能。[#4555](https://github.com/vllm-project/vllm-"
"ascend/pull/4555)"

#: ../../source/user_guide/release_notes.md:178
msgid ""
"Fixed a quantization bug for deepseek3.2-exp. [#4797](https://github.com"
"/vllm-project/vllm-ascend/pull/4797)"
msgstr ""
"修复了 deepseek3.2-exp 的一个量化错误。[#4797](https://github.com/vllm-"
"project/vllm-ascend/pull/4797)"

#: ../../source/user_guide/release_notes.md:179
msgid ""
"Fixed qwen3-vl-moe bug under high concurrency. [#4658](https://github.com"
"/vllm-project/vllm-ascend/pull/4658)"
msgstr ""
"修复了 qwen3-vl-moe 在高并发下的错误。[#4658](https://github.com/vllm-"
"project/vllm-ascend/pull/4658)"

#: ../../source/user_guide/release_notes.md:180
msgid ""
"Fixed an accuracy bug for Prefill Decode disaggregation case. "
"[#4437](https://github.com/vllm-project/vllm-ascend/pull/4437)"
msgstr ""
"修复了 Prefill Decode 分离场景下的一个精度错误。[#4437](https://github.com/vllm-"
"project/vllm-ascend/pull/4437)"

#: ../../source/user_guide/release_notes.md:181
msgid ""
"Fixed some bugs for EPLB [#4576](https://github.com/vllm-project/vllm-"
"ascend/pull/4576) [#4777](https://github.com/vllm-project/vllm-"
"ascend/pull/4777)"
msgstr ""
"修复了 EPLB 的一些错误。[#4576](https://github.com/vllm-project/vllm-"
"ascend/pull/4576) [#4777](https://github.com/vllm-project/vllm-"
"ascend/pull/4777)"

#: ../../source/user_guide/release_notes.md:182
msgid ""
"Fixed the version incompatibility issue for openEuler docker image. "
"[#4745](https://github.com/vllm-project/vllm-ascend/pull/4745)"
msgstr ""
"修复了 openEuler docker 镜像的版本兼容性问题。[#4745](https://github.com/vllm-"
"project/vllm-ascend/pull/4745)"

#: ../../source/user_guide/release_notes.md:184
msgid "Deprecation announcement"
msgstr "弃用公告"

#: ../../source/user_guide/release_notes.md:186
msgid "LLMdatadist connector has been deprecated, it'll be removed in v0.12.0rc1"
msgstr "LLMdatadist 连接器已被弃用，将在 v0.12.0rc1 中移除"

#: ../../source/user_guide/release_notes.md:187
msgid "Torchair graph has been deprecated, it'll be removed in v0.12.0rc1"
msgstr "Torchair 图模式已被弃用，将在 v0.12.0rc1 中移除"

#: ../../source/user_guide/release_notes.md:188
msgid "Ascend scheduler has been deprecated, it'll be removed in v0.12.0rc1"
msgstr "Ascend 调度器已被弃用，将在 v0.12.0rc1 中移除"

#: ../../source/user_guide/release_notes.md:190
msgid "Upgrade notice"
msgstr "升级须知"

#: ../../source/user_guide/release_notes.md:192
#: ../../source/user_guide/release_notes.md:259
msgid ""
"torch-npu is upgraded to 2.7.1.post1. Please note that the package is "
"pushed to [pypi mirror](https://mirrors.huaweicloud.com/ascend/repos/pypi"
"/torch-npu/). So it's hard to add it to auto dependence. Please install "
"it by yourself."
msgstr ""
"torch-npu 已升级至 2.7.1.post1。请注意，该软件包已推送至 [pypi 镜像](https://mirrors.huaweicloud.com/ascend/repos/pypi/torch-npu/)。因此难以将其加入自动依赖。请自行安装。"

#: ../../source/user_guide/release_notes.md:193
msgid "CANN is upgraded to 8.3.rc2."
msgstr "CANN 已升级至 8.3.rc2。"

#: ../../source/user_guide/release_notes.md:197
msgid ""
"Qwen3-Next doesn't support expert parallel and MTP features in this "
"release. And it'll be oom if the input is too long. We'll improve it in "
"the next release"
msgstr ""
"Qwen3-Next 在本版本中不支持专家并行和 MTP 功能。并且如果输入过长会导致内存不足。我们将在下个版本中改进。"

#: ../../source/user_guide/release_notes.md:198
msgid ""
"Deepseek 3.2 only work with torchair graph mode in this release. We'll "
"make it work with aclgraph mode in the next release."
msgstr ""
"Deepseek 3.2 在本版本中仅支持 torchair 图模式。我们将在下个版本中使其支持 aclgraph 模式。"

#: ../../source/user_guide/release_notes.md:199
msgid ""
"Qwen2-audio doesn't work by default. Temporary solution is to set `--gpu-"
"memory-utilization` to a suitable value, such as 0.8."
msgstr ""
"Qwen2-audio 默认无法工作。临时解决方案是将 `--gpu-memory-utilization` 设置为合适的值，例如 0.8。"

#: ../../source/user_guide/release_notes.md:200
msgid ""
"CPU bind feature doesn't work if more than one vLLM instance is running "
"on the same node."
msgstr "如果在同一节点上运行多个 vLLM 实例，CPU 绑定功能将无法工作。"

#: ../../source/user_guide/release_notes.md:202
msgid "v0.12.0rc1 - 2025.12.13"
msgstr "v0.12.0rc1 - 2025.12.13"

#: ../../source/user_guide/release_notes.md:204
msgid ""
"This is the first release candidate of v0.12.0 for vLLM Ascend. We landed"
" lots of bug fix, performance improvement and feature support in this "
"release. Any feedback is welcome to help us to improve vLLM Ascend. "
"Please follow the [official "
"doc](https://docs.vllm.ai/projects/ascend/en/latest) to get started."
msgstr ""
"这是 vLLM Ascend v0.12.0 的第一个候选版本。我们在本版本中修复了大量错误，提升了性能并增加了功能支持。欢迎任何反馈以帮助我们改进 vLLM Ascend。请按照[官方文档](https://docs.vllm.ai/projects/ascend/en/latest)开始使用。"

#: ../../source/user_guide/release_notes.md:208
msgid ""
"DeepSeek 3.2 is stable and performance is improved. In this release, you "
"don't need to install any other packages now. Following the [official "
"tutorial](https://docs.vllm.ai/projects/ascend/en/latest/tutorials/DeepSeek-V3.2.html)"
" to start using it."
msgstr ""
"DeepSeek 3.2 已稳定且性能得到提升。在本版本中，您无需再安装任何其他软件包。请按照[官方教程](https://docs.vllm.ai/projects/ascend/en/latest/tutorials/DeepSeek-V3.2.html)开始使用。"

#: ../../source/user_guide/release_notes.md:209
msgid ""
"Async scheduler is more stable and ready to enable now. Please set "
"`--async-scheduling` to enable it."
msgstr "异步调度器现已更加稳定并可以启用。请设置 `--async-scheduling` 来启用它。"

#: ../../source/user_guide/release_notes.md:210
msgid ""
"More new models, such as Qwen3-omni, DeepSeek OCR, PaddleOCR, OpenCUA are"
" supported now."
msgstr "现已支持更多新模型，例如 Qwen3-omni、DeepSeek OCR、PaddleOCR、OpenCUA。"

#: ../../source/user_guide/release_notes.md:212
#: ../../source/user_guide/release_notes.md:282
#: ../../source/user_guide/release_notes.md:316
#: ../../source/user_guide/release_notes.md:350
#: ../../source/user_guide/release_notes.md:374
#: ../../source/user_guide/release_notes.md:418
#: ../../source/user_guide/release_notes.md:487
#: ../../source/user_guide/release_notes.md:512
#: ../../source/user_guide/release_notes.md:547
#: ../../source/user_guide/release_notes.md:651
#: ../../source/user_guide/release_notes.md:719
#: ../../source/user_guide/release_notes.md:771
#: ../../source/user_guide/release_notes.md:830
#: ../../source/user_guide/release_notes.md:894
#: ../../source/user_guide/release_notes.md:920
#: ../../source/user_guide/release_notes.md:946
#: ../../source/user_guide/release_notes.md:969
#: ../../source/user_guide/release_notes.md:995
#: ../../source/user_guide/release_notes.md:1021
#: ../../source/user_guide/release_notes.md:1058
msgid "Core"
msgstr "核心"

#: ../../source/user_guide/release_notes.md:214
#, python-brace-format
msgid ""
"[Experimental] Full decode only graph mode is supported now. Although it "
"is not enabled by default, we suggest to enable it by `--compilation-"
"config '{\"cudagraph_mode\":\"FULL_DECODE_ONLY\"}'` in most case. Let us "
"know if you hit any error. We'll keep improve it and enable it by default"
" in next few release."
msgstr ""
"[实验性] 现已支持 Full decode only 图模式。虽然默认未启用，但我们建议在大多数情况下通过 `--compilation-config '{\"cudagraph_mode\":\"FULL_DECODE_ONLY\"}'` 启用它。如果您遇到任何错误，请告知我们。我们将持续改进，并在未来几个版本中默认启用。"

#: ../../source/user_guide/release_notes.md:215
msgid ""
"Lots of triton kernel are added. The performance of vLLM Ascend, "
"especially Qwen3-Next and DeepSeek 3.2 is improved. Please note that "
"triton is not installed and enabled by default, but we suggest to enable "
"it in most case. You can download and install it by hand from [package "
"url](https://vllm-ascend.obs.cn-north-4.myhuaweicloud.com/vllm-"
"ascend/triton_ascend-3.2.0.dev2025110717-cp311-cp311-manylinux_2_27_aarch64.whl)."
" If you're running vLLM Ascend with X86, you need to build triton ascend "
"by yourself from [source](https://gitcode.com/Ascend/triton-ascend)"
msgstr ""
"新增了大量 triton 内核。vLLM Ascend 的性能，特别是 Qwen3-Next 和 DeepSeek 3.2 的性能得到了提升。请注意，triton 默认未安装和启用，但我们建议在大多数情况下启用它。您可以从[软件包链接](https://vllm-ascend.obs.cn-north-4.myhuaweicloud.com/vllm-ascend/triton_ascend-3.2.0.dev2025110717-cp311-cp311-manylinux_2_27_aarch64.whl)手动下载安装。如果您在 X86 架构上运行 vLLM Ascend，则需要从[源代码](https://gitcode.com/Ascend/triton-ascend)自行构建 triton ascend。"

#: ../../source/user_guide/release_notes.md:216
msgid ""
"Lots of Ascend ops are added to improve the performance. It means that "
"from this release vLLM Ascend only works with custom ops built. So we "
"removed the env `COMPILE_CUSTOM_KERNELS`. You can not set it to 0 now."
msgstr ""
"新增了大量 Ascend 算子以提升性能。这意味着从本版本开始，vLLM Ascend 仅在使用自定义算子构建时才能工作。因此我们移除了环境变量 `COMPILE_CUSTOM_KERNELS`。您现在不能将其设置为 0。"

#: ../../source/user_guide/release_notes.md:217
msgid ""
"speculative decode method `MTP` is more stable now. It can be enabled "
"with most case and decode token number can be 1,2,3."
msgstr "推测解码方法 `MTP` 现已更加稳定。它可以在大多数情况下启用，解码令牌数可以是 1、2、3。"

#: ../../source/user_guide/release_notes.md:218
msgid ""
"speculative decode method `suffix` is supported now. Thanks for the "
"contribution from China Merchants Bank."
msgstr "现已支持推测解码方法 `suffix`。感谢招商银行的贡献。"

#: ../../source/user_guide/release_notes.md:219
msgid ""
"llm-comppressor quantization tool with W8A8 works now. You can now deploy"
" the model with W8A8 quantization from this tool directly."
msgstr "支持 W8A8 量化的 llm-comppressor 量化工具现已可用。您现在可以直接使用此工具部署 W8A8 量化模型。"

#: ../../source/user_guide/release_notes.md:220
msgid "W4A4 quantization works now."
msgstr "W4A4 量化现已可用。"

#: ../../source/user_guide/release_notes.md:221
msgid ""
"Support features flashcomm1 and flashcomm2 in paper "
"[flashcomm](https://arxiv.org/pdf/2412.04964) [#3004](https://github.com"
"/vllm-project/vllm-ascend/pull/3004) [#3334](https://github.com/vllm-"
"project/vllm-ascend/pull/3334)"
msgstr ""
"支持论文 [flashcomm](https://arxiv.org/pdf/2412.04964) 中的 flashcomm1 和 flashcomm2 功能。[#3004](https://github.com/vllm-project/vllm-ascend/pull/3004) [#3334](https://github.com/vllm-project/vllm-ascend/pull/3334)"

#: ../../source/user_guide/release_notes.md:222
msgid "Pooling model, such as bge, reranker, etc. are supported now"
msgstr "现已支持池化模型，例如 bge、reranker 等。"

#: ../../source/user_guide/release_notes.md:223
msgid ""
"Official doc has been improved. we refactored the tutorial to make it "
"more clear. The user guide and developer guide is more complete now. "
"We'll keep improving it."
msgstr "官方文档已得到改进。我们重构了教程使其更清晰。用户指南和开发者指南现已更加完整。我们将持续改进。"

#: ../../source/user_guide/release_notes.md:227
msgid "[Experimental] Mooncake layerwise connector is supported now."
msgstr "[实验性] 现已支持 Mooncake 分层连接器。"

#: ../../source/user_guide/release_notes.md:228
msgid ""
"[Experimental] [KV cache "
"pool](https://docs.vllm.ai/projects/ascend/en/latest/developer_guide/feature_guide/KV_Cache_Pool_Guide.html)"
" feature is added"
msgstr ""
"[实验性] 新增了 [KV 缓存池](https://docs.vllm.ai/projects/ascend/en/latest/developer_guide/feature_guide/KV_Cache_Pool_Guide.html) 功能"

#: ../../source/user_guide/release_notes.md:229
msgid ""
"[Experimental] A new graph mode `xlite` is introduced. It performs good "
"with some models. Following the [official "
"tutorial](https://docs.vllm.ai/projects/ascend/en/latest/user_guide/feature_guide/graph_mode.html"
"#using-xlitegraph) to start using it."
msgstr ""
"[实验性] 引入了一种新的图模式 `xlite`。它在某些模型上表现良好。请按照[官方教程](https://docs.vllm.ai/projects/ascend/en/latest/user_guide/feature_guide/graph_mode.html#using-xlitegraph)开始使用。"

#: ../../source/user_guide/release_notes.md:230
msgid ""
"LLMdatadist kv connector is removed. Please use mooncake connector "
"instead."
msgstr "LLMdatadist kv 连接器已被移除。请改用 mooncake 连接器。"

#: ../../source/user_guide/release_notes.md:231
msgid ""
"Ascend scheduler is removed. `--additional-config {\"ascend_scheudler\": "
"{\"enabled\": true}` doesn't work anymore."
msgstr "Ascend 调度器已被移除。`--additional-config {\"ascend_scheudler\": {\"enabled\": true}` 不再有效。"

#: ../../source/user_guide/release_notes.md:232
#, python-brace-format
msgid ""
"Torchair graph mode is removed. `--additional-config "
"{\"torchair_graph_config\": {\"enabled\": true}}` doesn't work anymore. "
"Please use aclgraph instead."
msgstr "Torchair 图模式已被移除。`--additional-config {\"torchair_graph_config\": {\"enabled\": true}}` 不再有效。请改用 aclgraph。"

#: ../../source/user_guide/release_notes.md:233
msgid ""
"`VLLM_ASCEND_ENABLE_TOPK_TOPP_OPTIMIZATION` env is removed. This feature "
"is stable enough. We enable it by default now."
msgstr "环境变量 `VLLM_ASCEND_ENABLE_TOPK_TOPP_OPTIMIZATION` 已被移除。此功能已足够稳定。我们现在默认启用它。"
msgstr ""

#: ../../source/user_guide/release_notes.md:234
msgid "speculative decode method `Ngram` is back now."
msgstr "推测解码方法 `Ngram` 现已恢复。"

#: ../../source/user_guide/release_notes.md:235
msgid ""
"msprobe tool is added to help user to check the model accuracy. Please "
"follow the [official "
"doc](https://docs.vllm.ai/projects/ascend/en/latest/developer_guide/performance_and_debug/msprobe_guide.html)"
" to get started."
msgstr ""
"新增 msprobe 工具以帮助用户检查模型精度。请按照[官方文档](https://docs.vllm.ai/projects/ascend/en/latest/developer_guide/performance_and_debug/msprobe_guide.html)开始使用。"

#: ../../source/user_guide/release_notes.md:236
msgid ""
"msserviceprofiler tool is added to help user to profile the model "
"performance. Please follow the [official "
"doc](https://docs.vllm.ai/projects/ascend/en/latest/developer_guide/performance_and_debug/service_profiling_guide.html)"
" to get started."
msgstr ""
"新增 msserviceprofiler 工具以帮助用户分析模型性能。请按照[官方文档](https://docs.vllm.ai/projects/ascend/en/latest/developer_guide/performance_and_debug/service_profiling_guide.html)开始使用。"

#: ../../source/user_guide/release_notes.md:238
msgid "Upgrade Note"
msgstr "升级说明"

#: ../../source/user_guide/release_notes.md:240
msgid ""
"vLLM Ascend self maintained modeling file has been removed. The related "
"python entrypoint is removed as well. So please uninstall the old version"
" of vLLM Ascend in your env before upgrade."
msgstr ""
"vLLM Ascend 自维护的建模文件已被移除，相关的 Python 入口点也已删除。因此，请在升级前卸载您环境中的旧版本 vLLM Ascend。"

#: ../../source/user_guide/release_notes.md:241
msgid ""
"CANN is upgraded to 8.3.RC2, Pytorch and torch-npu are upgraded to 2.8.0."
" Don't forget to install them."
msgstr ""
"CANN 已升级至 8.3.RC2，Pytorch 和 torch-npu 已升级至 2.8.0。请勿忘记安装它们。"

#: ../../source/user_guide/release_notes.md:242
msgid "Python 3.9 support is dropped to keep the same with vLLM v0.12.0"
msgstr "为与 vLLM v0.12.0 保持一致，已停止支持 Python 3.9"

#: ../../source/user_guide/release_notes.md:246
msgid ""
"DeepSeek 3/3.1 and Qwen3 doesn't work with FULL_DECODE_ONLY graph mode. "
"We'll fix it in next release. [#4990](https://github.com/vllm-project"
"/vllm-ascend/pull/4990)"
msgstr ""
"DeepSeek 3/3.1 和 Qwen3 目前无法在 FULL_DECODE_ONLY 图模式下工作。我们将在下个版本中修复此问题。[#4990](https://github.com/vllm-project/vllm-ascend/pull/4990)"

#: ../../source/user_guide/release_notes.md:247
msgid ""
"Hunyuan OCR doesn't work. We'll fix it in the next release. "
"[#4989](https://github.com/vllm-project/vllm-ascend/pull/4989) "
"[#4992](https://github.com/vllm-project/vllm-ascend/pull/4992)"
msgstr ""
"Hunyuan OCR 目前无法工作。我们将在下个版本中修复此问题。[#4989](https://github.com/vllm-project/vllm-ascend/pull/4989) [#4992](https://github.com/vllm-project/vllm-ascend/pull/4992)"

#: ../../source/user_guide/release_notes.md:248
msgid ""
"DeepSeek 3.2 doesn't work with chat template. It because that vLLM "
"v0.12.0 doesn't support it. We'll support in the next v0.13.0rc1 version."
msgstr ""
"DeepSeek 3.2 目前无法与聊天模板协同工作。这是因为 vLLM v0.12.0 尚未支持。我们将在下一个 v0.13.0rc1 版本中提供支持。"

#: ../../source/user_guide/release_notes.md:249
msgid ""
"DeepSeek 3.2 doesn't work with high concurrency in some case. We'll fix "
"it in next release. [#4996](https://github.com/vllm-project/vllm-"
"ascend/pull/4996)"
msgstr ""
"DeepSeek 3.2 在某些情况下无法在高并发下正常工作。我们将在下个版本中修复此问题。[#4996](https://github.com/vllm-project/vllm-ascend/pull/4996)"

#: ../../source/user_guide/release_notes.md:250
msgid ""
"We notice that bf16/fp16 model doesn't perform well, it's mainly because "
"that `VLLM_ASCEND_ENABLE_NZ` is enabled by default. Please set "
"`VLLM_ASCEND_ENABLE_NZ=0` to disable it. We'll add the auto detection "
"mechanism in next release."
msgstr ""
"我们注意到 bf16/fp16 模型性能不佳，这主要是因为 `VLLM_ASCEND_ENABLE_NZ` 默认启用。请设置 `VLLM_ASCEND_ENABLE_NZ=0` 来禁用它。我们将在下个版本中添加自动检测机制。"

#: ../../source/user_guide/release_notes.md:251
msgid ""
"speculative decode method `suffix` doesn't work. We'll fix it in next "
"release. You can pick this commit to fix the issue: "
"[#5010](https://github.com/vllm-project/vllm-ascend/pull/5010)"
msgstr ""
"推测解码方法 `suffix` 目前无法工作。我们将在下个版本中修复此问题。您可以选择此提交来修复该问题：[#5010](https://github.com/vllm-project/vllm-ascend/pull/5010)"

#: ../../source/user_guide/release_notes.md:253
msgid "v0.11.0rc3 - 2025.12.03"
msgstr "v0.11.0rc3 - 2025年12月03日"

#: ../../source/user_guide/release_notes.md:255
msgid ""
"This is the third release candidate of v0.11.0 for vLLM Ascend. For "
"quality reasons, we released a new rc before the official release. Thanks"
" for all your feedback. Please follow the [official "
"doc](https://docs.vllm.ai/projects/ascend/en/v0.11.0) to get started."
msgstr ""
"这是 vLLM Ascend v0.11.0 的第三个候选发布版本。出于质量考虑，我们在正式发布前发布了新的 rc 版本。感谢您的所有反馈。请按照[官方文档](https://docs.vllm.ai/projects/ascend/en/v0.11.0)开始使用。"

#: ../../source/user_guide/release_notes.md:260
msgid ""
"Disable NZ weight loader to speed up dense model. Please note that this "
"is a temporary solution. If you find the performance becomes bad, please "
"let us know. We'll keep improving it. [#4495](https://github.com/vllm-"
"project/vllm-ascend/pull/4495)"
msgstr ""
"禁用 NZ 权重加载器以加速密集模型。请注意，这是一个临时解决方案。如果您发现性能变差，请告知我们。我们将持续改进。[#4495](https://github.com/vllm-project/vllm-ascend/pull/4495)"

#: ../../source/user_guide/release_notes.md:261
msgid ""
"mooncake is installed in official docker image now. You can use it "
"directly in container now. [#4506](https://github.com/vllm-project/vllm-"
"ascend/pull/4506)"
msgstr ""
"mooncake 现已安装在官方 Docker 镜像中。您现在可以直接在容器中使用它。[#4506](https://github.com/vllm-project/vllm-ascend/pull/4506)"

#: ../../source/user_guide/release_notes.md:265
msgid ""
"Fix an OOM issue for moe models. [#4367](https://github.com/vllm-project"
"/vllm-ascend/pull/4367)"
msgstr ""
"修复了 MoE 模型的一个 OOM 问题。[#4367](https://github.com/vllm-project/vllm-ascend/pull/4367)"

#: ../../source/user_guide/release_notes.md:266
msgid ""
"Fix hang issue of multimodal model when running with DP>1 "
"[#4393](https://github.com/vllm-project/vllm-ascend/pull/4393)"
msgstr ""
"修复了多模态模型在 DP>1 运行时出现的挂起问题。[#4393](https://github.com/vllm-project/vllm-ascend/pull/4393)"

#: ../../source/user_guide/release_notes.md:267
msgid ""
"Fix some bugs for EPLB [#4416](https://github.com/vllm-project/vllm-"
"ascend/pull/4416)"
msgstr ""
"修复了 EPLB 的一些错误。[#4416](https://github.com/vllm-project/vllm-ascend/pull/4416)"

#: ../../source/user_guide/release_notes.md:268
msgid ""
"Fix bug for mtp>1 + lm_head_tp>1 case [#4360](https://github.com/vllm-"
"project/vllm-ascend/pull/4360)"
msgstr ""
"修复了 mtp>1 + lm_head_tp>1 情况下的错误。[#4360](https://github.com/vllm-project/vllm-ascend/pull/4360)"

#: ../../source/user_guide/release_notes.md:269
msgid ""
"Fix a accuracy issue when running vLLM serve for long time. "
"[#4117](https://github.com/vllm-project/vllm-ascend/pull/4117)"
msgstr ""
"修复了 vLLM serve 长时间运行时的精度问题。[#4117](https://github.com/vllm-project/vllm-ascend/pull/4117)"

#: ../../source/user_guide/release_notes.md:270
msgid ""
"Fix a function bug when running qwen2.5 vl under high concurrency. "
"[#4553](https://github.com/vllm-project/vllm-ascend/pull/4553)"
msgstr ""
"修复了 qwen2.5 vl 在高并发下运行时的功能错误。[#4553](https://github.com/vllm-project/vllm-ascend/pull/4553)"

#: ../../source/user_guide/release_notes.md:272
msgid "v0.11.0rc2 - 2025.11.21"
msgstr "v0.11.0rc2 - 2025年11月21日"

#: ../../source/user_guide/release_notes.md:274
msgid ""
"This is the second release candidate of v0.11.0 for vLLM Ascend. In this "
"release, we solved many bugs to improve the quality. Thanks for all your "
"feedback. We'll keep working on bug fix and performance improvement. The "
"v0.11.0 official release will come soon. Please follow the [official "
"doc](https://docs.vllm.ai/projects/ascend/en/v0.11.0) to get started."
msgstr ""
"这是 vLLM Ascend v0.11.0 的第二个候选发布版本。在此版本中，我们修复了许多错误以提升质量。感谢您的所有反馈。我们将继续致力于错误修复和性能改进。v0.11.0 正式版即将发布。请按照[官方文档](https://docs.vllm.ai/projects/ascend/en/v0.11.0)开始使用。"

#: ../../source/user_guide/release_notes.md:278
msgid ""
"CANN is upgraded to 8.3.RC2. [#4332](https://github.com/vllm-project"
"/vllm-ascend/pull/4332)"
msgstr ""
"CANN 已升级至 8.3.RC2。[#4332](https://github.com/vllm-project/vllm-ascend/pull/4332)"

#: ../../source/user_guide/release_notes.md:279
msgid ""
"Ngram spec decode method is back now. [#4092](https://github.com/vllm-"
"project/vllm-ascend/pull/4092)"
msgstr ""
"Ngram 推测解码方法现已恢复。[#4092](https://github.com/vllm-project/vllm-ascend/pull/4092)"

#: ../../source/user_guide/release_notes.md:280
msgid ""
"The performance of aclgraph is improved by updating default capture size."
" [#4205](https://github.com/vllm-project/vllm-ascend/pull/4205)"
msgstr ""
"通过更新默认捕获大小，提升了 aclgraph 的性能。[#4205](https://github.com/vllm-project/vllm-ascend/pull/4205)"

#: ../../source/user_guide/release_notes.md:284
msgid ""
"Speed up vLLM startup time. [#4099](https://github.com/vllm-project/vllm-"
"ascend/pull/4099)"
msgstr ""
"加速了 vLLM 的启动时间。[#4099](https://github.com/vllm-project/vllm-ascend/pull/4099)"

#: ../../source/user_guide/release_notes.md:285
msgid ""
"Kimi k2 with quantization works now. [#4190](https://github.com/vllm-"
"project/vllm-ascend/pull/4190)"
msgstr ""
"支持量化的 Kimi k2 现已可用。[#4190](https://github.com/vllm-project/vllm-ascend/pull/4190)"

#: ../../source/user_guide/release_notes.md:286
msgid ""
"Fix a bug for qwen3-next. It's more stable now. "
"[#4025](https://github.com/vllm-project/vllm-ascend/pull/4025)"
msgstr ""
"修复了 qwen3-next 的一个错误。现在它更稳定了。[#4025](https://github.com/vllm-project/vllm-ascend/pull/4025)"

#: ../../source/user_guide/release_notes.md:290
msgid ""
"Fix an issue for full decode only mode. Full graph mode is more stable "
"now. [#4106](https://github.com/vllm-project/vllm-ascend/pull/4106) "
"[#4282](https://github.com/vllm-project/vllm-ascend/pull/4282)"
msgstr ""
"修复了仅全解码模式的一个问题。全图模式现在更稳定了。[#4106](https://github.com/vllm-project/vllm-ascend/pull/4106) [#4282](https://github.com/vllm-project/vllm-ascend/pull/4282)"

#: ../../source/user_guide/release_notes.md:291
msgid ""
"Fix a allgather ops bug for DeepSeek V3 series models. "
"[#3711](https://github.com/vllm-project/vllm-ascend/pull/3711)"
msgstr ""
"修复了 DeepSeek V3 系列模型的一个 allgather 操作错误。[#3711](https://github.com/vllm-project/vllm-ascend/pull/3711)"

#: ../../source/user_guide/release_notes.md:292
msgid ""
"Fix some bugs for EPLB feature. [#4150](https://github.com/vllm-project"
"/vllm-ascend/pull/4150) [#4334](https://github.com/vllm-project/vllm-"
"ascend/pull/4334)"
msgstr ""
"修复了 EPLB 功能的一些错误。[#4150](https://github.com/vllm-project/vllm-ascend/pull/4150) [#4334](https://github.com/vllm-project/vllm-ascend/pull/4334)"

#: ../../source/user_guide/release_notes.md:293
msgid ""
"Fix a bug that vl model doesn't work on x86 machine. "
"[#4285](https://github.com/vllm-project/vllm-ascend/pull/4285)"
msgstr ""
"修复了 VL 模型在 x86 机器上无法工作的错误。[#4285](https://github.com/vllm-project/vllm-ascend/pull/4285)"

#: ../../source/user_guide/release_notes.md:294
msgid ""
"Support ipv6 for prefill disaggregation proxy. Please note that mooncake "
"connector doesn't work with ipv6. We're working on it. "
"[#4242](https://github.com/vllm-project/vllm-ascend/pull/4242)"
msgstr ""
"为预填充解耦代理支持 ipv6。请注意，mooncake 连接器目前不支持 ipv6。我们正在解决此问题。[#4242](https://github.com/vllm-project/vllm-ascend/pull/4242)"
msgstr ""
"添加检查以确保EPLB在量化场景下仅支持w8a8方法。[#4315](https://github.com/vllm-project/vllm-ascend/pull/4315)"

#: ../../source/user_guide/release_notes.md:296
msgstr ""
"添加检查以确保FLASHCOMM功能不与VL模型同时工作。该功能计划于2025年第四季度支持。[#4222](https://github.com/vllm-project/vllm-ascend/pull/4222)"

#: ../../source/user_guide/release_notes.md:297
msgstr ""
"音频所需库已安装于容器中。[#4324](https://github.com/vllm-project/vllm-ascend/pull/4324)"

#: ../../source/user_guide/release_notes.md:301
msgstr ""
"Ray与专家并行（EP）同时使用时存在问题。若需在Ray环境下运行vLLM Ascend，请禁用专家并行。[#4123](https://github.com/vllm-project/vllm-ascend/pull/4123)"

#: ../../source/user_guide/release_notes.md:302
msgstr ""
"`response_format` 参数暂不支持，我们将在后续版本中添加支持。[#4175](https://github.com/vllm-project/vllm-ascend/pull/4175)"

#: ../../source/user_guide/release_notes.md:303
msgstr ""
"CPU绑定功能在多实例场景下（例如单节点多数据并行）无法正常工作。我们将在下一个版本中修复此问题。"

#: ../../source/user_guide/release_notes.md:305
msgstr "v0.11.0rc1 - 2025年11月10日"

#: ../../source/user_guide/release_notes.md:307
msgstr ""
"这是vLLM Ascend v0.11.0的第一个候选发布版本。请参阅[官方文档](https://docs.vllm.ai/projects/ascend/en/v0.11.0)开始使用。v0.11.0将是vLLM Ascend的下一个正式发布版本，我们将在未来几天内发布。欢迎提供任何反馈以帮助我们改进v0.11.0。"

#: ../../source/user_guide/release_notes.md:312
msgstr ""
"CANN已升级至8.3.RC1，Torch-npu已升级至2.7.1。[#3945](https://github.com/vllm-project/vllm-ascend/pull/3945) [#3896](https://github.com/vllm-project/vllm-ascend/pull/3896)"

#: ../../source/user_guide/release_notes.md:313
msgstr ""
"前缀缓存（PrefixCache）和分块预填充（Chunked Prefill）现已默认启用。[#3967](https://github.com/vllm-project/vllm-ascend/pull/3967)"

#: ../../source/user_guide/release_notes.md:314
msgstr ""
"现已支持W4A4量化。[#3427](https://github.com/vllm-project/vllm-ascend/pull/3427) 官方教程请参阅[此处](https://docs.vllm.ai/projects/ascend/en/latest/tutorials/single_npu_qwen3_w4a4.html)。"

#: ../../source/user_guide/release_notes.md:318
msgstr "Qwen3和Deepseek V3系列模型的性能得到提升。"

#: ../../source/user_guide/release_notes.md:319
msgstr ""
"现已支持Mooncake分层连接器。[#2602](https://github.com/vllm-project/vllm-ascend/pull/2602) 教程请参阅[此处](https://docs.vllm.ai/projects/ascend/en/latest/tutorials/pd_disaggregation_mooncake_multi_node.html)。"

#: ../../source/user_guide/release_notes.md:320
msgstr ""
"现已支持MTP > 1。[#2708](https://github.com/vllm-project/vllm-ascend/pull/2708)"

#: ../../source/user_guide/release_notes.md:321
msgstr ""
"[实验性功能] 现已支持图模式 `FULL_DECODE_ONLY`！`FULL` 模式将在未来几周内推出。[#2128](https://github.com/vllm-project/vllm-ascend/pull/2128)"

#: ../../source/user_guide/release_notes.md:322
msgstr ""
"现已支持池化模型，例如bge-m3。[#3171](https://github.com/vllm-project/vllm-ascend/pull/3171)"

#: ../../source/user_guide/release_notes.md:326
msgstr ""
"重构MOE模块，使其更清晰易懂，并在量化和非量化场景下均提升了性能。"

#: ../../source/user_guide/release_notes.md:327
msgstr ""
"重构模型注册模块以降低维护难度。我们计划在2025年第四季度移除该模块。[#3004](https://github.com/vllm-project/vllm-ascend/pull/3004)"

#: ../../source/user_guide/release_notes.md:328
msgstr ""
"Torchair已被弃用。一旦ACL Graph性能足够好，我们将移除它，截止日期为2026年第一季度。"

#: ../../source/user_guide/release_notes.md:329
msgstr "LLMDatadist KV连接器已被弃用，我们将在2026年第一季度移除它。"

#: ../../source/user_guide/release_notes.md:330
msgstr ""
"重构线性模块以支持论文[flashcomm](https://arxiv.org/pdf/2412.04964)中的flashcomm1和flashcomm2特性。[#3004](https://github.com/vllm-project/vllm-ascend/pull/3004) [#3334](https://github.com/vllm-project/vllm-ascend/pull/3334)"

#: ../../source/user_guide/release_notes.md:332
msgstr "已知问题"

#: ../../source/user_guide/release_notes.md:334
msgstr ""
"长时间服务后可能出现内存泄漏，导致服务卡住。此问题源于torch-npu，我们将尽快升级并修复。"

#: ../../source/user_guide/release_notes.md:335
msgstr ""
"Qwen2.5 VL模型的准确性欠佳。此问题由CANN导致，我们将尽快修复。"

#: ../../source/user_guide/release_notes.md:336
msgstr ""
"对于长序列输入场景，有时无响应且KV缓存使用率升高。这是调度器的一个缺陷，我们正在处理中。"

#: ../../source/user_guide/release_notes.md:337
msgstr ""
"Qwen2-audio默认无法工作，我们正在修复。临时解决方案是将 `--gpu-memory-utilization` 设置为合适的值，例如0.8。"

#: ../../source/user_guide/release_notes.md:338
msgstr ""
"当启用专家并行运行Qwen3-Next时，请将 `HCCL_BUFFSIZE` 环境变量设置为合适的值，例如1024。"

#: ../../source/user_guide/release_notes.md:339
msgstr ""
"DeepSeek3.2在使用aclgraph时准确性不正确。临时解决方案是根据输入批次大小，将 `cudagraph_capture_sizes` 设置为合适的值。"

#: ../../source/user_guide/release_notes.md:341
msgstr "v0.11.0rc0 - 2025年09月30日"

#: ../../source/user_guide/release_notes.md:343
msgstr ""
"这是vLLM Ascend v0.11.0的特殊候选发布版本。请参阅[官方文档](https://docs.vllm.ai/projects/ascend/en/)开始使用。"

#: ../../source/user_guide/release_notes.md:347
msgstr ""
"现已支持DeepSeek V3.2。[#3270](https://github.com/vllm-project/vllm-ascend/pull/3270)"

#: ../../source/user_guide/release_notes.md:348
msgstr ""
"现已支持Qwen3-vl。[#3103](https://github.com/vllm-project/vllm-ascend/pull/3103)"

#: ../../source/user_guide/release_notes.md:352
msgstr ""
"DeepSeek现已支持与aclgraph协同工作。[#2707](https://github.com/vllm-project/vllm-ascend/pull/2707)"

#: ../../source/user_guide/release_notes.md:353
msgstr ""
"MTP现已支持与aclgraph协同工作。[#2932](https://github.com/vllm-project/vllm-ascend/pull/2932)"

#: ../../source/user_guide/release_notes.md:354
msgstr ""
"现已支持EPLB。[#2956](https://github.com/vllm-project/vllm-ascend/pull/2956)"

#: ../../source/user_guide/release_notes.md:355
msgstr ""
"现已支持Mooncake存储KV缓存连接器。[#2913](https://github.com/vllm-project/vllm-ascend/pull/2913)"

#: ../../source/user_guide/release_notes.md:356
msgstr ""
"现已支持CPU卸载连接器。[#1659](https://github.com/vllm-project/vllm-ascend/pull/1659)"

#: ../../source/user_guide/release_notes.md:360
msgstr ""
"Qwen3-next现已稳定。[#3007](https://github.com/vllm-project/vllm-ascend/pull/3007)"
#: ../../source/user_guide/release_notes.md:361
msgid ""
"Fixed a lot of bugs introduced in v0.10.2 by Qwen3-next. "
"[#2964](https://github.com/vllm-project/vllm-ascend/pull/2964) "
"[#2781](https://github.com/vllm-project/vllm-ascend/pull/2781) "
"[#3070](https://github.com/vllm-project/vllm-ascend/pull/3070) "
"[#3113](https://github.com/vllm-project/vllm-ascend/pull/3113)"
msgstr ""
"修复了 Qwen3-next 在 v0.10.2 版本中引入的大量错误。"
"[#2964](https://github.com/vllm-project/vllm-ascend/pull/2964) "
"[#2781](https://github.com/vllm-project/vllm-ascend/pull/2781) "
"[#3070](https://github.com/vllm-project/vllm-ascend/pull/3070) "
"[#3113](https://github.com/vllm-project/vllm-ascend/pull/3113)"

#: ../../source/user_guide/release_notes.md:362
msgid ""
"The LoRA feature is back now. [#3044](https://github.com/vllm-project"
"/vllm-ascend/pull/3044)"
msgstr "LoRA 功能现已恢复。[#3044](https://github.com/vllm-project/vllm-ascend/pull/3044)"

#: ../../source/user_guide/release_notes.md:363
msgid ""
"Eagle3 spec decode method is back now. [#2949](https://github.com/vllm-"
"project/vllm-ascend/pull/2949)"
msgstr "Eagle3 推测解码方法现已恢复。[#2949](https://github.com/vllm-project/vllm-ascend/pull/2949)"

#: ../../source/user_guide/release_notes.md:365
msgid "v0.10.2rc1 - 2025.09.16"
msgstr "v0.10.2rc1 - 2025年09月16日"

#: ../../source/user_guide/release_notes.md:367
msgid ""
"This is the 1st release candidate of v0.10.2 for vLLM Ascend. Please "
"follow the [official doc](https://docs.vllm.ai/projects/ascend/en/) to "
"get started."
msgstr ""
"这是 vLLM Ascend v0.10.2 的第一个候选发布版本。请按照[官方文档](https://docs.vllm.ai/projects/ascend/en/)开始使用。"

#: ../../source/user_guide/release_notes.md:371
msgid ""
"Added support for Qwen3-Next. Please note that the expert parallel and "
"MTP features do not work with this release. We will be adding support for"
" them soon. Follow the [official "
"guide](https://docs.vllm.ai/projects/ascend/en/latest/tutorials/multi_npu_qwen3_next.html)"
" to get started. [#2917](https://github.com/vllm-project/vllm-"
"ascend/pull/2917)"
msgstr ""
"新增对 Qwen3-Next 的支持。请注意，专家并行和 MTP 功能在此版本中不可用。我们将很快添加对它们的支持。请按照[官方指南](https://docs.vllm.ai/projects/ascend/en/latest/tutorials/multi_npu_qwen3_next.html)开始使用。"
"[#2917](https://github.com/vllm-project/vllm-ascend/pull/2917)"

#: ../../source/user_guide/release_notes.md:372
msgid ""
"Added quantization support for aclgraph [#2841](https://github.com/vllm-"
"project/vllm-ascend/pull/2841)"
msgstr "为 aclgraph 添加量化支持 [#2841](https://github.com/vllm-project/vllm-ascend/pull/2841)"

#: ../../source/user_guide/release_notes.md:376
msgid ""
"Aclgraph now works with Ray backend. [#2589](https://github.com/vllm-"
"project/vllm-ascend/pull/2589)"
msgstr "Aclgraph 现在可与 Ray 后端协同工作。[#2589](https://github.com/vllm-project/vllm-ascend/pull/2589)"

#: ../../source/user_guide/release_notes.md:377
msgid ""
"MTP now works with the token > 1. [#2708](https://github.com/vllm-project"
"/vllm-ascend/pull/2708)"
msgstr "MTP 现在支持 token 数大于 1 的情况。[#2708](https://github.com/vllm-project/vllm-ascend/pull/2708)"

#: ../../source/user_guide/release_notes.md:378
msgid ""
"Qwen2.5 VL now works with quantization. [#2778](https://github.com/vllm-"
"project/vllm-ascend/pull/2778)"
msgstr "Qwen2.5 VL 现在支持量化。[#2778](https://github.com/vllm-project/vllm-ascend/pull/2778)"

#: ../../source/user_guide/release_notes.md:379
msgid ""
"Improved the performance with async scheduler enabled. "
"[#2783](https://github.com/vllm-project/vllm-ascend/pull/2783)"
msgstr "启用了异步调度器后，性能得到提升。[#2783](https://github.com/vllm-project/vllm-ascend/pull/2783)"

#: ../../source/user_guide/release_notes.md:380
msgid ""
"Fixed the performance regression with non MLA model when using default "
"scheduler. [#2894](https://github.com/vllm-project/vllm-ascend/pull/2894)"
msgstr "修复了非 MLA 模型在使用默认调度器时的性能回退问题。[#2894](https://github.com/vllm-project/vllm-ascend/pull/2894)"

#: ../../source/user_guide/release_notes.md:384
msgid ""
"The performance of W8A8 quantization is improved. "
"[#2275](https://github.com/vllm-project/vllm-ascend/pull/2275)"
msgstr "W8A8 量化的性能得到提升。[#2275](https://github.com/vllm-project/vllm-ascend/pull/2275)"

#: ../../source/user_guide/release_notes.md:385
msgid ""
"The performance is improved for moe models. [#2689](https://github.com"
"/vllm-project/vllm-ascend/pull/2689) [#2842](https://github.com/vllm-"
"project/vllm-ascend/pull/2842)"
msgstr "MoE 模型的性能得到提升。[#2689](https://github.com/vllm-project/vllm-ascend/pull/2689) [#2842](https://github.com/vllm-project/vllm-ascend/pull/2842)"

#: ../../source/user_guide/release_notes.md:386
msgid ""
"Fixed resources limit error when apply speculative decoding and aclgraph."
" [#2472](https://github.com/vllm-project/vllm-ascend/pull/2472)"
msgstr "修复了应用推测解码和 aclgraph 时的资源限制错误。[#2472](https://github.com/vllm-project/vllm-ascend/pull/2472)"

#: ../../source/user_guide/release_notes.md:387
msgid ""
"Fixed the git config error in Docker images. [#2746](https://github.com"
"/vllm-project/vllm-ascend/pull/2746)"
msgstr "修复了 Docker 镜像中的 git 配置错误。[#2746](https://github.com/vllm-project/vllm-ascend/pull/2746)"

#: ../../source/user_guide/release_notes.md:388
msgid ""
"Fixed the sliding windows attention bug with prefill. "
"[#2758](https://github.com/vllm-project/vllm-ascend/pull/2758)"
msgstr "修复了预填充阶段的滑动窗口注意力错误。[#2758](https://github.com/vllm-project/vllm-ascend/pull/2758)"

#: ../../source/user_guide/release_notes.md:389
msgid ""
"The official doc for Prefill-Decode Disaggregation with Qwen3 is added. "
"[#2751](https://github.com/vllm-project/vllm-ascend/pull/2751)"
msgstr "新增了关于 Qwen3 预填充-解码分离的官方文档。[#2751](https://github.com/vllm-project/vllm-ascend/pull/2751)"

#: ../../source/user_guide/release_notes.md:390
msgid ""
"`VLLM_ENABLE_FUSED_EXPERTS_ALLGATHER_EP` env works again. "
"[#2740](https://github.com/vllm-project/vllm-ascend/pull/2740)"
msgstr "`VLLM_ENABLE_FUSED_EXPERTS_ALLGATHER_EP` 环境变量重新生效。[#2740](https://github.com/vllm-project/vllm-ascend/pull/2740)"

#: ../../source/user_guide/release_notes.md:391
msgid ""
"A new improvement for oproj in deepseek is added. Set "
"`oproj_tensor_parallel_size` to enable this feature. "
"[#2167](https://github.com/vllm-project/vllm-ascend/pull/2167)"
msgstr "为 DeepSeek 的 oproj 添加了一项新优化。设置 `oproj_tensor_parallel_size` 以启用此功能。[#2167](https://github.com/vllm-project/vllm-ascend/pull/2167)"

#: ../../source/user_guide/release_notes.md:392
msgid ""
"Fix a bug that deepseek with torchair doesn't work as expect when "
"`graph_batch_sizes` is set. [#2760](https://github.com/vllm-project/vllm-"
"ascend/pull/2760)"
msgstr "修复了一个 bug：当设置了 `graph_batch_sizes` 时，使用 torchair 的 DeepSeek 模型未按预期工作。[#2760](https://github.com/vllm-project/vllm-ascend/pull/2760)"

#: ../../source/user_guide/release_notes.md:393
msgid ""
"Avoid duplicate generation of sin_cos_cache in rope when kv_seqlen > 4k. "
"[#2744](https://github.com/vllm-project/vllm-ascend/pull/2744)"
msgstr "当 kv_seqlen > 4k 时，避免在 rope 中重复生成 sin_cos_cache。[#2744](https://github.com/vllm-project/vllm-ascend/pull/2744)"

#: ../../source/user_guide/release_notes.md:394
msgid ""
"The performance of Qwen3 dense model is improved with flashcomm_v1. Set "
"`VLLM_ASCEND_ENABLE_DENSE_OPTIMIZE=1` and "
"`VLLM_ASCEND_ENABLE_FLASHCOMM=1` to enable it. [#2779](https://github.com"
"/vllm-project/vllm-ascend/pull/2779)"
msgstr "通过 flashcomm_v1 提升了 Qwen3 稠密模型的性能。设置 `VLLM_ASCEND_ENABLE_DENSE_OPTIMIZE=1` 和 `VLLM_ASCEND_ENABLE_FLASHCOMM=1` 以启用此优化。[#2779](https://github.com/vllm-project/vllm-ascend/pull/2779)"

#: ../../source/user_guide/release_notes.md:395
msgid ""
"The performance of Qwen3 dense model is improved with prefetch feature. "
"Set `VLLM_ASCEND_ENABLE_PREFETCH_MLP=1` to enable it. "
"[#2816](https://github.com/vllm-project/vllm-ascend/pull/2816)"
msgstr "通过预取功能提升了 Qwen3 稠密模型的性能。设置 `VLLM_ASCEND_ENABLE_PREFETCH_MLP=1` 以启用此功能。[#2816](https://github.com/vllm-project/vllm-ascend/pull/2816)"

#: ../../source/user_guide/release_notes.md:396
msgid ""
"The performance of Qwen3 MoE model is improved with rope ops update. "
"[#2571](https://github.com/vllm-project/vllm-ascend/pull/2571)"
msgstr "通过更新 rope 算子提升了 Qwen3 MoE 模型的性能。[#2571](https://github.com/vllm-project/vllm-ascend/pull/2571)"

#: ../../source/user_guide/release_notes.md:397
msgid ""
"Fix the weight load error for RLHF case. [#2756](https://github.com/vllm-"
"project/vllm-ascend/pull/2756)"
msgstr "修复了 RLHF 场景下的权重加载错误。[#2756](https://github.com/vllm-project/vllm-ascend/pull/2756)"

#: ../../source/user_guide/release_notes.md:398
msgid ""
"Add warm_up_atb step to speed up the inference. "
"[#2823](https://github.com/vllm-project/vllm-ascend/pull/2823)"
msgstr "添加 warm_up_atb 步骤以加速推理。[#2823](https://github.com/vllm-project/vllm-ascend/pull/2823)"

#: ../../source/user_guide/release_notes.md:399
msgid ""
"Fixed the aclgraph steam error for moe model. [#2827](https://github.com"
"/vllm-project/vllm-ascend/pull/2827)"
msgstr "修复了 MoE 模型的 aclgraph 流错误。[#2827](https://github.com/vllm-project/vllm-ascend/pull/2827)"

#: ../../source/user_guide/release_notes.md:403
msgid ""
"The server will hang when running Prefill Decode Disaggregation with "
"different TP size for P and D. It's fixed by [vLLM "
"commit](https://github.com/vllm-project/vllm/pull/23917) which is not "
"included in v0.10.2. You can pick this commit to fix the issue."
msgstr ""
"当预填充和解码阶段使用不同的 TP 大小时，运行预填充-解码分离会导致服务器挂起。此问题已由 [vLLM commit](https://github.com/vllm-project/vllm/pull/23917) 修复，但该提交未包含在 v0.10.2 中。您可以选取此提交来修复该问题。"

#: ../../source/user_guide/release_notes.md:404
msgid ""
"The HBM usage of Qwen3-Next is higher than expected. It is a [known "
"issue](https://github.com/vllm-project/vllm-ascend/issues/2884) and we "
"are working on it. You can set `max_model_len` and "
"`gpu_memory_utilization` to suitable value based on your parallel "
"configuration to avoid oom error."
msgstr ""
"Qwen3-Next 的 HBM 使用率高于预期。这是一个[已知问题](https://github.com/vllm-project/vllm-ascend/issues/2884)，我们正在处理中。您可以根据您的并行配置，将 `max_model_len` 和 `gpu_memory_utilization` 设置为合适的值以避免内存溢出错误。"

#: ../../source/user_guide/release_notes.md:405
msgid ""
"We notice that LoRA does not work with this release due to the refactor "
"of KV cache. We will fix it soon. [2941](https://github.com/vllm-project"
"/vllm-ascend/issues/2941)"
msgstr ""
"我们注意到，由于 KV 缓存的重新设计，LoRA 在此版本中无法使用。我们将很快修复此问题。[2941](https://github.com/vllm-project/vllm-ascend/issues/2941)"

#: ../../source/user_guide/release_notes.md:406
msgid ""
"Please do not enable chunked prefill with prefix cache when running with "
"Ascend scheduler. The performance and accuracy is not good/correct. "
"[#2943](https://github.com/vllm-project/vllm-ascend/issues/2943)"
msgstr ""
"在使用 Ascend 调度器运行时，请不要启用带有前缀缓存的块式预填充。其性能和准确性不佳/不正确。[#2943](https://github.com/vllm-project/vllm-ascend/issues/2943)"

#: ../../source/user_guide/release_notes.md:408
msgid "v0.10.1rc1 - 2025.09.04"
msgstr "v0.10.1rc1 - 2025年09月04日"

#: ../../source/user_guide/release_notes.md:410
msgid ""
msgstr ""
"This is the 1st release candidate of v0.10.1 for vLLM Ascend. Please "
"follow the [official doc](https://docs.vllm.ai/projects/ascend/en/) to "
"get started."
msgstr ""
"这是 vLLM Ascend v0.10.1 的第一个候选发布版本。请按照[官方文档](https://docs.vllm.ai/projects/ascend/en/)开始使用。"

#: ../../source/user_guide/release_notes.md:414
msgid ""
"LoRA Performance improved much through adding Custom Kernels by China "
"Merchants Bank. [#2325](https://github.com/vllm-project/vllm-"
"ascend/pull/2325)"
msgstr ""
"通过招商银行添加的自定义内核，LoRA 性能得到大幅提升。[#2325](https://github.com/vllm-project/vllm-ascend/pull/2325)"

#: ../../source/user_guide/release_notes.md:415
msgid ""
"Support Mooncake TransferEngine for kv cache register and pull_blocks "
"style disaggregate prefill implementation. [#1568](https://github.com"
"/vllm-project/vllm-ascend/pull/1568)"
msgstr ""
"支持使用 Mooncake TransferEngine 进行 KV 缓存注册和 pull_blocks 风格的解耦预填充实现。[#1568](https://github.com/vllm-project/vllm-ascend/pull/1568)"

#: ../../source/user_guide/release_notes.md:416
msgid ""
"Support capture custom ops into aclgraph now. [#2113](https://github.com"
"/vllm-project/vllm-ascend/pull/2113)"
msgstr ""
"现在支持将自定义算子捕获到 aclgraph 中。[#2113](https://github.com/vllm-project/vllm-ascend/pull/2113)"

#: ../../source/user_guide/release_notes.md:420
msgid ""
"Added MLP tensor parallel to improve performance, but note that this will"
" increase memory usage. [#2120](https://github.com/vllm-project/vllm-"
"ascend/pull/2120)"
msgstr ""
"增加了 MLP 张量并行以提升性能，但请注意这会增加内存使用量。[#2120](https://github.com/vllm-project/vllm-ascend/pull/2120)"

#: ../../source/user_guide/release_notes.md:421
msgid ""
"openEuler is upgraded to 24.03. [#2631](https://github.com/vllm-project"
"/vllm-ascend/pull/2631)"
msgstr ""
"openEuler 已升级至 24.03。[#2631](https://github.com/vllm-project/vllm-ascend/pull/2631)"

#: ../../source/user_guide/release_notes.md:422
msgid ""
"Added custom lmhead tensor parallel to achieve reduced memory consumption"
" and improved TPOT performance. [#2309](https://github.com/vllm-project"
"/vllm-ascend/pull/2309)"
msgstr ""
"增加了自定义 lmhead 张量并行，以降低内存消耗并提升 TPOT 性能。[#2309](https://github.com/vllm-project/vllm-ascend/pull/2309)"

#: ../../source/user_guide/release_notes.md:423
msgid ""
"Qwen3 MoE/Qwen2.5 support torchair graph now. [#2403](https://github.com"
"/vllm-project/vllm-ascend/pull/2403)"
msgstr ""
"Qwen3 MoE/Qwen2.5 现在支持 torchair graph。[#2403](https://github.com/vllm-project/vllm-ascend/pull/2403)"

#: ../../source/user_guide/release_notes.md:424
msgid ""
"Support Sliding Window Attention with AscendSceduler, thus fixing Gemma3 "
"accuracy issue. [#2528](https://github.com/vllm-project/vllm-"
"ascend/pull/2528)"
msgstr ""
"支持 AscendScheduler 的滑动窗口注意力机制，从而修复了 Gemma3 的精度问题。[#2528](https://github.com/vllm-project/vllm-ascend/pull/2528)"

#: ../../source/user_guide/release_notes.md:428
#: ../../source/user_guide/release_notes.md:559
msgid "Bug fixes:"
msgstr "漏洞修复："

#: ../../source/user_guide/release_notes.md:429
msgid ""
"Updated the graph capture size calculation, somehow alleviated the "
"problem that NPU stream not enough in some scenarios. "
"[#2511](https://github.com/vllm-project/vllm-ascend/pull/2511)"
msgstr ""
"更新了图捕获大小的计算方式，在一定程度上缓解了某些场景下 NPU 流不足的问题。[#2511](https://github.com/vllm-project/vllm-ascend/pull/2511)"

#: ../../source/user_guide/release_notes.md:430
msgid ""
"Fixed bugs and refactor cached mask generation logic. "
"[#2442](https://github.com/vllm-project/vllm-ascend/pull/2442)"
msgstr ""
"修复了漏洞并重构了缓存掩码生成逻辑。[#2442](https://github.com/vllm-project/vllm-ascend/pull/2442)"

#: ../../source/user_guide/release_notes.md:431
msgid ""
"Fixed the nz format does not work in quantization scenarios. "
"[#2549](https://github.com/vllm-project/vllm-ascend/pull/2549)"
msgstr ""
"修复了 nz 格式在量化场景下无效的问题。[#2549](https://github.com/vllm-project/vllm-ascend/pull/2549)"

#: ../../source/user_guide/release_notes.md:432
msgid ""
"Fixed the accuracy issue on Qwen series caused by enabling "
"`enable_shared_pert_dp` by default. [#2457](https://github.com/vllm-"
"project/vllm-ascend/pull/2457)"
msgstr ""
"修复了因默认启用 `enable_shared_pert_dp` 导致的 Qwen 系列模型的精度问题。[#2457](https://github.com/vllm-project/vllm-ascend/pull/2457)"

#: ../../source/user_guide/release_notes.md:433
msgid ""
"Fixed the accuracy issue on models whose rope dim is not equal to head "
"dim, e.g., GLM4.5. [#2601](https://github.com/vllm-project/vllm-"
"ascend/pull/2601)"
msgstr ""
"修复了在 rope 维度不等于头维度的模型（例如 GLM4.5）上的精度问题。[#2601](https://github.com/vllm-project/vllm-ascend/pull/2601)"

#: ../../source/user_guide/release_notes.md:434
#: ../../source/user_guide/release_notes.md:564
msgid "Performance improved through a lot of prs:"
msgstr "通过大量 PR 提升了性能："

#: ../../source/user_guide/release_notes.md:435
msgid ""
"Removed torch.cat and replaced it with List[0]. "
"[#2153](https://github.com/vllm-project/vllm-ascend/pull/2153)"
msgstr ""
"移除了 torch.cat 并用 List[0] 替代。[#2153](https://github.com/vllm-project/vllm-ascend/pull/2153)"

#: ../../source/user_guide/release_notes.md:436
msgid ""
"Converted the format of gmm to nz. [#2474](https://github.com/vllm-"
"project/vllm-ascend/pull/2474)"
msgstr ""
"将 gmm 的格式转换为 nz。[#2474](https://github.com/vllm-project/vllm-ascend/pull/2474)"

#: ../../source/user_guide/release_notes.md:437
msgid ""
"Optimized parallel strategies to reduce communication overhead. "
"[#2198](https://github.com/vllm-project/vllm-ascend/pull/2198)"
msgstr ""
"优化了并行策略以减少通信开销。[#2198](https://github.com/vllm-project/vllm-ascend/pull/2198)"

#: ../../source/user_guide/release_notes.md:438
msgid ""
"Optimized reject sampler in greedy situation. [#2137](https://github.com"
"/vllm-project/vllm-ascend/pull/2137)"
msgstr ""
"优化了贪婪情况下的拒绝采样器。[#2137](https://github.com/vllm-project/vllm-ascend/pull/2137)"

#: ../../source/user_guide/release_notes.md:439
msgid "A batch of refactoring PRs to enhance the code architecture:"
msgstr "一系列重构 PR 以增强代码架构："

#: ../../source/user_guide/release_notes.md:440
msgid ""
"Refactor on MLA. [#2465](https://github.com/vllm-project/vllm-"
"ascend/pull/2465)"
msgstr ""
"重构 MLA。[#2465](https://github.com/vllm-project/vllm-ascend/pull/2465)"

#: ../../source/user_guide/release_notes.md:441
msgid ""
"Refactor on torchair fused_moe. [#2438](https://github.com/vllm-project"
"/vllm-ascend/pull/2438)"
msgstr ""
"重构 torchair fused_moe。[#2438](https://github.com/vllm-project/vllm-ascend/pull/2438)"

#: ../../source/user_guide/release_notes.md:442
msgid ""
"Refactor on allgather/mc2-related fused_experts. "
"[#2369](https://github.com/vllm-project/vllm-ascend/pull/2369)"
msgstr ""
"重构 allgather/mc2 相关的 fused_experts。[#2369](https://github.com/vllm-project/vllm-ascend/pull/2369)"

#: ../../source/user_guide/release_notes.md:443
msgid ""
"Refactor on torchair model runner. [#2208](https://github.com/vllm-"
"project/vllm-ascend/pull/2208)"
msgstr ""
"重构 torchair model runner。[#2208](https://github.com/vllm-project/vllm-ascend/pull/2208)"

#: ../../source/user_guide/release_notes.md:444
msgid ""
"Refactor on CI. [#2276](https://github.com/vllm-project/vllm-"
"ascend/pull/2276)"
msgstr ""
"重构 CI。[#2276](https://github.com/vllm-project/vllm-ascend/pull/2276)"

#: ../../source/user_guide/release_notes.md:445
#: ../../source/user_guide/release_notes.md:579
msgid "Parameters changes:"
msgstr "参数变更："

#: ../../source/user_guide/release_notes.md:446
msgid ""
"Added `lmhead_tensor_parallel_size` in `additional_config`, set it to "
"enable lmhead tensor parallel. [#2309](https://github.com/vllm-project"
"/vllm-ascend/pull/2309)"
msgstr ""
"在 `additional_config` 中增加了 `lmhead_tensor_parallel_size` 参数，设置它以启用 lmhead 张量并行。[#2309](https://github.com/vllm-project/vllm-ascend/pull/2309)"

#: ../../source/user_guide/release_notes.md:447
msgid ""
"Some unused environment variables `HCCN_PATH`, `PROMPT_DEVICE_ID`, "
"`DECODE_DEVICE_ID`, `LLMDATADIST_COMM_PORT` and "
"`LLMDATADIST_SYNC_CACHE_WAIT_TIME`  are removed. "
"[#2448](https://github.com/vllm-project/vllm-ascend/pull/2448)"
msgstr ""
"移除了未使用的环境变量 `HCCN_PATH`、`PROMPT_DEVICE_ID`、`DECODE_DEVICE_ID`、`LLMDATADIST_COMM_PORT` 和 `LLMDATADIST_SYNC_CACHE_WAIT_TIME`。[#2448](https://github.com/vllm-project/vllm-ascend/pull/2448)"

#: ../../source/user_guide/release_notes.md:448
msgid ""
"Environment variable `VLLM_LLMDD_RPC_PORT` is renamed to "
"`VLLM_ASCEND_LLMDD_RPC_PORT` now. [#2450](https://github.com/vllm-project"
"/vllm-ascend/pull/2450)"
msgstr ""
"环境变量 `VLLM_LLMDD_RPC_PORT` 现已重命名为 `VLLM_ASCEND_LLMDD_RPC_PORT`。[#2450](https://github.com/vllm-project/vllm-ascend/pull/2450)"

#: ../../source/user_guide/release_notes.md:449
msgid ""
"Added `VLLM_ASCEND_ENABLE_MLP_OPTIMIZE` in environment variables, Whether"
" to enable mlp optimize when tensor parallel is enabled. This feature "
"provides better performance in eager mode. [#2120](https://github.com"
"/vllm-project/vllm-ascend/pull/2120)"
msgstr ""
"在环境变量中增加了 `VLLM_ASCEND_ENABLE_MLP_OPTIMIZE`，用于控制在启用张量并行时是否启用 MLP 优化。此功能在 eager 模式下能提供更好的性能。[#2120](https://github.com/vllm-project/vllm-ascend/pull/2120)"

#: ../../source/user_guide/release_notes.md:450
msgid ""
"Removed `MOE_ALL2ALL_BUFFER` and `VLLM_ASCEND_ENABLE_MOE_ALL2ALL_SEQ` in "
"environment variables. [#2612](https://github.com/vllm-project/vllm-"
"ascend/pull/2612)"
msgstr ""
"移除了环境变量中的 `MOE_ALL2ALL_BUFFER` 和 `VLLM_ASCEND_ENABLE_MOE_ALL2ALL_SEQ`。[#2612](https://github.com/vllm-project/vllm-ascend/pull/2612)"

#: ../../source/user_guide/release_notes.md:451
msgid ""
"Added `enable_prefetch` in `additional_config`, Whether to enable weight "
"prefetch. [#2465](https://github.com/vllm-project/vllm-ascend/pull/2465)"
msgstr ""
"在 `additional_config` 中增加了 `enable_prefetch` 参数，用于控制是否启用权重预取。[#2465](https://github.com/vllm-project/vllm-ascend/pull/2465)"

#: ../../source/user_guide/release_notes.md:452
msgid ""
"Added `mode` in `additional_config.torchair_graph_config`, When using "
"reduce-overhead mode for torchair, mode needs to be set. "
"[#2461](https://github.com/vllm-project/vllm-ascend/pull/2461)"
msgstr ""
"在 `additional_config.torchair_graph_config` 中增加了 `mode` 参数，当为 torchair 使用 reduce-overhead 模式时，需要设置此参数。[#2461](https://github.com/vllm-project/vllm-ascend/pull/2461)"

#: ../../source/user_guide/release_notes.md:453
msgid ""
"`enable_shared_expert_dp` in `additional_config` is disabled by default "
"now, and it is recommended to be enabled when inferencing with deepseek. "
"[#2457](https://github.com/vllm-project/vllm-ascend/pull/2457)"
msgstr ""
"`additional_config` 中的 `enable_shared_expert_dp` 参数现在默认禁用，建议在使用 deepseek 进行推理时启用它。[#2457](https://github.com/vllm-project/vllm-ascend/pull/2457)"
msgid ""
"Sliding window attention not support chunked prefill currently, thus we "
"could only enable AscendScheduler to run with it. "
"[#2729](https://github.com/vllm-project/vllm-ascend/issues/2729)"
msgstr ""
"滑动窗口注意力目前不支持分块预填充，因此我们只能启用 AscendScheduler 来运行它。"
"[#2729](https://github.com/vllm-project/vllm-ascend/issues/2729)"

#: ../../source/user_guide/release_notes.md:458
msgid ""
"There is a bug with creating mc2_mask when MultiStream is enabled, will "
"fix it in next release. [#2681](https://github.com/vllm-project/vllm-"
"ascend/pull/2681)"
msgstr ""
"启用 MultiStream 时创建 mc2_mask 存在一个 bug，将在下一个版本中修复。"
"[#2681](https://github.com/vllm-project/vllm-ascend/pull/2681)"

#: ../../source/user_guide/release_notes.md:460
msgid "v0.9.1 - 2025.09.03"
msgstr "v0.9.1 - 2025.09.03"

#: ../../source/user_guide/release_notes.md:462
msgid ""
"We are excited to announce the newest official release of vLLM Ascend. "
"This release includes many feature supports, performance improvements and"
" bug fixes. We recommend users to upgrade from 0.7.3 to this version. "
"Please always set `VLLM_USE_V1=1` to use V1 engine."
msgstr ""
"我们很高兴地宣布 vLLM Ascend 的最新正式版本。此版本包含许多功能支持、性能改进和错误修复。我们建议用户从 0.7.3 版本升级到此版本。请始终设置 `VLLM_USE_V1=1` 以使用 V1 引擎。"

#: ../../source/user_guide/release_notes.md:464
msgid ""
"In this release, we added many enhancements for large scale expert "
"parallel case. It's recommended to follow the [official "
"guide](https://docs.vllm.ai/projects/ascend/en/v0.9.1/tutorials/large_scale_ep.html)."
msgstr ""
"在此版本中，我们为大规模专家并行场景添加了许多增强功能。建议遵循[官方指南](https://docs.vllm.ai/projects/ascend/en/v0.9.1/tutorials/large_scale_ep.html)。"

#: ../../source/user_guide/release_notes.md:466
msgid ""
"Please note that this release note will list all the important changes "
"from last official release(v0.7.3)"
msgstr ""
"请注意，本发布说明将列出自上一个正式版本 (v0.7.3) 以来的所有重要变更。"

#: ../../source/user_guide/release_notes.md:470
msgid ""
"DeepSeek V3/R1 is supported with high quality and performance. MTP can "
"work with DeepSeek as well. Please refer to [muliti node "
"tutorials](https://docs.vllm.ai/projects/ascend/en/v0.9.1/tutorials/multi_node.html)"
" and [Large Scale Expert "
"Parallelism](https://docs.vllm.ai/projects/ascend/en/v0.9.1/tutorials/large_scale_ep.html)."
msgstr ""
"高质量、高性能地支持 DeepSeek V3/R1。MTP 也可与 DeepSeek 协同工作。请参考[多节点教程](https://docs.vllm.ai/projects/ascend/en/v0.9.1/tutorials/multi_node.html)和[大规模专家并行](https://docs.vllm.ai/projects/ascend/en/v0.9.1/tutorials/large_scale_ep.html)。"

#: ../../source/user_guide/release_notes.md:471
msgid ""
"Qwen series models work with graph mode now. It works by default with V1 "
"Engine. Please refer to [Qwen "
"tutorials](https://docs.vllm.ai/projects/ascend/en/v0.9.1/tutorials/index.html)."
msgstr ""
"Qwen 系列模型现在支持图模式。默认与 V1 引擎配合工作。请参考 [Qwen 教程](https://docs.vllm.ai/projects/ascend/en/v0.9.1/tutorials/index.html)。"

#: ../../source/user_guide/release_notes.md:472
msgid ""
"Disaggregated Prefilling support for V1 Engine. Please refer to [Large "
"Scale Expert "
"Parallelism](https://docs.vllm.ai/projects/ascend/en/v0.9.1/tutorials/large_scale_ep.html)"
" tutorials."
msgstr ""
"V1 引擎支持解耦预填充。请参考[大规模专家并行](https://docs.vllm.ai/projects/ascend/en/v0.9.1/tutorials/large_scale_ep.html)教程。"

#: ../../source/user_guide/release_notes.md:473
msgid "Automatic prefix caching and chunked prefill feature is supported."
msgstr "支持自动前缀缓存和分块预填充功能。"

#: ../../source/user_guide/release_notes.md:474
msgid "Speculative decoding feature works with Ngram and MTP method."
msgstr "推测解码功能支持 Ngram 和 MTP 方法。"

#: ../../source/user_guide/release_notes.md:475
msgid ""
"MOE and dense w4a8 quantization support now. Please refer to "
"[quantization "
"guide](https://docs.vllm.ai/projects/ascend/en/v0.9.1/user_guide/feature_guide/quantization.html)."
msgstr ""
"现已支持 MOE 和密集模型的 w4a8 量化。请参考[量化指南](https://docs.vllm.ai/projects/ascend/en/v0.9.1/user_guide/feature_guide/quantization.html)。"

#: ../../source/user_guide/release_notes.md:476
msgid ""
"Sleep Mode feature is supported for V1 engine. Please refer to [Sleep "
"mode "
"tutorials](https://docs.vllm.ai/projects/ascend/en/v0.9.1/user_guide/feature_guide/sleep_mode.html)."
msgstr ""
"V1 引擎支持睡眠模式功能。请参考[睡眠模式教程](https://docs.vllm.ai/projects/ascend/en/v0.9.1/user_guide/feature_guide/sleep_mode.html)。"

#: ../../source/user_guide/release_notes.md:477
msgid ""
"Dynamic and Static EPLB support is added. This feature is still "
"experimental."
msgstr "已添加动态和静态 EPLB 支持。此功能仍处于实验阶段。"

#: ../../source/user_guide/release_notes.md:479
msgid "Note"
msgstr "注意"

#: ../../source/user_guide/release_notes.md:481
msgid ""
"The following notes are especially for reference when upgrading from last"
" final release (v0.7.3):"
msgstr "以下说明特别适用于从上一个最终版本 (v0.7.3) 升级时参考："

#: ../../source/user_guide/release_notes.md:483
msgid ""
"V0 Engine is not supported from this release. Please always set "
"`VLLM_USE_V1=1` to use V1 engine with vLLM Ascend."
msgstr "自本版本起不再支持 V0 引擎。请始终设置 `VLLM_USE_V1=1` 以在 vLLM Ascend 中使用 V1 引擎。"

#: ../../source/user_guide/release_notes.md:484
msgid ""
"Mindie Turbo is not needed with this release. And the old version of "
"Mindie Turbo is not compatible. Please do not install it. Currently all "
"the function and enhancement is included in vLLM Ascend already. We'll "
"consider to add it back in the future in needed."
msgstr "本版本不再需要 Mindie Turbo。且旧版本的 Mindie Turbo 不兼容。请不要安装它。目前所有功能和增强已包含在 vLLM Ascend 中。我们将在未来根据需要考虑重新添加它。"

#: ../../source/user_guide/release_notes.md:485
msgid ""
"Torch-npu is upgraded to 2.5.1.post1. CANN is upgraded to 8.2.RC1. Don't "
"forget to upgrade them."
msgstr "Torch-npu 已升级至 2.5.1.post1。CANN 已升级至 8.2.RC1。请勿忘记升级它们。"

#: ../../source/user_guide/release_notes.md:489
msgid ""
"The Ascend scheduler is added for V1 engine. This scheduler is more "
"affine with Ascend hardware."
msgstr "为 V1 引擎新增了 Ascend 调度器。该调度器与 Ascend 硬件更加适配。"

#: ../../source/user_guide/release_notes.md:490
msgid "Structured output feature works now on V1 Engine."
msgstr "结构化输出功能现已在 V1 引擎上可用。"

#: ../../source/user_guide/release_notes.md:491
msgid "A batch of custom ops are added to improve the performance."
msgstr "添加了一批自定义算子以提升性能。"

#: ../../source/user_guide/release_notes.md:493
msgid "Changes"
msgstr "变更"

#: ../../source/user_guide/release_notes.md:495
msgid ""
"EPLB support for Qwen3-moe model. [#2000](https://github.com/vllm-project"
"/vllm-ascend/pull/2000)"
msgstr "为 Qwen3-moe 模型添加 EPLB 支持。[#2000](https://github.com/vllm-project/vllm-ascend/pull/2000)"

#: ../../source/user_guide/release_notes.md:496
msgid ""
"Fix the bug that MTP doesn't work well with Prefill Decode "
"Disaggregation. [#2610](https://github.com/vllm-project/vllm-"
"ascend/pull/2610) [#2554](https://github.com/vllm-project/vllm-"
"ascend/pull/2554) [#2531](https://github.com/vllm-project/vllm-"
"ascend/pull/2531)"
msgstr "修复 MTP 与预填充解码解耦功能配合不佳的 bug。[#2610](https://github.com/vllm-project/vllm-ascend/pull/2610) [#2554](https://github.com/vllm-project/vllm-ascend/pull/2554) [#2531](https://github.com/vllm-project/vllm-ascend/pull/2531)"

#: ../../source/user_guide/release_notes.md:497
msgid ""
"Fix few bugs to make sure Prefill Decode Disaggregation works well. "
"[#2538](https://github.com/vllm-project/vllm-ascend/pull/2538) "
"[#2509](https://github.com/vllm-project/vllm-ascend/pull/2509) "
"[#2502](https://github.com/vllm-project/vllm-ascend/pull/2502)"
msgstr "修复若干 bug 以确保预填充解码解耦功能正常工作。[#2538](https://github.com/vllm-project/vllm-ascend/pull/2538) [#2509](https://github.com/vllm-project/vllm-ascend/pull/2509) [#2502](https://github.com/vllm-project/vllm-ascend/pull/2502)"

#: ../../source/user_guide/release_notes.md:498
msgid ""
"Fix file not found error with shutil.rmtree in torchair mode. "
"[#2506](https://github.com/vllm-project/vllm-ascend/pull/2506)"
msgstr "修复 torchair 模式下 shutil.rmtree 的文件未找到错误。[#2506](https://github.com/vllm-project/vllm-ascend/pull/2506)"

#: ../../source/user_guide/release_notes.md:502
msgid ""
"When running MoE model, Aclgraph mode only work with tensor parallel. "
"DP/EP doesn't work in this release."
msgstr "运行 MoE 模型时，Aclgraph 模式仅支持张量并行。DP/EP 在本版本中不可用。"

#: ../../source/user_guide/release_notes.md:503
msgid "Pipeline parallelism is not supported in this release for V1 engine."
msgstr "本版本中 V1 引擎不支持流水线并行。"

#: ../../source/user_guide/release_notes.md:504
msgid ""
"If you use w4a8 quantization with eager mode, please set "
"`VLLM_ASCEND_MLA_PARALLEL=1` to avoid oom error."
msgstr "如果在 eager 模式下使用 w4a8 量化，请设置 `VLLM_ASCEND_MLA_PARALLEL=1` 以避免内存不足错误。"

#: ../../source/user_guide/release_notes.md:505
msgid ""
"Accuracy test with some tools may not be correct. It doesn't affect the "
"real user case. We'll fix it in the next post release. "
"[#2654](https://github.com/vllm-project/vllm-ascend/pull/2654)"
msgstr "使用某些工具进行的精度测试可能不准确。这不影响实际用户场景。我们将在下一个后续版本中修复此问题。[#2654](https://github.com/vllm-project/vllm-ascend/pull/2654)"

#: ../../source/user_guide/release_notes.md:506
msgid ""
"We notice that there are still some problems when running vLLM Ascend "
"with Prefill Decode Disaggregation. For example, the memory may be leaked"
" and the service may be stuck. It's caused by known issue by vLLM and "
"vLLM Ascend. We'll fix it in the next post release. "
"[#2650](https://github.com/vllm-project/vllm-ascend/pull/2650) "
"[#2604](https://github.com/vllm-project/vllm-ascend/pull/2604) "
"[vLLM#22736](https://github.com/vllm-project/vllm/pull/22736) "
"[vLLM#23554](https://github.com/vllm-project/vllm/pull/23554) "
"[vLLM#23981](https://github.com/vllm-project/vllm/pull/23981)"
msgstr "我们注意到，在使用预填充解码解耦功能运行 vLLM Ascend 时仍存在一些问题。例如，可能出现内存泄漏或服务卡住。这是由 vLLM 和 vLLM Ascend 的已知问题引起的。我们将在下一个后续版本中修复此问题。[#2650](https://github.com/vllm-project/vllm-ascend/pull/2650) [#2604](https://github.com/vllm-project/vllm-ascend/pull/2604) [vLLM#22736](https://github.com/vllm-project/vllm/pull/22736) [vLLM#23554](https://github.com/vllm-project/vllm/pull/23554) [vLLM#23981](https://github.com/vllm-project/vllm/pull/23981)"

#: ../../source/user_guide/release_notes.md:508
msgid "v0.9.1rc3 - 2025.08.22"
msgstr "v0.9.1rc3 - 2025.08.22"

#: ../../source/user_guide/release_notes.md:510
msgid ""
"This is the 3rd release candidate of v0.9.1 for vLLM Ascend. Please "
"follow the [official "
"doc](https://docs.vllm.ai/projects/ascend/en/v0.9.1/) to get started."
msgstr "这是 vLLM Ascend v0.9.1 的第三个候选发布版本。请按照[官方文档](https://docs.vllm.ai/projects/ascend/en/v0.9.1/)开始使用。"

#: ../../source/user_guide/release_notes.md:514
msgid ""
"MTP supports V1 scheduler [#2371](https://github.com/vllm-project/vllm-"
"ascend/pull/2371)"
msgstr "MTP 支持 V1 调度器 [#2371](https://github.com/vllm-project/vllm-ascend/pull/2371)"

#: ../../source/user_guide/release_notes.md:515
msgid ""
"Add LMhead TP communication groups [#1956](https://github.com/vllm-"
"project/vllm-ascend/pull/1956)"
msgstr "添加 LMhead TP 通信组 [#1956](https://github.com/vllm-project/vllm-ascend/pull/1956)"

#: ../../source/user_guide/release_notes.md:516
msgid ""
"Fix the bug that qwen3 moe doesn't work with aclgraph "
"[#2478](https://github.com/vllm-project/vllm-ascend/pull/2478)"
msgstr "修复 qwen3 moe 无法与 aclgraph 协同工作的 bug [#2478](https://github.com/vllm-project/vllm-ascend/pull/2478)"

#: ../../source/user_guide/release_notes.md:517
msgid ""
"Fix `grammar_bitmask` IndexError caused by outdated "
"`apply_grammar_bitmask` method [#2314](https://github.com/vllm-project"
"/vllm-ascend/pull/2314)"
msgstr "修复因过时的 `apply_grammar_bitmask` 方法导致的 `grammar_bitmask` IndexError [#2314](https://github.com/vllm-project/vllm-ascend/pull/2314)"

#: ../../source/user_guide/release_notes.md:518
msgid ""
"Remove `chunked_prefill_for_mla` [#2177](https://github.com/vllm-project"
"/vllm-ascend/pull/2177)"
msgstr "移除 `chunked_prefill_for_mla` [#2177](https://github.com/vllm-project/vllm-ascend/pull/2177)"

#: ../../source/user_guide/release_notes.md:519
msgid ""
"Fix bugs and refactor cached mask generation logic "
"[#2326](https://github.com/vllm-project/vllm-ascend/pull/2326)"
msgstr "修复 bug 并重构缓存掩码生成逻辑 [#2326](https://github.com/vllm-project/vllm-ascend/pull/2326)"
msgstr ""
"重构 AscendFusedMoE [#1229](https://github.com/vllm-project/vllm-"
"ascend/pull/1229)"

#: ../../source/user_guide/release_notes.md:520
msgid ""
"Fix configuration check logic about ascend scheduler "
"[#2327](https://github.com/vllm-project/vllm-ascend/pull/2327)"
msgstr ""
"修复关于 Ascend 调度器的配置检查逻辑 [#2327](https://github.com/vllm-project/vllm-"
"ascend/pull/2327)"

#: ../../source/user_guide/release_notes.md:521
msgid ""
"Cancel the verification between deepseek-mtp and non-ascend scheduler in "
"disaggregated-prefill deployment [#2368](https://github.com/vllm-project"
"/vllm-ascend/pull/2368)"
msgstr ""
"在解耦式预填充部署中取消 deepseek-mtp 与非 Ascend 调度器之间的验证 [#2368](https://github.com/vllm-"
"project/vllm-ascend/pull/2368)"

#: ../../source/user_guide/release_notes.md:522
msgid ""
"Fix issue that failed with ray distributed backend "
"[#2306](https://github.com/vllm-project/vllm-ascend/pull/2306)"
msgstr ""
"修复使用 ray 分布式后端时失败的问题 [#2306](https://github.com/vllm-project/vllm-"
"ascend/pull/2306)"

#: ../../source/user_guide/release_notes.md:523
msgid ""
"Fix incorrect req block length in ascend scheduler "
"[#2394](https://github.com/vllm-project/vllm-ascend/pull/2394)"
msgstr ""
"修复 Ascend 调度器中请求块长度不正确的问题 [#2394](https://github.com/vllm-project/vllm-"
"ascend/pull/2394)"

#: ../../source/user_guide/release_notes.md:524
msgid ""
"Fix header include issue in rope [#2398](https://github.com/vllm-project"
"/vllm-ascend/pull/2398)"
msgstr ""
"修复 rope 中的头文件包含问题 [#2398](https://github.com/vllm-project/vllm-"
"ascend/pull/2398)"

#: ../../source/user_guide/release_notes.md:525
msgid ""
"Fix mtp config bug [#2412](https://github.com/vllm-project/vllm-"
"ascend/pull/2412)"
msgstr ""
"修复 mtp 配置错误 [#2412](https://github.com/vllm-project/vllm-"
"ascend/pull/2412)"

#: ../../source/user_guide/release_notes.md:526
msgid ""
"Fix error info and adapt `attn_metedata` refactor "
"[#2402](https://github.com/vllm-project/vllm-ascend/pull/2402)"
msgstr ""
"修复错误信息并适配 `attn_metedata` 重构 [#2402](https://github.com/vllm-project/vllm-"
"ascend/pull/2402)"

#: ../../source/user_guide/release_notes.md:527
msgid ""
"Fix torchair runtime error caused by configuration mismtaches and "
"`.kv_cache_bytes` file missing [#2312](https://github.com/vllm-project"
"/vllm-ascend/pull/2312)"
msgstr ""
"修复因配置不匹配和 `.kv_cache_bytes` 文件缺失导致的 torchair 运行时错误 [#2312](https://github.com/vllm-"
"project/vllm-ascend/pull/2312)"

#: ../../source/user_guide/release_notes.md:528
msgid ""
"Move `with_prefill` allreduce from cpu to npu [#2230](https://github.com"
"/vllm-project/vllm-ascend/pull/2230)"
msgstr ""
"将 `with_prefill` 的 allreduce 操作从 CPU 移至 NPU [#2230](https://github.com/vllm-"
"project/vllm-ascend/pull/2230)"

#: ../../source/user_guide/release_notes.md:530
#: ../../source/user_guide/release_notes.md:694
#: ../../source/user_guide/release_notes.md:876
msgid "Docs"
msgstr "文档"

#: ../../source/user_guide/release_notes.md:532
msgid ""
"Add document for deepseek large EP [#2339](https://github.com/vllm-"
"project/vllm-ascend/pull/2339)"
msgstr ""
"添加 deepseek 大 EP 相关文档 [#2339](https://github.com/vllm-project/vllm-"
"ascend/pull/2339)"

#: ../../source/user_guide/release_notes.md:536
msgid ""
"`test_aclgraph.py` failed with `\"full_cuda_graph\": True` on A2 (910B1) "
"[#2182](https://github.com/vllm-project/vllm-ascend/issues/2182)"
msgstr ""
"在 A2 (910B1) 上，当 `\"full_cuda_graph\": True` 时 `test_aclgraph.py` 测试失败 "
"[#2182](https://github.com/vllm-project/vllm-ascend/issues/2182)"

#: ../../source/user_guide/release_notes.md:538
msgid "v0.10.0rc1 - 2025.08.07"
msgstr "v0.10.0rc1 - 2025.08.07"

#: ../../source/user_guide/release_notes.md:540
msgid ""
"This is the 1st release candidate of v0.10.0 for vLLM Ascend. Please "
"follow the [official doc](https://docs.vllm.ai/projects/ascend/en/) to "
"get started. V0 is completely removed from this version."
msgstr ""
"这是 vLLM Ascend v0.10.0 的第一个候选发布版本。请按照[官方文档](https://docs.vllm.ai/projects/ascend/en/)开始使用。V0 已在此版本中完全移除。"

#: ../../source/user_guide/release_notes.md:544
msgid ""
"Disaggregate prefill works with V1 engine now. You can take a try with "
"DeepSeek model [#950](https://github.com/vllm-project/vllm-"
"ascend/pull/950), following this [tutorial](https://github.com/vllm-"
"project/vllm-"
"ascend/blob/main/examples/disaggregated_prefill_v1/README.md)."
msgstr ""
"解耦式预填充现已支持 V1 引擎。您可以尝试使用 DeepSeek 模型 [#950](https://github.com/vllm-project/vllm-"
"ascend/pull/950)，并遵循此[教程](https://github.com/vllm-project/vllm-"
"ascend/blob/main/examples/disaggregated_prefill_v1/README.md)。"

#: ../../source/user_guide/release_notes.md:545
msgid ""
"W4A8 quantization method is supported for dense and MoE model now. "
"[#2060](https://github.com/vllm-project/vllm-ascend/pull/2060) "
"[#2172](https://github.com/vllm-project/vllm-ascend/pull/2172)"
msgstr ""
"现在已为稠密模型和 MoE 模型支持 W4A8 量化方法。 [#2060](https://github.com/vllm-project/vllm-"
"ascend/pull/2060) [#2172](https://github.com/vllm-project/vllm-ascend/pull/2172)"

#: ../../source/user_guide/release_notes.md:549
msgid ""
"Ascend PyTorch adapter (torch_npu) has been upgraded to "
"`2.7.1.dev20250724`. [#1562](https://github.com/vllm-project/vllm-"
"ascend/pull/1562) And CANN hase been upgraded to `8.2.RC1`. "
"[#1653](https://github.com/vllm-project/vllm-ascend/pull/1653) Don’t "
"forget to update them in your environment or using the latest images."
msgstr ""
"Ascend PyTorch 适配器 (torch_npu) 已升级至 `2.7.1.dev20250724`。 [#1562](https://github.com/vllm-"
"project/vllm-ascend/pull/1562) 并且 CANN 已升级至 `8.2.RC1`。 [#1653](https://github.com/vllm-"
"project/vllm-ascend/pull/1653) 请勿忘记在您的环境中更新它们或使用最新的镜像。"

#: ../../source/user_guide/release_notes.md:550
msgid ""
"vLLM Ascend works on Atlas 800I A3 now, and the image on A3 will be "
"released from this version on. [#1582](https://github.com/vllm-project"
"/vllm-ascend/pull/1582)"
msgstr ""
"vLLM Ascend 现可在 Atlas 800I A3 上运行，A3 的镜像将从此版本开始发布。 [#1582](https://github.com/vllm-"
"project/vllm-ascend/pull/1582)"

#: ../../source/user_guide/release_notes.md:551
msgid ""
"Kimi-K2 with w8a8 quantization, Qwen3-Coder and GLM-4.5 is supported in "
"vLLM Ascend, please following this "
"[tutorial](https://docs.vllm.ai/projects/ascend/en/latest/tutorials/multi_node_kimi.md.html)"
" to have a try. [#2162](https://github.com/vllm-project/vllm-"
"ascend/pull/2162)"
msgstr ""
"vLLM Ascend 现已支持 w8a8 量化的 Kimi-K2、Qwen3-Coder 和 GLM-4.5，请按照此[教程](https://docs.vllm.ai/projects/ascend/en/latest/tutorials/multi_node_kimi.md.html)进行尝试。 [#2162](https://github.com/vllm-project/vllm-ascend/pull/2162)"

#: ../../source/user_guide/release_notes.md:552
msgid ""
"Pipeline Parallelism is supported in V1 now. [#1800](https://github.com"
"/vllm-project/vllm-ascend/pull/1800)"
msgstr ""
"V1 引擎现已支持流水线并行。 [#1800](https://github.com/vllm-project/vllm-"
"ascend/pull/1800)"

#: ../../source/user_guide/release_notes.md:553
msgid ""
"Prefix cache feature now work with the Ascend Scheduler. "
"[#1446](https://github.com/vllm-project/vllm-ascend/pull/1446)"
msgstr ""
"前缀缓存功能现在可与 Ascend 调度器协同工作。 [#1446](https://github.com/vllm-project/vllm-"
"ascend/pull/1446)"

#: ../../source/user_guide/release_notes.md:554
msgid ""
"Torchair graph mode works with tp > 4 now. [#1508](https://github.com"
"/vllm-project/vllm-ascend/issues/1508)"
msgstr ""
"Torchair 图模式现在支持 tp > 4。 [#1508](https://github.com/vllm-project/vllm-"
"ascend/issues/1508)"

#: ../../source/user_guide/release_notes.md:555
msgid ""
"MTP support torchair graph mode now [#2145](https://github.com/vllm-"
"project/vllm-ascend/pull/2145)"
msgstr ""
"MTP 现在支持 torchair 图模式 [#2145](https://github.com/vllm-project/vllm-"
"ascend/pull/2145)"

#: ../../source/user_guide/release_notes.md:560
msgid ""
"Fix functional problem of multi-modality models like Qwen2-audio with "
"Aclgraph. [#1803](https://github.com/vllm-project/vllm-ascend/pull/1803)"
msgstr ""
"修复了 Qwen2-audio 等多模态模型与 Aclgraph 的功能性问题。 [#1803](https://github.com/vllm-"
"project/vllm-ascend/pull/1803)"

#: ../../source/user_guide/release_notes.md:561
msgid ""
"Fix the process group creating error with external launch scenario. "
"[#1681](https://github.com/vllm-project/vllm-ascend/pull/1681)"
msgstr ""
"修复了外部启动场景下的进程组创建错误。 [#1681](https://github.com/vllm-project/vllm-"
"ascend/pull/1681)"

#: ../../source/user_guide/release_notes.md:562
msgid ""
"Fix the functional problem with guided decoding. "
"[#2022](https://github.com/vllm-project/vllm-ascend/pull/2022)"
msgstr ""
"修复了引导式解码的功能性问题。 [#2022](https://github.com/vllm-project/vllm-"
"ascend/pull/2022)"

#: ../../source/user_guide/release_notes.md:563
msgid ""
"Fix the accuracy issue with common MoE models in DP scenario. "
"[#1856](https://github.com/vllm-project/vllm-ascend/pull/1856)"
msgstr ""
"修复了在 DP 场景下常见 MoE 模型的准确性问题。 [#1856](https://github.com/vllm-project/vllm-"
"ascend/pull/1856)"

#: ../../source/user_guide/release_notes.md:565
msgid ""
"Caching sin/cos instead of calculate it every layer. "
"[#1890](https://github.com/vllm-project/vllm-ascend/pull/1890)"
msgstr ""
"缓存 sin/cos 值，而非每层计算。 [#1890](https://github.com/vllm-project/vllm-"
"ascend/pull/1890)"

#: ../../source/user_guide/release_notes.md:566
msgid ""
"Improve shared expert multi-stream parallelism [#1891](https://github.com"
"/vllm-project/vllm-ascend/pull/1891)"
msgstr ""
"改进共享专家的多流并行 [#1891](https://github.com/vllm-project/vllm-"
"ascend/pull/1891)"

#: ../../source/user_guide/release_notes.md:567
msgid ""
"Implement the fusion of allreduce and matmul in prefill phase when tp is "
"enabled. Enable this feature by setting "
"`VLLM_ASCEND_ENABLE_MATMUL_ALLREDUCE` to `1`. [#1926](https://github.com"
"/vllm-project/vllm-ascend/pull/1926)"
msgstr ""
"在启用 tp 时，实现了预填充阶段 allreduce 与 matmul 的融合。通过设置 `VLLM_ASCEND_ENABLE_MATMUL_ALLREDUCE` 为 `1` 来启用此功能。 [#1926](https://github.com/vllm-project/vllm-ascend/pull/1926)"

#: ../../source/user_guide/release_notes.md:568
msgid ""
"Optimize Quantized MoE Performance by Reducing All2All Communication. "
"[#2195](https://github.com/vllm-project/vllm-ascend/pull/2195)"
msgstr ""
"通过减少 All2All 通信来优化量化 MoE 性能。 [#2195](https://github.com/vllm-project/vllm-"
"ascend/pull/2195)"

#: ../../source/user_guide/release_notes.md:569
msgid ""
"Use AddRmsNormQuant ops in the custom model to optimize Qwen3's "
"performance [#1806](https://github.com/vllm-project/vllm-"
"ascend/pull/1806)"
msgstr ""
"在自定义模型中使用 AddRmsNormQuant 算子以优化 Qwen3 的性能 [#1806](https://github.com/vllm-"
"project/vllm-ascend/pull/1806)"

#: ../../source/user_guide/release_notes.md:570
msgid ""
"Use multicast to avoid padding decode request to prefill size "
"[#1555](https://github.com/vllm-project/vllm-ascend/pull/1555)"
msgstr ""
"使用组播以避免将解码请求填充至预填充大小 [#1555](https://github.com/vllm-project/vllm-"
"ascend/pull/1555)"

#: ../../source/user_guide/release_notes.md:571
msgid ""
msgid ""
"The performance of LoRA has been improved. [#1884](https://github.com"
"/vllm-project/vllm-ascend/pull/1884)"
msgstr ""
"LoRA 的性能已得到提升。[#1884](https://github.com/vllm-project/vllm-"
"ascend/pull/1884)"

#: ../../source/user_guide/release_notes.md:572
msgid "A batch of refactoring prs to enhance the code architecture:"
msgstr "一系列重构 PR 以增强代码架构："

#: ../../source/user_guide/release_notes.md:573
msgid ""
"Torchair model runner refactor [#2205](https://github.com/vllm-project"
"/vllm-ascend/pull/2205)"
msgstr ""
"重构 Torchair 模型运行器 [#2205](https://github.com/vllm-project/vllm-"
"ascend/pull/2205)"

#: ../../source/user_guide/release_notes.md:574
msgid ""
"Refactoring forward_context and model_runner_v1. "
"[#1979](https://github.com/vllm-project/vllm-ascend/pull/1979)"
msgstr ""
"重构 forward_context 和 model_runner_v1。 "
"[#1979](https://github.com/vllm-project/vllm-ascend/pull/1979)"

#: ../../source/user_guide/release_notes.md:575
msgid ""
"Refactor AscendMetaData Comments. [#1967](https://github.com/vllm-project"
"/vllm-ascend/pull/1967)"
msgstr ""
"重构 AscendMetaData 注释。 [#1967](https://github.com/vllm-project/vllm-"
"ascend/pull/1967)"

#: ../../source/user_guide/release_notes.md:576
msgid ""
"Refactor torchair utils. [#1892](https://github.com/vllm-project/vllm-"
"ascend/pull/1892)"
msgstr ""
"重构 torchair 工具集。 [#1892](https://github.com/vllm-project/vllm-"
"ascend/pull/1892)"

#: ../../source/user_guide/release_notes.md:577
msgid ""
"Refactor torchair worker. [#1885](https://github.com/vllm-project/vllm-"
"ascend/pull/1885)"
msgstr ""
"重构 torchair worker。 [#1885](https://github.com/vllm-project/vllm-"
"ascend/pull/1885)"

#: ../../source/user_guide/release_notes.md:578
msgid ""
"Register activation customop instead of overwrite forward_oot. "
"[#1841](https://github.com/vllm-project/vllm-ascend/pull/1841)"
msgstr ""
"注册激活自定义算子，而非覆盖 forward_oot。 "
"[#1841](https://github.com/vllm-project/vllm-ascend/pull/1841)"

#: ../../source/user_guide/release_notes.md:580
msgid ""
"`expert_tensor_parallel_size` in `additional_config` is removed now, and "
"the EP and TP is aligned with vLLM now. [#1681](https://github.com/vllm-"
"project/vllm-ascend/pull/1681)"
msgstr ""
"`additional_config` 中的 `expert_tensor_parallel_size` 现已被移除，EP 和 TP 现已与 vLLM 对齐。 [#1681](https://github.com/vllm-project/vllm-ascend/pull/1681)"

#: ../../source/user_guide/release_notes.md:581
msgid ""
"Add `VLLM_ASCEND_MLA_PA` in environ variables, use this to enable mla "
"paged attention operator for deepseek mla decode."
msgstr ""
"在环境变量中添加 `VLLM_ASCEND_MLA_PA`，用于为 deepseek mla 解码启用 mla 分页注意力算子。"

#: ../../source/user_guide/release_notes.md:582
msgid ""
"Add `VLLM_ASCEND_ENABLE_MATMUL_ALLREDUCE` in environ variables, enable "
"`MatmulAllReduce` fusion kernel when tensor parallel is enabled. This "
"feature is supported in A2, and eager mode will get better performance."
msgstr ""
"在环境变量中添加 `VLLM_ASCEND_ENABLE_MATMUL_ALLREDUCE`，当启用张量并行时启用 `MatmulAllReduce` 融合内核。此功能在 A2 中受支持，eager 模式将获得更好的性能。"

#: ../../source/user_guide/release_notes.md:583
msgid ""
"Add `VLLM_ASCEND_ENABLE_MOE_ALL2ALL_SEQ` in environ variables, Whether to"
" enable moe all2all seq, this provides a basic framework on the basis of "
"alltoall for easy expansion."
msgstr ""
"在环境变量中添加 `VLLM_ASCEND_ENABLE_MOE_ALL2ALL_SEQ`，用于决定是否启用 moe all2all seq，这提供了一个基于 alltoall 的基础框架以便于扩展。"

#: ../../source/user_guide/release_notes.md:585
msgid ""
"UT coverage reached 76.34% after a batch of prs followed by this rfc: "
"[#1298](https://github.com/vllm-project/vllm-ascend/issues/1298)"
msgstr ""
"在遵循此 RFC [#1298](https://github.com/vllm-project/vllm-ascend/issues/1298) 的一系列 PR 之后，单元测试覆盖率已达到 76.34%。"

#: ../../source/user_guide/release_notes.md:586
msgid ""
"Sequence Parallelism works for Qwen3 MoE. [#2209](https://github.com"
"/vllm-project/vllm-ascend/issues/2209)"
msgstr ""
"序列并行已可用于 Qwen3 MoE。 [#2209](https://github.com/vllm-project/vllm-ascend/issues/2209)"

#: ../../source/user_guide/release_notes.md:587
msgid ""
"Chinese online document is added now. [#1870](https://github.com/vllm-"
"project/vllm-ascend/issues/1870)"
msgstr ""
"现已添加中文在线文档。 [#1870](https://github.com/vllm-project/vllm-ascend/issues/1870)"

#: ../../source/user_guide/release_notes.md:591
msgid ""
"Aclgraph could not work with DP + EP currently, the mainly gap is the "
"number of npu stream that Aclgraph needed to capture graph is not enough."
" [#2229](https://github.com/vllm-project/vllm-ascend/issues/2229)"
msgstr ""
"Aclgraph 目前无法与 DP + EP 协同工作，主要差距在于 Aclgraph 捕获图所需的 NPU 流数量不足。 [#2229](https://github.com/vllm-project/vllm-ascend/issues/2229)"

#: ../../source/user_guide/release_notes.md:592
msgid ""
"There is an accuracy issue on W8A8 dynamic quantized DeepSeek with "
"multistream enabled. This will be fixed in the next release. "
"[#2232](https://github.com/vllm-project/vllm-ascend/issues/2232)"
msgstr ""
"启用多流时，W8A8 动态量化的 DeepSeek 存在精度问题。此问题将在下一个版本中修复。 [#2232](https://github.com/vllm-project/vllm-ascend/issues/2232)"

#: ../../source/user_guide/release_notes.md:593
msgid ""
"In Qwen3 MoE, SP cannot be incorporated into the Aclgraph. "
"[#2246](https://github.com/vllm-project/vllm-ascend/issues/2246)"
msgstr ""
"在 Qwen3 MoE 中，SP 无法集成到 Aclgraph 中。 [#2246](https://github.com/vllm-project/vllm-ascend/issues/2246)"

#: ../../source/user_guide/release_notes.md:594
msgid ""
"MTP not support V1 scheduler currently, will fix it in Q3. "
"[#2254](https://github.com/vllm-project/vllm-ascend/issues/2254)"
msgstr ""
"MTP 目前不支持 V1 调度器，将在 Q3 修复此问题。 [#2254](https://github.com/vllm-project/vllm-ascend/issues/2254)"

#: ../../source/user_guide/release_notes.md:595
msgid ""
"When running MTP with DP > 1, we need to disable metrics logger due to "
"some issue on vLLM. [#2254](https://github.com/vllm-project/vllm-"
"ascend/issues/2254)"
msgstr ""
"当以 DP > 1 运行 MTP 时，由于 vLLM 的某些问题，我们需要禁用指标记录器。 [#2254](https://github.com/vllm-project/vllm-ascend/issues/2254)"

#: ../../source/user_guide/release_notes.md:597
msgid "v0.9.1rc2 - 2025.08.04"
msgstr "v0.9.1rc2 - 2025.08.04"

#: ../../source/user_guide/release_notes.md:599
msgid ""
"This is the 2nd release candidate of v0.9.1 for vLLM Ascend. Please "
"follow the [official "
"doc](https://docs.vllm.ai/projects/ascend/en/v0.9.1/) to get started."
msgstr ""
"这是 vLLM Ascend v0.9.1 的第二个候选发布版本。请按照[官方文档](https://docs.vllm.ai/projects/ascend/en/v0.9.1/)开始使用。"

#: ../../source/user_guide/release_notes.md:603
msgid ""
"MOE and dense w4a8 quantization support now: [#1320](https://github.com"
"/vllm-project/vllm-ascend/pull/1320) [#1910](https://github.com/vllm-"
"project/vllm-ascend/pull/1910) [#1275](https://github.com/vllm-project"
"/vllm-ascend/pull/1275) [#1480](https://github.com/vllm-project/vllm-"
"ascend/pull/1480)"
msgstr ""
"现已支持 MOE 和稠密模型的 w4a8 量化： [#1320](https://github.com/vllm-project/vllm-ascend/pull/1320) [#1910](https://github.com/vllm-project/vllm-ascend/pull/1910) [#1275](https://github.com/vllm-project/vllm-ascend/pull/1275) [#1480](https://github.com/vllm-project/vllm-ascend/pull/1480)"

#: ../../source/user_guide/release_notes.md:604
msgid ""
"Dynamic EPLB support in [#1943](https://github.com/vllm-project/vllm-"
"ascend/pull/1943)"
msgstr ""
"在 [#1943](https://github.com/vllm-project/vllm-ascend/pull/1943) 中支持动态 EPLB"

#: ../../source/user_guide/release_notes.md:605
msgid ""
"Disaggregated Prefilling support for V1 Engine and improvement, continued"
" development and stabilization of the disaggregated prefill feature, "
"including performance enhancements and bug fixes for single-machine "
"setups:[#1953](https://github.com/vllm-project/vllm-ascend/pull/1953) "
"[#1612](https://github.com/vllm-project/vllm-ascend/pull/1612) "
"[#1361](https://github.com/vllm-project/vllm-ascend/pull/1361) "
"[#1746](https://github.com/vllm-project/vllm-ascend/pull/1746) "
"[#1552](https://github.com/vllm-project/vllm-ascend/pull/1552) "
"[#1801](https://github.com/vllm-project/vllm-ascend/pull/1801) "
"[#2083](https://github.com/vllm-project/vllm-ascend/pull/2083) "
"[#1989](https://github.com/vllm-project/vllm-ascend/pull/1989)"
msgstr ""
"支持 V1 引擎的解耦预填充及改进，持续开发和稳定解耦预填充功能，包括针对单机设置的性能增强和错误修复：[#1953](https://github.com/vllm-project/vllm-ascend/pull/1953) [#1612](https://github.com/vllm-project/vllm-ascend/pull/1612) [#1361](https://github.com/vllm-project/vllm-ascend/pull/1361) [#1746](https://github.com/vllm-project/vllm-ascend/pull/1746) [#1552](https://github.com/vllm-project/vllm-ascend/pull/1552) [#1801](https://github.com/vllm-project/vllm-ascend/pull/1801) [#2083](https://github.com/vllm-project/vllm-ascend/pull/2083) [#1989](https://github.com/vllm-project/vllm-ascend/pull/1989)"

#: ../../source/user_guide/release_notes.md:607
msgid "Model Improvement"
msgstr "模型改进"

#: ../../source/user_guide/release_notes.md:609
msgid ""
"DeepSeek DeepSeek DBO support and improvement: [#1285](https://github.com"
"/vllm-project/vllm-ascend/pull/1285) [#1291](https://github.com/vllm-"
"project/vllm-ascend/pull/1291) [#1328](https://github.com/vllm-project"
"/vllm-ascend/pull/1328) [#1420](https://github.com/vllm-project/vllm-"
"ascend/pull/1420) [#1445](https://github.com/vllm-project/vllm-"
"ascend/pull/1445) [#1589](https://github.com/vllm-project/vllm-"
"ascend/pull/1589) [#1759](https://github.com/vllm-project/vllm-"
"ascend/pull/1759) [#1827](https://github.com/vllm-project/vllm-"
"ascend/pull/1827) [#2093](https://github.com/vllm-project/vllm-"
"ascend/pull/2093)"
msgstr ""
"DeepSeek DBO 支持与改进： [#1285](https://github.com/vllm-project/vllm-ascend/pull/1285) [#1291](https://github.com/vllm-project/vllm-ascend/pull/1291) [#1328](https://github.com/vllm-project/vllm-ascend/pull/1328) [#1420](https://github.com/vllm-project/vllm-ascend/pull/1420) [#1445](https://github.com/vllm-project/vllm-ascend/pull/1445) [#1589](https://github.com/vllm-project/vllm-ascend/pull/1589) [#1759](https://github.com/vllm-project/vllm-ascend/pull/1759) [#1827](https://github.com/vllm-project/vllm-ascend/pull/1827) [#2093](https://github.com/vllm-project/vllm-ascend/pull/2093)"

#: ../../source/user_guide/release_notes.md:610
msgid ""
"DeepSeek MTP improvement and bugfix: [#1214](https://github.com/vllm-"
"project/vllm-ascend/pull/1214) [#943](https://github.com/vllm-project"
"/vllm-ascend/pull/943) [#1584](https://github.com/vllm-project/vllm-"
"ascend/pull/1584) [#1473](https://github.com/vllm-project/vllm-"
"ascend/pull/1473) [#1294](https://github.com/vllm-project/vllm-"
"ascend/pull/1294) [#1632](https://github.com/vllm-project/vllm-"
"ascend/pull/1632) [#1694](https://github.com/vllm-project/vllm-"
"ascend/pull/1694) [#1840](https://github.com/vllm-project/vllm-"
"ascend/pull/1840) [#2076](https://github.com/vllm-project/vllm-"
"ascend/pull/2076) [#1990](https://github.com/vllm-project/vllm-"
"ascend/pull/1990) [#2019](https://github.com/vllm-project/vllm-"
"ascend/pull/2019)"
msgstr ""
"DeepSeek MTP 改进与错误修复： [#1214](https://github.com/vllm-project/vllm-ascend/pull/1214) [#943](https://github.com/vllm-project/vllm-ascend/pull/943) [#1584](https://github.com/vllm-project/vllm-ascend/pull/1584) [#1473](https://github.com/vllm-project/vllm-ascend/pull/1473) [#1294](https://github.com/vllm-project/vllm-ascend/pull/1294) [#1632](https://github.com/vllm-project/vllm-ascend/pull/1632) [#1694](https://github.com/vllm-project/vllm-ascend/pull/1694) [#1840](https://github.com/vllm-project/vllm-ascend/pull/1840) [#2076](https://github.com/vllm-project/vllm-ascend/pull/2076) [#1990](https://github.com/vllm-project/vllm-ascend/pull/1990) [#2019](https://github.com/vllm-project/vllm-ascend/pull/2019)"

#: ../../source/user_guide/release_notes.md:611
msgid ""
"Qwen3 MoE support improvement and bugfix around graph mode and DP:  "
"[#1940](https://github.com/vllm-project/vllm-ascend/pull/1940) "
"[#2006](https://github.com/vllm-project/vllm-ascend/pull/2006) "
"[#1832](https://github.com/vllm-project/vllm-ascend/pull/1832)"
msgstr ""
"Qwen3 MoE 在图模式和 DP 方面的支持改进与错误修复： [#1940](https://github.com/vllm-project/vllm-ascend/pull/1940) [#2006](https://github.com/vllm-project/vllm-ascend/pull/2006) [#1832](https://github.com/vllm-project/vllm-ascend/pull/1832)"

#: ../../source/user_guide/release_notes.md:612
msgid ""
"Qwen3 performance improvement around rmsnorm/repo/mlp ops: "
"[#1545](https://github.com/vllm-project/vllm-ascend/pull/1545) "
"[#1719](https://github.com/vllm-project/vllm-ascend/pull/1719) "
"[#1726](https://github.com/vllm-project/vllm-ascend/pull/1726) "
"[#1782](https://github.com/vllm-project/vllm-ascend/pull/1782) "
"[#1745](https://github.com/vllm-project/vllm-ascend/pull/1745)"
msgstr ""
"Qwen3 在 rmsnorm/repo/mlp 算子方面的性能改进： [#1545](https://github.com/vllm-project/vllm-ascend/pull/1545) [#1719](https://github.com/vllm-project/vllm-ascend/pull/1719) [#1726](https://github.com/vllm-project/vllm-ascend/pull/1726) [#1782](https://github.com/vllm-project/vllm-ascend/pull/1782) [#1745](https://github.com/vllm-project/vllm-ascend/pull/1745)"

#: ../../source/user_guide/release_notes.md:613
msgid ""
"DeepSeek MLA chunked prefill/graph mode/multistream improvement and "
"bugfix: [#1240](https://github.com/vllm-project/vllm-ascend/pull/1240) "
"[#933](https://github.com/vllm-project/vllm-ascend/pull/933) "
"[#1135](https://github.com/vllm-project/vllm-ascend/pull/1135) "
"[#1311](https://github.com/vllm-project/vllm-ascend/pull/1311) "
"[#1750](https://github.com/vllm-project/vllm-ascend/pull/1750) "
"[#1872](https://github.com/vllm-project/vllm-ascend/pull/1872) "
"[#2170](https://github.com/vllm-project/vllm-ascend/pull/2170) "
"[#1551](https://github.com/vllm-project/vllm-ascend/pull/1551)"
msgstr ""
"DeepSeek MLA 分块预填充/图模式/多流改进与错误修复： [#1240](https://github.com/vllm-project/vllm-ascend/pull/1240) [#933](https://github.com/vllm-project/vllm-ascend/pull/933) [#1135](https://github.com/vllm-project/vllm-ascend/pull/1135) [#1311](https://github.com/vllm-project/vllm-ascend/pull/1311) [#1750](https://github.com/vllm-project/vllm-ascend/pull/1750) [#1872](https://github.com/vllm-project/vllm-ascend/pull/1872) [#2170](https://github.com/vllm-project/vllm-ascend/pull/2170) [#1551](https://github.com/vllm-project/vllm-ascend/pull/1551)"
#: ../../source/user_guide/release_notes.md:614
msgid ""
"Qwen2.5 VL improvement via mrope/padding mechanism improvement: "
"[#1261](https://github.com/vllm-project/vllm-ascend/pull/1261) "
"[#1705](https://github.com/vllm-project/vllm-ascend/pull/1705) "
"[#1929](https://github.com/vllm-project/vllm-ascend/pull/1929) "
"[#2007](https://github.com/vllm-project/vllm-ascend/pull/2007)"
msgstr ""
"通过改进 mrope/填充机制提升 Qwen2.5 VL 性能: "
"[#1261](https://github.com/vllm-project/vllm-ascend/pull/1261) "
"[#1705](https://github.com/vllm-project/vllm-ascend/pull/1705) "
"[#1929](https://github.com/vllm-project/vllm-ascend/pull/1929) "
"[#2007](https://github.com/vllm-project/vllm-ascend/pull/2007)"

#: ../../source/user_guide/release_notes.md:615
msgid ""
"Ray: Fix the device error when using ray and add initialize_cache and "
"improve warning info: [#1234](https://github.com/vllm-project/vllm-"
"ascend/pull/1234) [#1501](https://github.com/vllm-project/vllm-"
"ascend/pull/1501)"
msgstr ""
"Ray: 修复使用 ray 时的设备错误，并添加 initialize_cache 和改进警告信息: "
"[#1234](https://github.com/vllm-project/vllm-ascend/pull/1234) "
"[#1501](https://github.com/vllm-project/vllm-ascend/pull/1501)"

#: ../../source/user_guide/release_notes.md:617
msgid "Graph Mode Improvement"
msgstr "图模式改进"

#: ../../source/user_guide/release_notes.md:619
msgid ""
"Fix DeepSeek with deepseek with mc2 in [#1269](https://github.com/vllm-"
"project/vllm-ascend/pull/1269)"
msgstr ""
"修复 DeepSeek 模型在 mc2 上的问题 [#1269](https://github.com/vllm-project/vllm-"
"ascend/pull/1269)"

#: ../../source/user_guide/release_notes.md:620
msgid ""
"Fix accuracy problem for deepseek V3/R1 models with torchair graph in "
"long sequence predictions in [#1332](https://github.com/vllm-project"
"/vllm-ascend/pull/1332)"
msgstr ""
"修复 deepseek V3/R1 模型在使用 torchair 图模式进行长序列预测时的精度问题 "
"[#1332](https://github.com/vllm-project/vllm-ascend/pull/1332)"

#: ../../source/user_guide/release_notes.md:621
msgid ""
"Fix torchair_graph_batch_sizes bug in [#1570](https://github.com/vllm-"
"project/vllm-ascend/pull/1570)"
msgstr ""
"修复 torchair_graph_batch_sizes 错误 [#1570](https://github.com/vllm-project/vllm-"
"ascend/pull/1570)"

#: ../../source/user_guide/release_notes.md:622
msgid ""
"Enable the limit of tp <= 4 for torchair graph mode in "
"[#1404](https://github.com/vllm-project/vllm-ascend/pull/1404)"
msgstr ""
"为 torchair 图模式启用 tp <= 4 的限制 [#1404](https://github.com/vllm-project/vllm-"
"ascend/pull/1404)"

#: ../../source/user_guide/release_notes.md:623
msgid ""
"Fix rope accuracy bug [#1887](https://github.com/vllm-project/vllm-"
"ascend/pull/1887)"
msgstr ""
"修复 rope 精度错误 [#1887](https://github.com/vllm-project/vllm-ascend/pull/1887)"

#: ../../source/user_guide/release_notes.md:624
msgid ""
"Support multistream of shared experts in FusedMoE "
"[#997](https://github.com/vllm-project/vllm-ascend/pull/997)"
msgstr ""
"支持 FusedMoE 中共享专家的多流处理 [#997](https://github.com/vllm-project/vllm-"
"ascend/pull/997)"

#: ../../source/user_guide/release_notes.md:625
msgid ""
"Enable kvcache_nz for the decode process in torchair graph "
"mode[#1098](https://github.com/vllm-project/vllm-ascend/pull/1098)"
msgstr ""
"为 torchair 图模式中的解码过程启用 kvcache_nz [#1098](https://github.com/vllm-"
"project/vllm-ascend/pull/1098)"

#: ../../source/user_guide/release_notes.md:626
msgid ""
"Fix chunked-prefill with torchair case to resolve UnboundLocalError: "
"local variable 'decode_hs_or_q_c' issue in [#1378](https://github.com"
"/vllm-project/vllm-ascend/pull/1378)"
msgstr ""
"修复 torchair 场景下的分块预填充，解决 UnboundLocalError: local variable "
"'decode_hs_or_q_c' 问题 [#1378](https://github.com/vllm-project/vllm-"
"ascend/pull/1378)"

#: ../../source/user_guide/release_notes.md:627
msgid ""
"Improve shared experts multi-stream perf for w8a8 dynamic. in "
"[#1561](https://github.com/vllm-project/vllm-ascend/pull/1561)"
msgstr ""
"改进 w8a8 动态量化下共享专家的多流性能 [#1561](https://github.com/vllm-project/vllm-"
"ascend/pull/1561)"

#: ../../source/user_guide/release_notes.md:628
msgid ""
"Repair moe error when set multistream. in [#1882](https://github.com"
"/vllm-project/vllm-ascend/pull/1882)"
msgstr ""
"修复设置多流时的 MoE 错误 [#1882](https://github.com/vllm-project/vllm-"
"ascend/pull/1882)"

#: ../../source/user_guide/release_notes.md:629
msgid ""
"Round up graph batch size to tp size in EP case "
"[#1610](https://github.com/vllm-project/vllm-ascend/pull/1610)"
msgstr ""
"在 EP 场景下将图批次大小向上取整至 tp 大小 [#1610](https://github.com/vllm-"
"project/vllm-ascend/pull/1610)"

#: ../../source/user_guide/release_notes.md:630
msgid ""
"Fix torchair bug when DP is enabled in [#1727](https://github.com/vllm-"
"project/vllm-ascend/pull/1727)"
msgstr ""
"修复启用 DP 时的 torchair 错误 [#1727](https://github.com/vllm-project/vllm-"
"ascend/pull/1727)"

#: ../../source/user_guide/release_notes.md:631
msgid ""
"Add extra checking to torchair_graph_config. in "
"[#1675](https://github.com/vllm-project/vllm-ascend/pull/1675)"
msgstr ""
"为 torchair_graph_config 添加额外检查 [#1675](https://github.com/vllm-project/vllm-"
"ascend/pull/1675)"

#: ../../source/user_guide/release_notes.md:632
msgid ""
"Fix rope bug in torchair+chunk-prefill scenario in "
"[#1693](https://github.com/vllm-project/vllm-ascend/pull/1693)"
msgstr ""
"修复 torchair+分块预填充场景下的 rope 错误 [#1693](https://github.com/vllm-"
"project/vllm-ascend/pull/1693)"

#: ../../source/user_guide/release_notes.md:633
msgid ""
"torchair_graph bugfix when chunked_prefill is true in "
"[#1748](https://github.com/vllm-project/vllm-ascend/pull/1748)"
msgstr ""
"修复 chunked_prefill 为 true 时的 torchair_graph 错误 [#1748](https://github.com"
"/vllm-project/vllm-ascend/pull/1748)"

#: ../../source/user_guide/release_notes.md:634
msgid ""
"Improve prefill optimization to support torchair graph mode in "
"[#2090](https://github.com/vllm-project/vllm-ascend/pull/2090)"
msgstr ""
"改进预填充优化以支持 torchair 图模式 [#2090](https://github.com/vllm-project/vllm-"
"ascend/pull/2090)"

#: ../../source/user_guide/release_notes.md:635
msgid ""
"Fix rank set in DP scenario [#1247](https://github.com/vllm-project/vllm-"
"ascend/pull/1247)"
msgstr ""
"修复 DP 场景中的 rank 设置 [#1247](https://github.com/vllm-project/vllm-"
"ascend/pull/1247)"

#: ../../source/user_guide/release_notes.md:636
msgid ""
"Reset all unused positions to prevent out-of-bounds to resolve GatherV3 "
"bug in [#1397](https://github.com/vllm-project/vllm-ascend/pull/1397)"
msgstr ""
"重置所有未使用的位置以防止越界，解决 GatherV3 错误 [#1397](https://github.com/vllm-"
"project/vllm-ascend/pull/1397)"

#: ../../source/user_guide/release_notes.md:637
msgid ""
"Remove duplicate multimodal codes in ModelRunner in "
"[#1393](https://github.com/vllm-project/vllm-ascend/pull/1393)"
msgstr ""
"移除 ModelRunner 中重复的多模态代码 [#1393](https://github.com/vllm-project/vllm-"
"ascend/pull/1393)"

#: ../../source/user_guide/release_notes.md:638
msgid ""
"Fix block table shape to resolve accuracy issue in "
"[#1297](https://github.com/vllm-project/vllm-ascend/pull/1297)"
msgstr ""
"修复块表形状以解决精度问题 [#1297](https://github.com/vllm-project/vllm-"
"ascend/pull/1297)"

#: ../../source/user_guide/release_notes.md:639
msgid ""
"Implement primal full graph with limited scenario in "
"[#1503](https://github.com/vllm-project/vllm-ascend/pull/1503)"
msgstr ""
"在有限场景下实现原始全图 [#1503](https://github.com/vllm-project/vllm-"
"ascend/pull/1503)"

#: ../../source/user_guide/release_notes.md:640
msgid ""
"Restore paged attention kernel in Full Graph for performance in "
"[#1677](https://github.com/vllm-project/vllm-ascend/pull/1677)"
msgstr ""
"在全图中恢复分页注意力内核以提升性能 [#1677](https://github.com/vllm-project/vllm-"
"ascend/pull/1677)"

#: ../../source/user_guide/release_notes.md:641
msgid ""
"Fix DeepSeek OOM issue in extreme `--gpu-memory-utilization` scenario in "
"[#1829](https://github.com/vllm-project/vllm-ascend/pull/1829)"
msgstr ""
"修复极端 `--gpu-memory-utilization` 场景下的 DeepSeek OOM 问题 "
"[#1829](https://github.com/vllm-project/vllm-ascend/pull/1829)"

#: ../../source/user_guide/release_notes.md:642
msgid ""
"Turn off aclgraph when enabling TorchAir in [#2154](https://github.com"
"/vllm-project/vllm-ascend/pull/2154)"
msgstr ""
"启用 TorchAir 时关闭 aclgraph [#2154](https://github.com/vllm-project/vllm-"
"ascend/pull/2154)"

#: ../../source/user_guide/release_notes.md:644
msgid "Operator Improvement"
msgstr "算子改进"

#: ../../source/user_guide/release_notes.md:646
msgid ""
"Added custom AscendC kernel `vocabparallelembedding` "
"[#796](https://github.com/vllm-project/vllm-ascend/pull/796)"
msgstr ""
"添加自定义 AscendC 内核 `vocabparallelembedding` [#796](https://github.com/vllm-"
"project/vllm-ascend/pull/796)"

#: ../../source/user_guide/release_notes.md:647
msgid ""
"Fixed rope sin/cos cache bug in [#1267](https://github.com/vllm-project"
"/vllm-ascend/pull/1267)"
msgstr ""
"修复 rope sin/cos 缓存错误 [#1267](https://github.com/vllm-project/vllm-"
"ascend/pull/1267)"

#: ../../source/user_guide/release_notes.md:648
msgid ""
"Refactored AscendFusedMoE (#1229) in [#1264](https://github.com/vllm-"
"project/vllm-ascend/pull/1264)"
msgstr ""
"重构 AscendFusedMoE (#1229) [#1264](https://github.com/vllm-project/vllm-"
"ascend/pull/1264)"

#: ../../source/user_guide/release_notes.md:649
msgid ""
"Used fused ops npu_top_k_top_p in sampler [#1920](https://github.com"
"/vllm-project/vllm-ascend/pull/1920)"
msgstr ""
"在采样器中使用融合算子 npu_top_k_top_p [#1920](https://github.com/vllm-project/vllm-"
"ascend/pull/1920)"

#: ../../source/user_guide/release_notes.md:653
msgid ""
"Upgraded CANN to 8.2.rc1 in [#2036](https://github.com/vllm-project/vllm-"
"ascend/pull/2036)"
msgstr ""
"将 CANN 升级至 8.2.rc1 [#2036](https://github.com/vllm-project/vllm-"
"ascend/pull/2036)"

#: ../../source/user_guide/release_notes.md:654
msgid ""
"Upgraded torch-npu to 2.5.1.post1 in [#2135](https://github.com/vllm-"
"project/vllm-ascend/pull/2135)"
msgstr ""
"将 torch-npu 升级至 2.5.1.post1 [#2135](https://github.com/vllm-project/vllm-"
"ascend/pull/2135)"

#: ../../source/user_guide/release_notes.md:655
msgid ""
"Upgraded python to 3.11 in [#2136](https://github.com/vllm-project/vllm-"
msgstr ""
"将 Python 升级至 3.11 [#2136](https://github.com/vllm-project/vllm-"
"ascend/pull/2136)"
#: ../../source/user_guide/release_notes.md:655
msgid ""
"ascend/pull/2136)"
msgstr ""
"将 vLLM 升级到 0.9.1 [#1165](https://github.com/vllm-project/vllm-"
"ascend/pull/1165)"

#: ../../source/user_guide/release_notes.md:656
msgid ""
"Disabled quantization in mindie_turbo  in [#1749](https://github.com"
"/vllm-project/vllm-ascend/pull/1749)"
msgstr ""
"在 mindie_turbo 中禁用量化 [#1749](https://github.com/vllm-project/vllm-"
"ascend/pull/1749)"

#: ../../source/user_guide/release_notes.md:657
msgid ""
"Fixed v0 spec decode in [#1323](https://github.com/vllm-project/vllm-"
"ascend/pull/1323)"
msgstr ""
"修复了 v0 规范解码 [#1323](https://github.com/vllm-project/vllm-"
"ascend/pull/1323)"

#: ../../source/user_guide/release_notes.md:658
msgid ""
"Enabled `ACL_OP_INIT_MODE=1` directly only when using V0 spec decode in "
"[#1271](https://github.com/vllm-project/vllm-ascend/pull/1271)"
msgstr ""
"仅在使用 V0 规范解码时直接启用 `ACL_OP_INIT_MODE=1` [#1271](https://github.com/vllm-project/vllm-ascend/pull/1271)"

#: ../../source/user_guide/release_notes.md:659
msgid ""
"Refactoring forward_context and model_runner_v1 in "
"[#1422](https://github.com/vllm-project/vllm-ascend/pull/1422)"
msgstr ""
"重构 forward_context 和 model_runner_v1 [#1422](https://github.com/vllm-project/vllm-ascend/pull/1422)"

#: ../../source/user_guide/release_notes.md:660
msgid ""
"Fixed sampling params in [#1423](https://github.com/vllm-project/vllm-"
"ascend/pull/1423)"
msgstr ""
"修复了采样参数 [#1423](https://github.com/vllm-project/vllm-"
"ascend/pull/1423)"

#: ../../source/user_guide/release_notes.md:661
msgid ""
"Added a switch for enabling NZ layout in weights and enable NZ for GMM. "
"in [#1409](https://github.com/vllm-project/vllm-ascend/pull/1409)"
msgstr ""
"添加了用于在权重中启用 NZ 布局的开关，并为 GMM 启用 NZ [#1409](https://github.com/vllm-project/vllm-ascend/pull/1409)"

#: ../../source/user_guide/release_notes.md:662
msgid ""
"Resolved bug in ascend_forward_context in [#1449](https://github.com"
"/vllm-project/vllm-ascend/pull/1449) [#1554](https://github.com/vllm-"
"project/vllm-ascend/pull/1554) [#1598](https://github.com/vllm-project"
"/vllm-ascend/pull/1598)"
msgstr ""
"解决了 ascend_forward_context 中的 bug [#1449](https://github.com/vllm-project/vllm-"
"ascend/pull/1449) [#1554](https://github.com/vllm-project/vllm-"
"ascend/pull/1554) [#1598](https://github.com/vllm-project/vllm-"
"ascend/pull/1598)"

#: ../../source/user_guide/release_notes.md:663
msgid ""
"Address PrefillCacheHit state to fix prefix cache accuracy bug in "
"[#1492](https://github.com/vllm-project/vllm-ascend/pull/1492)"
msgstr ""
"处理 PrefillCacheHit 状态以修复前缀缓存准确性 bug [#1492](https://github.com/vllm-project/vllm-"
"ascend/pull/1492)"

#: ../../source/user_guide/release_notes.md:664
msgid ""
"Fixed load weight error and add new e2e case in "
"[#1651](https://github.com/vllm-project/vllm-ascend/pull/1651)"
msgstr ""
"修复了加载权重错误并添加了新的端到端测试用例 [#1651](https://github.com/vllm-project/vllm-ascend/pull/1651)"

#: ../../source/user_guide/release_notes.md:665
msgid ""
"Optimized the number of rope-related index selections in deepseek. in "
"[#1614](https://github.com/vllm-project/vllm-ascend/pull/1614)"
msgstr ""
"优化了 DeepSeek 中与 RoPE 相关的索引选择数量 [#1614](https://github.com/vllm-project/vllm-ascend/pull/1614)"

#: ../../source/user_guide/release_notes.md:666
msgid ""
"Added mc2 mask in [#1642](https://github.com/vllm-project/vllm-"
"ascend/pull/1642)"
msgstr ""
"添加了 mc2 掩码 [#1642](https://github.com/vllm-project/vllm-"
"ascend/pull/1642)"

#: ../../source/user_guide/release_notes.md:667
msgid ""
"Fixed static EPLB log2phy condition and improve unit test in "
"[#1667](https://github.com/vllm-project/vllm-ascend/pull/1667) "
"[#1896](https://github.com/vllm-project/vllm-ascend/pull/1896) "
"[#2003](https://github.com/vllm-project/vllm-ascend/pull/2003)"
msgstr ""
"修复了静态 EPLB log2phy 条件并改进了单元测试 [#1667](https://github.com/vllm-project/vllm-"
"ascend/pull/1667) [#1896](https://github.com/vllm-project/vllm-"
"ascend/pull/1896) [#2003](https://github.com/vllm-project/vllm-"
"ascend/pull/2003)"

#: ../../source/user_guide/release_notes.md:668
msgid ""
"Added chunk mc2 for prefill in [#1703](https://github.com/vllm-project"
"/vllm-ascend/pull/1703)"
msgstr ""
"为预填充添加了分块 mc2 [#1703](https://github.com/vllm-project/vllm-ascend/pull/1703)"

#: ../../source/user_guide/release_notes.md:669
msgid ""
"Fixed mc2 op GroupCoordinator bug in [#1711](https://github.com/vllm-"
"project/vllm-ascend/pull/1711)"
msgstr ""
"修复了 mc2 操作 GroupCoordinator 的 bug [#1711](https://github.com/vllm-"
"project/vllm-ascend/pull/1711)"

#: ../../source/user_guide/release_notes.md:670
msgid ""
"Fixed the failure to recognize the actual type of quantization in "
"[#1721](https://github.com/vllm-project/vllm-ascend/pull/1721)"
msgstr ""
"修复了无法识别量化实际类型的问题 [#1721](https://github.com/vllm-project/vllm-"
"ascend/pull/1721)"

#: ../../source/user_guide/release_notes.md:671
msgid ""
"Fixed DeepSeek bug when tp_size == 1  in [#1755](https://github.com/vllm-"
"project/vllm-ascend/pull/1755)"
msgstr ""
"修复了当 tp_size == 1 时的 DeepSeek bug [#1755](https://github.com/vllm-"
"project/vllm-ascend/pull/1755)"

#: ../../source/user_guide/release_notes.md:672
msgid ""
"Added support for delay-free blocks in prefill nodes in "
"[#1691](https://github.com/vllm-project/vllm-ascend/pull/1691)"
msgstr ""
"在预填充节点中添加了对无延迟块的支持 [#1691](https://github.com/vllm-project/vllm-ascend/pull/1691)"

#: ../../source/user_guide/release_notes.md:673
msgid ""
"MoE alltoallv communication optimization for unquantized RL training & "
"alltoallv support dpo in [#1547](https://github.com/vllm-project/vllm-"
"ascend/pull/1547)"
msgstr ""
"针对未量化 RL 训练的 MoE alltoallv 通信优化 & alltoallv 支持 DPO [#1547](https://github.com/vllm-project/vllm-ascend/pull/1547)"

#: ../../source/user_guide/release_notes.md:674
msgid ""
"Adapted dispatchV2 interface in [#1822](https://github.com/vllm-project"
"/vllm-ascend/pull/1822)"
msgstr ""
"适配了 dispatchV2 接口 [#1822](https://github.com/vllm-project/vllm-ascend/pull/1822)"

#: ../../source/user_guide/release_notes.md:675
msgid ""
"Fixed disaggregate prefill hang issue in long output in "
"[#1807](https://github.com/vllm-project/vllm-ascend/pull/1807)"
msgstr ""
"修复了长输出中 disaggregate prefill 挂起的问题 [#1807](https://github.com/vllm-project/vllm-ascend/pull/1807)"

#: ../../source/user_guide/release_notes.md:676
msgid ""
"Fixed flashcomm_v1 when engine v0 in [#1859](https://github.com/vllm-"
"project/vllm-ascend/pull/1859)"
msgstr ""
"修复了引擎为 v0 时的 flashcomm_v1 问题 [#1859](https://github.com/vllm-"
"project/vllm-ascend/pull/1859)"

#: ../../source/user_guide/release_notes.md:677
msgid ""
"ep_group is not equal to word_size in some cases in "
"[#1862](https://github.com/vllm-project/vllm-ascend/pull/1862)."
msgstr ""
"在某些情况下 ep_group 不等于 word_size [#1862](https://github.com/vllm-project/vllm-ascend/pull/1862)。"

#: ../../source/user_guide/release_notes.md:678
msgid ""
"Fixed wheel glibc version incompatibility in [#1808](https://github.com"
"/vllm-project/vllm-ascend/pull/1808)."
msgstr ""
"修复了 wheel glibc 版本不兼容问题 [#1808](https://github.com/vllm-project/vllm-ascend/pull/1808)。"

#: ../../source/user_guide/release_notes.md:679
msgid ""
"Fixed mc2 process group to resolve self.cpu_group is None in "
"[#1831](https://github.com/vllm-project/vllm-ascend/pull/1831)."
msgstr ""
"修复了 mc2 进程组以解决 self.cpu_group 为 None 的问题 [#1831](https://github.com/vllm-project/vllm-ascend/pull/1831)。"

#: ../../source/user_guide/release_notes.md:680
msgid ""
"Pin vllm version to v0.9.1 to make mypy check passed  in "
"[#1904](https://github.com/vllm-project/vllm-ascend/pull/1904)."
msgstr ""
"将 vllm 版本固定为 v0.9.1 以使 mypy 检查通过 [#1904](https://github.com/vllm-project/vllm-ascend/pull/1904)。"

#: ../../source/user_guide/release_notes.md:681
msgid ""
"Applied npu_moe_gating_top_k_softmax for moe to improve perf in "
"[#1902](https://github.com/vllm-project/vllm-ascend/pull/1902)."
msgstr ""
"为 MoE 应用 npu_moe_gating_top_k_softmax 以提高性能 [#1902](https://github.com/vllm-project/vllm-ascend/pull/1902)。"

#: ../../source/user_guide/release_notes.md:682
msgid ""
"Fixed bug in path_decorator when engine v0 in [#1919](https://github.com"
"/vllm-project/vllm-ascend/pull/1919)."
msgstr ""
"修复了引擎为 v0 时 path_decorator 中的 bug [#1919](https://github.com/vllm-project/vllm-ascend/pull/1919)。"

#: ../../source/user_guide/release_notes.md:683
msgid ""
"Avoid performing cpu all_reduce in disaggregated-prefill scenario in "
"[#1644](https://github.com/vllm-project/vllm-ascend/pull/1644)."
msgstr ""
"在 disaggregated-prefill 场景中避免执行 cpu all_reduce [#1644](https://github.com/vllm-project/vllm-ascend/pull/1644)。"

#: ../../source/user_guide/release_notes.md:684
msgid ""
"Added super kernel in decode MoE in [#1916](https://github.com/vllm-"
"project/vllm-ascend/pull/1916)"
msgstr ""
"在解码 MoE 中添加了超级内核 [#1916](https://github.com/vllm-"
"project/vllm-ascend/pull/1916)"

#: ../../source/user_guide/release_notes.md:685
msgid ""
"[Prefill Perf] Parallel Strategy Optimizations (VRAM-for-Speed Tradeoff) "
"in [#1802](https://github.com/vllm-project/vllm-ascend/pull/1802)."
msgstr ""
"[预填充性能] 并行策略优化（显存换速度权衡） [#1802](https://github.com/vllm-project/vllm-ascend/pull/1802)。"

#: ../../source/user_guide/release_notes.md:686
msgid ""
"Removed unnecessary reduce_results access in shared_experts.down_proj in "
"[#2016](https://github.com/vllm-project/vllm-ascend/pull/2016)."
msgstr ""
"移除了 shared_experts.down_proj 中不必要的 reduce_results 访问 [#2016](https://github.com/vllm-project/vllm-ascend/pull/2016)。"

#: ../../source/user_guide/release_notes.md:687
msgid ""
"Optimized greedy reject sampler with vectorization in "
"[#2002](https://github.com/vllm-project/vllm-ascend/pull/2002)."
msgstr ""
"通过向量化优化了贪婪拒绝采样器 [#2002](https://github.com/vllm-project/vllm-ascend/pull/2002)。"

#: ../../source/user_guide/release_notes.md:688
msgid ""
"Made multiple Ps and Ds work on a single machine in "
"[#1936](https://github.com/vllm-project/vllm-ascend/pull/1936)."
msgstr ""
"使多个 P 和 D 能在单台机器上工作 [#1936](https://github.com/vllm-project/vllm-ascend/pull/1936)。"

#: ../../source/user_guide/release_notes.md:689
msgid ""
"Fixed the shape conflicts between shared & routed experts for deepseek "
"model when tp > 1 and multistream_moe enabled in "
msgstr ""
"修复了当 tp > 1 且启用 multistream_moe 时，DeepSeek 模型中共享专家与路由专家之间的形状冲突问题。"
"[#2075](https://github.com/vllm-project/vllm-ascend/pull/2075)."
msgstr ""

#: ../../source/user_guide/release_notes.md:690
msgid ""
"Added CPU binding support [#2031](https://github.com/vllm-project/vllm-"
"ascend/pull/2031)."
msgstr "添加了 CPU 绑定支持 [#2031](https://github.com/vllm-project/vllm-ascend/pull/2031)。"

#: ../../source/user_guide/release_notes.md:691
msgid ""
"Added with_prefill cpu allreduce to handle D-node recomputation in "
"[#2129](https://github.com/vllm-project/vllm-ascend/pull/2129)."
msgstr "添加了 with_prefill cpu allreduce 以处理 D-node 重计算 [#2129](https://github.com/vllm-project/vllm-ascend/pull/2129)。"

#: ../../source/user_guide/release_notes.md:692
msgid ""
"Added D2H & initRoutingQuantV2 to improve prefill perf in "
"[#2038](https://github.com/vllm-project/vllm-ascend/pull/2038)."
msgstr "添加了 D2H 和 initRoutingQuantV2 以提升预填充性能 [#2038](https://github.com/vllm-project/vllm-ascend/pull/2038)。"

#: ../../source/user_guide/release_notes.md:696
msgid ""
"Provide an e2e guide for execute duration profiling "
"[#1113](https://github.com/vllm-project/vllm-ascend/pull/1113)"
msgstr "提供了执行时长性能分析的端到端指南 [#1113](https://github.com/vllm-project/vllm-ascend/pull/1113)"

#: ../../source/user_guide/release_notes.md:697
msgid ""
"Add Referer header for CANN package download url. "
"[#1192](https://github.com/vllm-project/vllm-ascend/pull/1192)"
msgstr "为 CANN 包下载 URL 添加了 Referer 请求头 [#1192](https://github.com/vllm-project/vllm-ascend/pull/1192)。"

#: ../../source/user_guide/release_notes.md:698
msgid ""
"Add reinstall instructions doc [#1370](https://github.com/vllm-project"
"/vllm-ascend/pull/1370)"
msgstr "添加了重新安装说明文档 [#1370](https://github.com/vllm-project/vllm-ascend/pull/1370)"

#: ../../source/user_guide/release_notes.md:699
msgid ""
"Update Disaggregate prefill README [#1379](https://github.com/vllm-"
"project/vllm-ascend/pull/1379)"
msgstr "更新了 Disaggregate prefill 的 README 文档 [#1379](https://github.com/vllm-project/vllm-ascend/pull/1379)"

#: ../../source/user_guide/release_notes.md:700
msgid ""
"Disaggregate prefill for kv cache register style  "
"[#1296](https://github.com/vllm-project/vllm-ascend/pull/1296)"
msgstr "支持了 kv cache 寄存器风格的解耦预填充 [#1296](https://github.com/vllm-project/vllm-ascend/pull/1296)"

#: ../../source/user_guide/release_notes.md:701
msgid ""
"Fix errors and non-standard parts in "
"examples/disaggregate_prefill_v1/README.md in [#1965](https://github.com"
"/vllm-project/vllm-ascend/pull/1965)"
msgstr "修复了 examples/disaggregate_prefill_v1/README.md 中的错误和非标准部分 [#1965](https://github.com/vllm-project/vllm-ascend/pull/1965)"

#: ../../source/user_guide/release_notes.md:705
msgid ""
"Full graph mode support are not yet available for specific hardware types"
" with full_cuda_graphenable. [#2182](https://github.com/vllm-project"
"/vllm-ascend/issues/2182)"
msgstr "对于特定硬件类型，使用 full_cuda_graphenable 的完整图模式支持尚不可用 [#2182](https://github.com/vllm-project/vllm-ascend/issues/2182)。"

#: ../../source/user_guide/release_notes.md:706
msgid ""
"Qwen3 MoE aclgraph mode with tp failed when enable ep due to bincount "
"error [#2226](https://github.com/vllm-project/vllm-ascend/issues/2226)"
msgstr "启用 EP 时，Qwen3 MoE 的 aclgraph 模式在 TP 下因 bincount 错误而失败 [#2226](https://github.com/vllm-project/vllm-ascend/issues/2226)。"

#: ../../source/user_guide/release_notes.md:707
msgid ""
"As mentioned in the v0.9.1rc1 release note, Atlas 300I series support "
"will NOT be included."
msgstr "如 v0.9.1rc1 版本说明中所述，将不包含对 Atlas 300I 系列的支持。"

#: ../../source/user_guide/release_notes.md:709
msgid "v0.9.2rc1 - 2025.07.11"
msgstr "v0.9.2rc1 - 2025年7月11日"

#: ../../source/user_guide/release_notes.md:711
msgid ""
"This is the 1st release candidate of v0.9.2 for vLLM Ascend. Please "
"follow the [official doc](https://docs.vllm.ai/projects/ascend/en/) to "
"get started. From this release, V1 engine will be enabled by default, "
"there is no need to set `VLLM_USE_V1=1` any more. And this release is the"
" last version to support V0 engine, V0 code will be clean up in the "
"future."
msgstr "这是 vLLM Ascend v0.9.2 的第一个候选发布版本。请参阅[官方文档](https://docs.vllm.ai/projects/ascend/en/)开始使用。从本次发布起，V1 引擎将默认启用，不再需要设置 `VLLM_USE_V1=1`。此外，该版本也是最后一个支持 V0 引擎的版本，V0 相关代码将在未来被清理。"

#: ../../source/user_guide/release_notes.md:715
msgid ""
"Pooling model works with V1 engine now. You can take a try with Qwen3 "
"embedding model [#1359](https://github.com/vllm-project/vllm-"
"ascend/pull/1359)."
msgstr "Pooling 模型现在可以与 V1 引擎一起使用。你可以尝试使用 Qwen3 embedding 模型 [#1359](https://github.com/vllm-project/vllm-ascend/pull/1359)。"

#: ../../source/user_guide/release_notes.md:716
msgid ""
"The performance on Atlas 300I series has been improved. "
"[#1591](https://github.com/vllm-project/vllm-ascend/pull/1591)"
msgstr "Atlas 300I 系列的性能已经提升。 [#1591](https://github.com/vllm-project/vllm-ascend/pull/1591)"

#: ../../source/user_guide/release_notes.md:717
msgid ""
"aclgraph mode works with Moe models now. Currently, only Qwen3 Moe is "
"well tested. [#1381](https://github.com/vllm-project/vllm-"
"ascend/pull/1381)"
msgstr "aclgraph 模式现在可以与 Moe 模型一起使用。目前，仅对 Qwen3 Moe 进行了充分测试。[#1381](https://github.com/vllm-project/vllm-ascend/pull/1381)"

#: ../../source/user_guide/release_notes.md:721
msgid ""
"Ascend PyTorch adapter (torch_npu) has been upgraded to "
"`2.5.1.post1.dev20250619`. Don’t forget to update it in your environment."
" [#1347](https://github.com/vllm-project/vllm-ascend/pull/1347)"
msgstr "Ascend PyTorch 适配器（torch_npu）已升级到 `2.5.1.post1.dev20250619`。请不要忘记在您的环境中进行更新。 [#1347](https://github.com/vllm-project/vllm-ascend/pull/1347)"

#: ../../source/user_guide/release_notes.md:722
msgid ""
"The GatherV3 error has been fixed with aclgraph mode. "
"[#1416](https://github.com/vllm-project/vllm-ascend/pull/1416)"
msgstr "GatherV3 错误已在 aclgraph 模式下修复。[#1416](https://github.com/vllm-project/vllm-ascend/pull/1416)"

#: ../../source/user_guide/release_notes.md:723
msgid ""
"W8A8 quantization works on Atlas 300I series now. "
"[#1560](https://github.com/vllm-project/vllm-ascend/pull/1560)"
msgstr "W8A8 量化现在可以在 Atlas 300I 系列上运行了。[#1560](https://github.com/vllm-project/vllm-ascend/pull/1560)"

#: ../../source/user_guide/release_notes.md:724
msgid ""
"Fix the accuracy problem with deploy models with parallel parameters. "
"[#1678](https://github.com/vllm-project/vllm-ascend/pull/1678)"
msgstr "修复了使用并行参数部署模型时的准确性问题。[#1678](https://github.com/vllm-project/vllm-ascend/pull/1678)"

#: ../../source/user_guide/release_notes.md:725
msgid ""
"The pre-built wheel package now requires lower version of glibc. Users "
"can use it by `pip install vllm-ascend` directly. "
"[#1582](https://github.com/vllm-project/vllm-ascend/pull/1582)"
msgstr "预编译的 wheel 包现在要求更低版本的 glibc。用户可以直接通过 `pip install vllm-ascend` 使用它。[#1582](https://github.com/vllm-project/vllm-ascend/pull/1582)"

#: ../../source/user_guide/release_notes.md:729
msgid ""
"Official doc has been updated for better read experience. For example, "
"more deployment tutorials are added, user/developer docs are updated. "
"More guide will coming soon."
msgstr "官方文档已更新，以提升阅读体验。例如，增加了更多部署教程，用户/开发者文档已更新。更多指南即将推出。"

#: ../../source/user_guide/release_notes.md:730
msgid ""
"Fix accuracy problem for deepseek V3/R1 models with torchair graph in "
"long sequence predictions. [#1331](https://github.com/vllm-project/vllm-"
"ascend/pull/1331)"
msgstr "修复 deepseek V3/R1 模型在使用 torchair 图进行长序列预测时的精度问题。[#1331](https://github.com/vllm-project/vllm-ascend/pull/1331)"

#: ../../source/user_guide/release_notes.md:731
msgid ""
"A new env variable `VLLM_ENABLE_FUSED_EXPERTS_ALLGATHER_EP` has been "
"added. It enables the fused allgather-experts kernel for Deepseek V3/R1 "
"models. The default value is `0`. [#1335](https://github.com/vllm-project"
"/vllm-ascend/pull/1335)"
msgstr "新增了一个环境变量 `VLLM_ENABLE_FUSED_EXPERTS_ALLGATHER_EP`。它用于启用 Deepseek V3/R1 模型的 fused allgather-experts 内核。默认值为 `0`。[#1335](https://github.com/vllm-project/vllm-ascend/pull/1335)"

#: ../../source/user_guide/release_notes.md:732
msgid ""
"A new env variable `VLLM_ASCEND_ENABLE_TOPK_TOPP_OPTIMIZATION` has been "
"added to improve the performance of topk-topp sampling. The default value"
" is 0, we'll consider to enable it by default in the "
"future[#1732](https://github.com/vllm-project/vllm-ascend/pull/1732)"
msgstr "新增了一个环境变量 `VLLM_ASCEND_ENABLE_TOPK_TOPP_OPTIMIZATION`，用于提升 topk-topp 采样的性能。该变量默认值为 0，未来我们会考虑默认启用此选项[#1732](https://github.com/vllm-project/vllm-ascend/pull/1732)。"

#: ../../source/user_guide/release_notes.md:733
msgid ""
"A batch of bugs have been fixed for Data Parallelism case "
"[#1273](https://github.com/vllm-project/vllm-ascend/pull/1273) "
"[#1322](https://github.com/vllm-project/vllm-ascend/pull/1322) "
"[#1275](https://github.com/vllm-project/vllm-ascend/pull/1275) "
"[#1478](https://github.com/vllm-project/vllm-ascend/pull/1478)"
msgstr "已修复了一批与数据并行相关的 bug [#1273](https://github.com/vllm-project/vllm-ascend/pull/1273) [#1322](https://github.com/vllm-project/vllm-ascend/pull/1322) [#1275](https://github.com/vllm-project/vllm-ascend/pull/1275) [#1478](https://github.com/vllm-project/vllm-ascend/pull/1478)"

#: ../../source/user_guide/release_notes.md:734
msgid ""
"The DeepSeek performance has been improved. [#1194](https://github.com"
"/vllm-project/vllm-ascend/pull/1194) [#1395](https://github.com/vllm-"
"project/vllm-ascend/pull/1395) [#1380](https://github.com/vllm-project"
"/vllm-ascend/pull/1380)"
msgstr "DeepSeek 的性能已得到提升。[#1194](https://github.com/vllm-project/vllm-ascend/pull/1194) [#1395](https://github.com/vllm-project/vllm-ascend/pull/1395) [#1380](https://github.com/vllm-project/vllm-ascend/pull/1380)"

#: ../../source/user_guide/release_notes.md:735
msgid ""
"Ascend scheduler works with prefix cache now. [#1446](https://github.com"
"/vllm-project/vllm-ascend/pull/1446)"
msgstr "Ascend 调度器现在支持前缀缓存。[#1446](https://github.com/vllm-project/vllm-ascend/pull/1446)"

#: ../../source/user_guide/release_notes.md:736
msgid ""
"DeepSeek now works with prefix cache now. [#1498](https://github.com"
"/vllm-project/vllm-ascend/pull/1498)"
msgstr "DeepSeek 现在支持前缀缓存了。[#1498](https://github.com/vllm-project/vllm-ascend/pull/1498)"

#: ../../source/user_guide/release_notes.md:737
msgid ""
"Support prompt logprobs to recover ceval accuracy in V1 "
"[#1483](https://github.com/vllm-project/vllm-ascend/pull/1483)"
msgstr "支持使用 prompt logprobs 恢复 V1 的 ceval 准确率 [#1483](https://github.com/vllm-project/vllm-ascend/pull/1483)"

#: ../../source/user_guide/release_notes.md:741
msgid ""
"Pipeline parallel does not work with ray and graph mode: "
"<https://github.com/vllm-project/vllm-ascend/issues/1751> "
"<https://github.com/vllm-project/vllm-ascend/issues/1754>"
msgstr "流水线并行无法与 ray 和图模式协同工作：<https://github.com/vllm-project/vllm-ascend/issues/1751> <https://github.com/vllm-project/vllm-ascend/issues/1754>"

#: ../../source/user_guide/release_notes.md:743
#: ../../source/user_guide/release_notes.md:802
msgid "New Contributors"
msgstr "新贡献者"

#: ../../source/user_guide/release_notes.md:745
#: ../../source/user_guide/release_notes.md:745
msgid ""
"@xleoken made their first contribution in <https://github.com/vllm-"
"project/vllm-ascend/pull/1357>"
msgstr "@xleoken 在 <https://github.com/vllm-project/vllm-ascend/pull/1357> 中完成了首次贡献"

#: ../../source/user_guide/release_notes.md:746
msgid ""
"@lyj-jjj made their first contribution in <https://github.com/vllm-"
"project/vllm-ascend/pull/1335>"
msgstr "@lyj-jjj 在 <https://github.com/vllm-project/vllm-ascend/pull/1335> 中完成了首次贡献"

#: ../../source/user_guide/release_notes.md:747
msgid ""
"@sharonyunyun made their first contribution in <https://github.com/vllm-"
"project/vllm-ascend/pull/1194>"
msgstr "@sharonyunyun 在 <https://github.com/vllm-project/vllm-ascend/pull/1194> 中完成了首次贡献"

#: ../../source/user_guide/release_notes.md:748
msgid ""
"@Pr0Wh1teGivee made their first contribution in <https://github.com/vllm-"
"project/vllm-ascend/pull/1308>"
msgstr "@Pr0Wh1teGivee 在 <https://github.com/vllm-project/vllm-ascend/pull/1308> 中完成了首次贡献"

#: ../../source/user_guide/release_notes.md:749
msgid ""
"@leo-pony made their first contribution in <https://github.com/vllm-"
"project/vllm-ascend/pull/1374>"
msgstr "@leo-pony 在 <https://github.com/vllm-project/vllm-ascend/pull/1374> 中完成了首次贡献"

#: ../../source/user_guide/release_notes.md:750
msgid ""
"@zeshengzong made their first contribution in <https://github.com/vllm-"
"project/vllm-ascend/pull/1452>"
msgstr "@zeshengzong 在 <https://github.com/vllm-project/vllm-ascend/pull/1452> 中完成了首次贡献"

#: ../../source/user_guide/release_notes.md:751
msgid ""
"@GDzhu01 made their first contribution in <https://github.com/vllm-"
"project/vllm-ascend/pull/1477>"
msgstr "@GDzhu01 在 <https://github.com/vllm-project/vllm-ascend/pull/1477> 中完成了首次贡献"

#: ../../source/user_guide/release_notes.md:752
msgid ""
"@Agonixiaoxiao made their first contribution in <https://github.com/vllm-"
"project/vllm-ascend/pull/1531>"
msgstr "@Agonixiaoxiao 在 <https://github.com/vllm-project/vllm-ascend/pull/1531> 中完成了首次贡献"

#: ../../source/user_guide/release_notes.md:753
msgid ""
"@zhanghw0354 made their first contribution in <https://github.com/vllm-"
"project/vllm-ascend/pull/1476>"
msgstr "@zhanghw0354 在 <https://github.com/vllm-project/vllm-ascend/pull/1476> 中完成了首次贡献"

#: ../../source/user_guide/release_notes.md:754
msgid ""
"@farawayboat made their first contribution in <https://github.com/vllm-"
"project/vllm-ascend/pull/1591>"
msgstr "@farawayboat 在 <https://github.com/vllm-project/vllm-ascend/pull/1591> 中完成了首次贡献"

#: ../../source/user_guide/release_notes.md:755
msgid ""
"@ZhengWG made their first contribution in <https://github.com/vllm-"
"project/vllm-ascend/pull/1196>"
msgstr "@ZhengWG 在 <https://github.com/vllm-project/vllm-ascend/pull/1196> 中完成了首次贡献"

#: ../../source/user_guide/release_notes.md:756
msgid ""
"@wm901115nwpu made their first contribution in <https://github.com/vllm-"
"project/vllm-ascend/pull/1654>"
msgstr "@wm901115nwpu 在 <https://github.com/vllm-project/vllm-ascend/pull/1654> 中完成了首次贡献"

#: ../../source/user_guide/release_notes.md:758
msgid ""
"**Full Changelog**: <https://github.com/vllm-project/vllm-"
"ascend/compare/v0.9.1rc1...v0.9.2rc1>"
msgstr "**完整更新日志**: <https://github.com/vllm-project/vllm-ascend/compare/v0.9.1rc1...v0.9.2rc1>"

#: ../../source/user_guide/release_notes.md:760
msgid "v0.9.1rc1 - 2025.06.22"
msgstr "v0.9.1rc1 - 2025.06.22"

#: ../../source/user_guide/release_notes.md:762
msgid ""
"This is the 1st release candidate of v0.9.1 for vLLM Ascend. Please "
"follow the [official doc](https://docs.vllm.ai/projects/ascend/en/) to "
"get started."
msgstr ""
"这是 vLLM Ascend v0.9.1 的第一个候选发布版本。请按照[官方文档](https://docs.vllm.ai/projects/ascend/en/)开始使用。"

#: ../../source/user_guide/release_notes.md:764
msgid "Experimental"
msgstr "实验性功能"

#: ../../source/user_guide/release_notes.md:766
msgid ""
"Atlas 300I series is experimental supported in this release (Functional "
"test passed with Qwen2.5-7b-"
"instruct/Qwen2.5-0.5b/Qwen3-0.6B/Qwen3-4B/Qwen3-8B). "
"[#1333](https://github.com/vllm-project/vllm-ascend/pull/1333)"
msgstr ""
"本版本对 Atlas 300I 系列提供了实验性支持（已通过 Qwen2.5-7b-instruct/Qwen2.5-0.5b/Qwen3-0.6B/Qwen3-4B/Qwen3-8B 的功能测试）。[#1333](https://github.com/vllm-project/vllm-ascend/pull/1333)"

#: ../../source/user_guide/release_notes.md:767
msgid ""
"Support EAGLE-3 for speculative decoding. [#1032](https://github.com"
"/vllm-project/vllm-ascend/pull/1032)"
msgstr ""
"支持 EAGLE-3 进行推测式解码。[#1032](https://github.com/vllm-project/vllm-"
"ascend/pull/1032)"

#: ../../source/user_guide/release_notes.md:769
msgid ""
"After careful consideration, above features **will NOT be included in "
"v0.9.1-dev branch (v0.9.1 final release)** taking into account the v0.9.1"
" release quality and the feature rapid iteration. We will improve this "
"from 0.9.2rc1 and later."
msgstr ""
"经过慎重考虑，鉴于 v0.9.1 版本的发布质量要求以及功能的快速迭代，上述功能**将不会被包含在 v0.9.1-dev 分支（即 v0.9.1 最终版本）中**。我们将在 0.9.2rc1 及之后的版本中进一步完善这些功能。"

#: ../../source/user_guide/release_notes.md:773
msgid ""
"Ascend PyTorch adapter (torch_npu) has been upgraded to "
"`2.5.1.post1.dev20250528`. Don’t forget to update it in your environment."
" [#1235](https://github.com/vllm-project/vllm-ascend/pull/1235)"
msgstr ""
"Ascend PyTorch 适配器（torch_npu）已升级到 "
"`2.5.1.post1.dev20250528`。请不要忘记在您的环境中进行更新。[#1235](https://github.com"
"/vllm-project/vllm-ascend/pull/1235)"

#: ../../source/user_guide/release_notes.md:774
msgid ""
"Support Atlas 300I series container image. You can get it from "
"[quay.io](https://quay.io/repository/vllm/vllm-ascend)"
msgstr ""
"支持 Atlas 300I 系列的容器镜像。您可以从 [quay.io](https://quay.io/repository/vllm/vllm-ascend) 获取。"

#: ../../source/user_guide/release_notes.md:775
msgid ""
"Fix token-wise padding mechanism to make multi-card graph mode work. "
"[#1300](https://github.com/vllm-project/vllm-ascend/pull/1300)"
msgstr ""
"修复按 token 填充机制以支持多卡图模式。 [#1300](https://github.com/vllm-project/vllm-"
"ascend/pull/1300)"

#: ../../source/user_guide/release_notes.md:776
msgid ""
"Upgrade vLLM to 0.9.1 [#1165](https://github.com/vllm-project/vllm-"
"ascend/pull/1165)"
msgstr ""
"将 vLLM 升级到 0.9.1 [#1165](https://github.com/vllm-project/vllm-"
"ascend/pull/1165)"

#: ../../source/user_guide/release_notes.md:778
msgid "Other Improvements"
msgstr "其他改进"

#: ../../source/user_guide/release_notes.md:780
msgid ""
"Initial support Chunked Prefill for MLA. [#1172](https://github.com/vllm-"
"project/vllm-ascend/pull/1172)"
msgstr ""
"为 MLA 初步支持分块预填充。 [#1172](https://github.com/vllm-project/vllm-"
"ascend/pull/1172)"

#: ../../source/user_guide/release_notes.md:781
msgid ""
"An example of best practices to run DeepSeek with ETP has been added. "
"[#1101](https://github.com/vllm-project/vllm-ascend/pull/1101)"
msgstr ""
"已新增一个使用 ETP 运行 DeepSeek 的最佳实践示例。[#1101](https://github.com/vllm-project"
"/vllm-ascend/pull/1101)"

#: ../../source/user_guide/release_notes.md:782
msgid ""
"Performance improvements for DeepSeek using the TorchAir graph. "
"[#1098](https://github.com/vllm-project/vllm-ascend/pull/1098), "
"[#1131](https://github.com/vllm-project/vllm-ascend/pull/1131)"
msgstr ""
"通过使用 TorchAir 图对 DeepSeek 进行了性能提升。[#1098](https://github.com/vllm-project"
"/vllm-ascend/pull/1098), [#1131](https://github.com/vllm-project/vllm-"
"ascend/pull/1131)"

#: ../../source/user_guide/release_notes.md:783
msgid ""
"Supports the speculative decoding feature with AscendScheduler. "
"[#943](https://github.com/vllm-project/vllm-ascend/pull/943)"
msgstr ""
"支持 AscendScheduler 的推测式解码功能。[#943](https://github.com/vllm-project/vllm-"
"ascend/pull/943)"

#: ../../source/user_guide/release_notes.md:784
msgid ""
"Improve `VocabParallelEmbedding` custom op performance. It will be "
"enabled in the next release. [#796](https://github.com/vllm-project/vllm-"
"ascend/pull/796)"
msgstr ""
"提升 `VocabParallelEmbedding` "
"自定义算子的性能。该优化将在下一个版本中启用。[#796](https://github.com/vllm-project/vllm-"
"ascend/pull/796)"

#: ../../source/user_guide/release_notes.md:785
msgid ""
"Fixed a device discovery and setup bug when running vLLM Ascend on Ray "
"[#884](https://github.com/vllm-project/vllm-ascend/pull/884)"
msgstr ""
"修复了在 Ray 上运行 vLLM Ascend 时的设备发现和设置错误 [#884](https://github.com/vllm-"
"project/vllm-ascend/pull/884)"

#: ../../source/user_guide/release_notes.md:786
msgid ""
"DeepSeek with "
"[MC2](https://www.hiascend.com/document/detail/zh/canncommercial/81RC1/developmentguide/opdevg/ascendcbestP/atlas_ascendc_best_practices_10_0043.html)"
" (Merged Compute and Communication) now works properly. "
"[#1268](https://github.com/vllm-project/vllm-ascend/pull/1268)"
msgstr ""
"DeepSeek 现已可以与 "
"[MC2](https://www.hiascend.com/document/detail/zh/canncommercial/81RC1/developmentguide/opdevg/ascendcbestP/atlas_ascendc_best_practices_10_0043.html)（计算与通信融合）正常工作。[#1268](https://github.com"
"/vllm-project/vllm-ascend/pull/1268)"

#: ../../source/user_guide/release_notes.md:787
msgid ""
"Fixed log2phy NoneType bug with static EPLB feature. "
"[#1186](https://github.com/vllm-project/vllm-ascend/pull/1186)"
msgstr ""
"修复了带有静态 EPLB 特性时 log2phy 为 NoneType 的 bug。[#1186](https://github.com"
"/vllm-project/vllm-ascend/pull/1186)"

#: ../../source/user_guide/release_notes.md:788
msgid ""
"Improved performance for DeepSeek with DBO enabled. "
"[#997](https://github.com/vllm-project/vllm-ascend/pull/997), "
"[#1135](https://github.com/vllm-project/vllm-ascend/pull/1135)"
msgstr ""
"启用 DBO 后，DeepSeek 的性能得到提升。[#997](https://github.com/vllm-project/vllm-"
"ascend/pull/997)，[#1135](https://github.com/vllm-project/vllm-"
"ascend/pull/1135)"

#: ../../source/user_guide/release_notes.md:789
msgid ""
"Refactoring AscendFusedMoE [#1229](https://github.com/vllm-project/vllm-"
"ascend/pull/1229)"
msgstr ""
"重构 AscendFusedMoE [#1229](https://github.com/vllm-project/vllm-"
"ascend/pull/1229)"

#: ../../source/user_guide/release_notes.md:790
msgid ""
"Add initial user stories page (include LLaMA-Factory/TRL/verl/MindIE "
"Turbo/GPUStack) [#1224](https://github.com/vllm-project/vllm-"
"ascend/pull/1224)"
msgstr ""
"新增初始用户故事页面（包括 LLaMA-Factory/TRL/verl/MindIE "
"Turbo/GPUStack）[#1224](https://github.com/vllm-project/vllm-"
"ascend/pull/1224)"

#: ../../source/user_guide/release_notes.md:791
msgid ""
"Add unit test framework [#1201](https://github.com/vllm-project/vllm-"
"ascend/pull/1201)"
msgstr "添加单元测试框架 [#1201](https://github.com/vllm-project/vllm-ascend/pull/1201)"

#: ../../source/user_guide/release_notes.md:795
msgid ""
"In some cases, the vLLM process may crash with a **GatherV3** error when "
"**aclgraph** is enabled. We are working on this issue and will fix it in "
msgstr ""
"在某些情况下，当启用 **aclgraph** 时，vLLM 进程可能会因 **GatherV3** 错误而崩溃。我们正在处理此问题，并将在"
#: ../../source/user_guide/release_notes.md:796
msgid ""
"Prefix cache feature does not work with the Ascend Scheduler but without "
"chunked prefill enabled. This will be fixed in the next release. "
"[#1350](https://github.com/vllm-project/vllm-ascend/issues/1350)"
msgstr ""
"前缀缓存功能在未启用分块预填充的情况下无法与 Ascend "
"调度器协同工作。此问题将在下一个版本中修复。[#1350](https://github.com/vllm-project/vllm-"
"ascend/issues/1350)"

#: ../../source/user_guide/release_notes.md:798
msgid "Full Changelog"
msgstr "完整更新日志"

#: ../../source/user_guide/release_notes.md:800
msgid ""
"<https://github.com/vllm-project/vllm-"
"ascend/compare/v0.9.0rc2...v0.9.1rc1>"
msgstr "<https://github.com/vllm-project/vllm-ascend/compare/v0.9.0rc2...v0.9.1rc1>"

#: ../../source/user_guide/release_notes.md:804
msgid ""
"@farawayboat made their first contribution in <https://github.com/vllm-"
"project/vllm-ascend/pull/1333>"
msgstr "@farawayboat 在 <https://github.com/vllm-project/vllm-ascend/pull/1333> 中做出了首次贡献"

#: ../../source/user_guide/release_notes.md:805
msgid ""
"@yzim made their first contribution in <https://github.com/vllm-project"
"/vllm-ascend/pull/1159>"
msgstr "@yzim 在 <https://github.com/vllm-project/vllm-ascend/pull/1159> 中做出了首次贡献"

#: ../../source/user_guide/release_notes.md:806
msgid ""
"@chenwaner made their first contribution in <https://github.com/vllm-"
"project/vllm-ascend/pull/1098>"
msgstr "@chenwaner 在 <https://github.com/vllm-project/vllm-ascend/pull/1098> 中做出了首次贡献"

#: ../../source/user_guide/release_notes.md:807
msgid ""
"@wangyanhui-cmss made their first contribution in <https://github.com"
"/vllm-project/vllm-ascend/pull/1184>"
msgstr "@wangyanhui-cmss 在 <https://github.com/vllm-project/vllm-ascend/pull/1184> 中做出了首次贡献"

#: ../../source/user_guide/release_notes.md:808
msgid ""
"@songshanhu07 made their first contribution in <https://github.com/vllm-"
"project/vllm-ascend/pull/1186>"
msgstr "@songshanhu07 在 <https://github.com/vllm-project/vllm-ascend/pull/1186> 中做出了首次贡献"

#: ../../source/user_guide/release_notes.md:809
msgid ""
"@yuancaoyaoHW made their first contribution in <https://github.com/vllm-"
"project/vllm-ascend/pull/1032>"
msgstr "@yuancaoyaoHW 在 <https://github.com/vllm-project/vllm-ascend/pull/1032> 中做出了首次贡献"

#: ../../source/user_guide/release_notes.md:811
msgid ""
"**Full Changelog**: <https://github.com/vllm-project/vllm-"
"ascend/compare/v0.9.0rc2...v0.9.1rc1>"
msgstr "**完整更新日志**: <https://github.com/vllm-project/vllm-ascend/compare/v0.9.0rc2...v0.9.1rc1>"

#: ../../source/user_guide/release_notes.md:813
msgid "v0.9.0rc2 - 2025.06.10"
msgstr "v0.9.0rc2 - 2025.06.10"

#: ../../source/user_guide/release_notes.md:815
msgid ""
"This release contains some quick fixes for v0.9.0rc1. Please use this "
"release instead of v0.9.0rc1."
msgstr "本次发布包含了一些针对 v0.9.0rc1 的快速修复。请使用本次发布版本，而不是 v0.9.0rc1。"

#: ../../source/user_guide/release_notes.md:819
msgid ""
"Fix the import error when vllm-ascend is installed without editable way. "
"[#1152](https://github.com/vllm-project/vllm-ascend/pull/1152)"
msgstr ""
"修复当以非可编辑方式安装 vllm-ascend 时的导入错误。[#1152](https://github.com/vllm-project"
"/vllm-ascend/pull/1152)"

#: ../../source/user_guide/release_notes.md:821
msgid "v0.9.0rc1 - 2025.06.09"
msgstr "v0.9.0rc1 - 2025.06.09"

#: ../../source/user_guide/release_notes.md:823
msgid ""
"This is the 1st release candidate of v0.9.0 for vllm-ascend. Please "
"follow the [official doc](https://docs.vllm.ai/projects/ascend/en/) to "
"start the journey. From this release, V1 Engine is recommended to use. "
"The code of V0 Engine is frozen and will not be maintained any more. "
"Please set environment `VLLM_USE_V1=1` to enable V1 Engine."
msgstr ""
"这是 vllm-ascend v0.9.0 的第一个候选发布版本。请按照[官方文档](https://docs.vllm.ai/projects/ascend/en/)开始使用。从此版本起，推荐使用 V1 引擎。V0 引擎的代码已被冻结，不再维护。如需启用 V1 引擎，请设置环境变量 `VLLM_USE_V1=1`。"

#: ../../source/user_guide/release_notes.md:827
msgid ""
"DeepSeek works with graph mode now. Follow the [official "
"doc](https://docs.vllm.ai/projects/ascend/en/latest/user_guide/feature_guide/graph_mode.html)"
" to take a try. [#789](https://github.com/vllm-project/vllm-"
"ascend/pull/789)"
msgstr ""
"DeepSeek 现在已支持图模式。请按照[官方文档](https://docs.vllm.ai/projects/ascend/en/latest/user_guide/feature_guide/graph_mode.html)进行尝试。[#789](https://github.com/vllm-project/vllm-ascend/pull/789)"

#: ../../source/user_guide/release_notes.md:828
msgid ""
"Qwen series models work with graph mode now. It works by default with V1 "
"Engine. Please note that in this release, only Qwen series models are "
"well tested with graph mode. We'll make it stable and generalize in the "
"next release. If you hit any issues, please feel free to open an issue on"
" GitHub and fallback to eager mode temporarily by set "
"`enforce_eager=True` when initializing the model."
msgstr ""
"Qwen 系列模型现在支持图模式。默认情况下，它在 V1 引擎下运行。请注意，本次发布中，仅 Qwen "
"系列模型经过了充分的图模式测试。我们将在下一个版本中进一步提升其稳定性并推广至更广泛的场景。如果你遇到任何问题，请随时在 GitHub 上提交 "
"issue，并在初始化模型时通过设置 `enforce_eager=True` 临时切换回 eager 模式。"

#: ../../source/user_guide/release_notes.md:832
msgid ""
"The performance of multi-step scheduler has been improved. Thanks for the"
" contribution from China Merchants Bank. [#814](https://github.com/vllm-"
"project/vllm-ascend/pull/814)"
msgstr ""
"多步调度器的性能得到了提升。感谢招商银行的贡献。[#814](https://github.com/vllm-project/vllm-"
"ascend/pull/814)"

#: ../../source/user_guide/release_notes.md:833
msgid ""
"LoRA、Multi-LoRA And Dynamic Serving is supported for V1 Engine now. "
"Thanks for the contribution from China Merchants Bank. "
"[#893](https://github.com/vllm-project/vllm-ascend/pull/893)"
msgstr ""
"V1 引擎现在支持 LoRA、多 LoRA 以及动态服务。感谢招商银行的贡献。[#893](https://github.com/vllm-"
"project/vllm-ascend/pull/893)"

#: ../../source/user_guide/release_notes.md:834
msgid ""
"Prefix cache and chunked prefill feature works now "
"[#782](https://github.com/vllm-project/vllm-ascend/pull/782) "
"[#844](https://github.com/vllm-project/vllm-ascend/pull/844)"
msgstr ""
"前缀缓存和分块预填充功能现已可用 [#782](https://github.com/vllm-project/vllm-"
"ascend/pull/782) [#844](https://github.com/vllm-project/vllm-"
"ascend/pull/844)"

#: ../../source/user_guide/release_notes.md:835
msgid ""
"Spec decode and MTP features work with V1 Engine now. "
"[#874](https://github.com/vllm-project/vllm-ascend/pull/874) "
"[#890](https://github.com/vllm-project/vllm-ascend/pull/890)"
msgstr ""
"Spec 解码和 MTP 功能现在已经支持 V1 引擎。[#874](https://github.com/vllm-project/vllm-"
"ascend/pull/874) [#890](https://github.com/vllm-project/vllm-"
"ascend/pull/890)"

#: ../../source/user_guide/release_notes.md:836
msgid ""
"DP feature works with DeepSeek now. [#1012](https://github.com/vllm-"
"project/vllm-ascend/pull/1012)"
msgstr ""
"DP 功能现在可以与 DeepSeek 一起使用。[#1012](https://github.com/vllm-project/vllm-"
"ascend/pull/1012)"

#: ../../source/user_guide/release_notes.md:837
msgid ""
"Input embedding feature works with V0 Engine now. "
"[#916](https://github.com/vllm-project/vllm-ascend/pull/916)"
msgstr ""
"输入嵌入特性现在已支持 V0 引擎。[#916](https://github.com/vllm-project/vllm-"
"ascend/pull/916)"

#: ../../source/user_guide/release_notes.md:838
msgid ""
"Sleep mode feature works with V1 Engine now. [#1084](https://github.com"
"/vllm-project/vllm-ascend/pull/1084)"
msgstr ""
"休眠模式功能现在已支持 V1 引擎。[#1084](https://github.com/vllm-project/vllm-"
"ascend/pull/1084)"

#: ../../source/user_guide/release_notes.md:840
#: ../../source/user_guide/release_notes.md:898
#: ../../source/user_guide/release_notes.md:999
#: ../../source/user_guide/release_notes.md:1026
msgid "Models"
msgstr "模型"

#: ../../source/user_guide/release_notes.md:842
msgid ""
"Qwen2.5 VL works with V1 Engine now. [#736](https://github.com/vllm-"
"project/vllm-ascend/pull/736)"
msgstr ""
"Qwen2.5 VL 现在可以与 V1 引擎协同工作。[#736](https://github.com/vllm-project/vllm-"
"ascend/pull/736)"

#: ../../source/user_guide/release_notes.md:843
msgid ""
"LLama4 works now. [#740](https://github.com/vllm-project/vllm-"
"ascend/pull/740)"
msgstr ""
"LLama4 现在可以使用了。[#740](https://github.com/vllm-project/vllm-"
"ascend/pull/740)"

#: ../../source/user_guide/release_notes.md:844
msgid ""
"A new kind of DeepSeek model called dual-batch overlap(DBO) is added. "
"Please set `VLLM_ASCEND_ENABLE_DBO=1` to use it. "
"[#941](https://github.com/vllm-project/vllm-ascend/pull/941)"
msgstr ""
"新增了一种名为双批次重叠（dual-batch overlap，DBO）的 DeepSeek 模型。请设置 "
"`VLLM_ASCEND_ENABLE_DBO=1` 以启用。 [#941](https://github.com/vllm-project"
"/vllm-ascend/pull/941)"

#: ../../source/user_guide/release_notes.md:848
msgid ""
"online serve with ascend quantization works now. "
"[#877](https://github.com/vllm-project/vllm-ascend/pull/877)"
msgstr ""
"在线服务现已支持 Ascend 量化。[#877](https://github.com/vllm-project/vllm-"
"ascend/pull/877)"

#: ../../source/user_guide/release_notes.md:849
msgid ""
"A batch of bugs for graph mode and moe model have been fixed. "
"[#773](https://github.com/vllm-project/vllm-ascend/pull/773) "
"[#771](https://github.com/vllm-project/vllm-ascend/pull/771) "
"[#774](https://github.com/vllm-project/vllm-ascend/pull/774) "
"[#816](https://github.com/vllm-project/vllm-ascend/pull/816) "
"[#817](https://github.com/vllm-project/vllm-ascend/pull/817) "
"[#819](https://github.com/vllm-project/vllm-ascend/pull/819) "
"[#912](https://github.com/vllm-project/vllm-ascend/pull/912) "
"[#897](https://github.com/vllm-project/vllm-ascend/pull/897) "
"[#961](https://github.com/vllm-project/vllm-ascend/pull/961) "
"[#958](https://github.com/vllm-project/vllm-ascend/pull/958) "
"[#913](https://github.com/vllm-project/vllm-ascend/pull/913) "
"[#905](https://github.com/vllm-project/vllm-ascend/pull/905)"
msgstr ""
"已修复一批关于图模式和 MoE 模型的 bug。[#773](https://github.com/vllm-project/vllm-"
"ascend/pull/773) [#771](https://github.com/vllm-project/vllm-"
"ascend/pull/771) [#774](https://github.com/vllm-project/vllm-"
"ascend/pull/774) [#816](https://github.com/vllm-project/vllm-"
"ascend/pull/816) [#817](https://github.com/vllm-project/vllm-"
"ascend/pull/817) [#819](https://github.com/vllm-project/vllm-"
"ascend/pull/819) [#912](https://github.com/vllm-project/vllm-"
"ascend/pull/912) [#897](https://github.com/vllm-project/vllm-"
"ascend/pull/897) [#961](https://github.com/vllm-project/vllm-"
"ascend/pull/961) [#958](https://github.com/vllm-project/vllm-"
"ascend/pull/958) [#913](https://github.com/vllm-project/vllm-"
"ascend/pull/913) [#905](https://github.com/vllm-project/vllm-"
"ascend/pull/905)"

#: ../../source/user_guide/release_notes.md:850
msgid ""
"A batch of performance improvement PRs have been merged. "
"[#784](https://github.com/vllm-project/vllm-ascend/pull/784) "
"[#803](https://github.com/vllm-project/vllm-ascend/pull/803) "
"[#966](https://github.com/vllm-project/vllm-ascend/pull/966) "
"[#839](https://github.com/vllm-project/vllm-ascend/pull/839) "
"[#970](https://github.com/vllm-project/vllm-ascend/pull/970) "
"[#947](https://github.com/vllm-project/vllm-ascend/pull/947) "
"[#987](https://github.com/vllm-project/vllm-ascend/pull/987) "
"[#1085](https://github.com/vllm-project/vllm-ascend/pull/1085)"
msgstr ""
"一批性能改进的 PR 已被合并。[#784](https://github.com/vllm-project/vllm-"
"ascend/pull/784) [#803](https://github.com/vllm-project/vllm-"
"ascend/pull/803) [#966](https://github.com/vllm-project/vllm-"
"ascend/pull/966) [#839](https://github.com/vllm-project/vllm-"
"ascend/pull/839) [#970](https://github.com/vllm-project/vllm-"
"ascend/pull/970) [#947](https://github.com/vllm-project/vllm-"
"ascend/pull/947) [#987](https://github.com/vllm-project/vllm-"
"ascend/pull/987) [#1085](https://github.com/vllm-project/vllm-"
"ascend/pull/1085)"

#: ../../source/user_guide/release_notes.md:851
msgid ""
msgstr ""
#: ../../source/user_guide/release_notes.md:852
msgid ""
"The contributor doc site is "
"[added](https://docs.vllm.ai/projects/ascend/en/latest/community/contributors.html)"
msgstr ""
"贡献者文档站点已[添加](https://docs.vllm.ai/projects/ascend/en/latest/community/contributors.html)"

#: ../../source/user_guide/release_notes.md:854
msgid "Known Issue"
msgstr "已知问题"

#: ../../source/user_guide/release_notes.md:856
msgid ""
"In some case, vLLM process may be crashed with aclgraph enabled. We're "
"working this issue and it'll be fixed in the next release."
msgstr "在某些情况下，启用 aclgraph 时 vLLM 进程可能会崩溃。我们正在处理这个问题，并将在下一个版本中修复。"

#: ../../source/user_guide/release_notes.md:857
msgid ""
"Multi node data-parallel doesn't work with this release. This is a known "
"issue in vllm and has been fixed on main branch. "
"[#18981](https://github.com/vllm-project/vllm/pull/18981)"
msgstr ""
"多节点数据并行在此版本中无法使用。这是 vllm 中已知的问题，并已在主分支中修复。 [#18981](https://github.com"
"/vllm-project/vllm/pull/18981)"

#: ../../source/user_guide/release_notes.md:859
msgid "v0.7.3.post1 - 2025.05.29"
msgstr "v0.7.3.post1 - 2025.05.29"

#: ../../source/user_guide/release_notes.md:861
msgid ""
"This is the first post release of 0.7.3. Please follow the [official "
"doc](https://docs.vllm.ai/projects/ascend/en/v0.7.3) to start the "
"journey. It includes the following changes:"
msgstr ""
"这是 0.7.3 的第一个补丁版本。请按照[官方文档](https://docs.vllm.ai/projects/ascend/en/v0.7.3)开始使用。本次更新包括以下更改："

#: ../../source/user_guide/release_notes.md:865
msgid ""
"Qwen3 and Qwen3MOE is supported now. The performance and accuracy of "
"Qwen3 is well tested. You can try it now. Mindie Turbo is recommended to "
"improve the performance of Qwen3. [#903](https://github.com/vllm-project"
"/vllm-ascend/pull/903) [#915](https://github.com/vllm-project/vllm-"
"ascend/pull/915)"
msgstr ""
"现已支持 Qwen3 和 Qwen3MOE。Qwen3 的性能和精度已经过充分测试，您可以立即试用。推荐使用 Mindie Turbo 以提升 "
"Qwen3 的性能。[#903](https://github.com/vllm-project/vllm-ascend/pull/903) "
"[#915](https://github.com/vllm-project/vllm-ascend/pull/915)"

#: ../../source/user_guide/release_notes.md:866
msgid ""
"Added a new performance guide. The guide aims to help users to improve "
"vllm-ascend performance on system level. It includes OS configuration, "
"library optimization, deploy guide and so on. [#878](https://github.com"
"/vllm-project/vllm-ascend/pull/878) [Doc "
"Link](https://docs.vllm.ai/projects/ascend/en/v0.7.3/developer_guide/performance/optimization_and_tuning.html)"
msgstr ""
"新增了一份性能指南。该指南旨在帮助用户在系统层面提升 vllm-ascend 的性能。内容包括操作系统配置、库优化、部署指南等。 "
"[#878](https://github.com/vllm-project/vllm-ascend/pull/878) [文档链接](https"
"://docs.vllm.ai/projects/ascend/en/v0.7.3/developer_guide/performance/optimization_and_tuning.html)"

#: ../../source/user_guide/release_notes.md:868
msgid "Bug Fixes"
msgstr "漏洞修复"

#: ../../source/user_guide/release_notes.md:870
msgid ""
"Qwen2.5-VL  works for RLHF scenarios now. [#928](https://github.com/vllm-"
"project/vllm-ascend/pull/928)"
msgstr ""
"Qwen2.5-VL 现已支持 RLHF 场景。[#928](https://github.com/vllm-project/vllm-"
"ascend/pull/928)"

#: ../../source/user_guide/release_notes.md:871
msgid ""
"Users can launch the model from online weights now. e.g. from huggingface"
" or modelscope directly [#858](https://github.com/vllm-project/vllm-"
"ascend/pull/858) [#918](https://github.com/vllm-project/vllm-"
"ascend/pull/918)"
msgstr ""
"用户现在可以直接从在线权重启动模型。例如，可以直接从 Hugging Face 或 ModelScope "
"获取。[#858](https://github.com/vllm-project/vllm-ascend/pull/858) "
"[#918](https://github.com/vllm-project/vllm-ascend/pull/918)"

#: ../../source/user_guide/release_notes.md:872
msgid ""
"The meaningless log info `UserWorkspaceSize0` has been cleaned. "
"[#911](https://github.com/vllm-project/vllm-ascend/pull/911)"
msgstr ""
"无意义的日志信息 `UserWorkspaceSize0` 已被清理。[#911](https://github.com/vllm-project"
"/vllm-ascend/pull/911)"

#: ../../source/user_guide/release_notes.md:873
msgid ""
"The log level for `Failed to import vllm_ascend_C` has been changed to "
"`warning` instead of `error`. [#956](https://github.com/vllm-project"
"/vllm-ascend/pull/956)"
msgstr ""
"`Failed to import vllm_ascend_C` 的日志级别已从 `error` 更改为 "
"`warning`。[#956](https://github.com/vllm-project/vllm-ascend/pull/956)"

#: ../../source/user_guide/release_notes.md:874
msgid ""
"DeepSeek MLA now works with chunked prefill in V1 Engine. Please note "
"that V1 engine in 0.7.3 is just expermential and only for test usage. "
"[#849](https://github.com/vllm-project/vllm-ascend/pull/849) "
"[#936](https://github.com/vllm-project/vllm-ascend/pull/936)"
msgstr ""
"DeepSeek MLA 现已在 V1 引擎中支持分块预填充。请注意，0.7.3 版本中的 V1 "
"引擎仅为实验性，仅供测试使用。[#849](https://github.com/vllm-project/vllm-"
"ascend/pull/849) [#936](https://github.com/vllm-project/vllm-"
"ascend/pull/936)"

#: ../../source/user_guide/release_notes.md:878
msgid ""
"The benchmark doc is updated for Qwen2.5 and Qwen2.5-VL "
"[#792](https://github.com/vllm-project/vllm-ascend/pull/792)"
msgstr ""
"基准测试文档已针对 Qwen2.5 和 Qwen2.5-VL 更新 [#792](https://github.com/vllm-project"
"/vllm-ascend/pull/792)"

#: ../../source/user_guide/release_notes.md:879
msgid ""
"Add the note to clear that only \"modelscope<1.23.0\" works with 0.7.3. "
"[#954](https://github.com/vllm-project/vllm-ascend/pull/954)"
msgstr ""
"添加说明，明确只有 \"modelscope<1.23.0\" 能与 0.7.3 版本兼容。[#954](https://github.com"
"/vllm-project/vllm-ascend/pull/954)"

#: ../../source/user_guide/release_notes.md:881
msgid "v0.7.3 - 2025.05.08"
msgstr "v0.7.3 - 2025.05.08"

#: ../../source/user_guide/release_notes.md:883
#: ../../source/user_guide/release_notes.md:1044
msgid "🎉 Hello, World!"
msgstr "🎉 你好，世界！"

#: ../../source/user_guide/release_notes.md:885
msgid ""
"We are excited to announce the release of 0.7.3 for vllm-ascend. This is "
"the first official release. The functionality, performance, and stability"
" of this release are fully tested and verified. We encourage you to try "
"it out and provide feedback. We'll post bug fix versions in the future if"
" needed. Please follow the [official "
"doc](https://docs.vllm.ai/projects/ascend/en/v0.7.3) to start the "
"journey."
msgstr ""
"我们很高兴地宣布 vllm-ascend 0.7.3 "
"版本正式发布。这是首个官方版本。该版本的功能、性能和稳定性已得到全面测试和验证。我们鼓励您试用并提供反馈。如有需要，未来我们将发布漏洞修复版本。请按照[官方文档](https"
"://docs.vllm.ai/projects/ascend/en/v0.7.3)开始您的体验之旅。"

#: ../../source/user_guide/release_notes.md:889
msgid ""
"This release includes all features landed in the previous release "
"candidates ([v0.7.1rc1](https://github.com/vllm-project/vllm-"
"ascend/releases/tag/v0.7.1rc1), [v0.7.3rc1](https://github.com/vllm-"
"project/vllm-ascend/releases/tag/v0.7.3rc1), "
"[v0.7.3rc2](https://github.com/vllm-project/vllm-"
"ascend/releases/tag/v0.7.3rc2)). And all the features are fully tested "
"and verified. Visit the official doc the get the detail "
"[feature](https://docs.vllm.ai/projects/ascend/en/v0.7.3/user_guide/suppoted_features.html)"
" and "
"[model](https://docs.vllm.ai/projects/ascend/en/v0.7.3/user_guide/supported_models.html)"
" support matrix."
msgstr ""
"本次发布包含了所有在之前候选版本中加入的功能（[v0.7.1rc1](https://github.com/vllm-project/vllm-"
"ascend/releases/tag/v0.7.1rc1)、[v0.7.3rc1](https://github.com/vllm-"
"project/vllm-"
"ascend/releases/tag/v0.7.3rc1)、[v0.7.3rc2](https://github.com/vllm-"
"project/vllm-"
"ascend/releases/tag/v0.7.3rc2)）。所有功能都经过了全面测试和验证。请访问官方文档获取详细的[功能](https"
"://docs.vllm.ai/projects/ascend/en/v0.7.3/user_guide/suppoted_features.html)和[模型](https"
"://docs.vllm.ai/projects/ascend/en/v0.7.3/user_guide/supported_models.html)支持矩阵。"

#: ../../source/user_guide/release_notes.md:890
msgid ""
"Upgrade CANN to 8.1.RC1 to enable chunked prefill and automatic prefix "
"caching features. You can now enable them now."
msgstr "将 CANN 升级到 8.1.RC1 以启用分块预填充和自动前缀缓存功能。您现在可以启用这些功能了。"

#: ../../source/user_guide/release_notes.md:891
msgid ""
"Upgrade PyTorch to 2.5.1. vLLM Ascend no longer relies on the dev version"
" of torch-npu now. Now users don't need to install the torch-npu by hand."
" The 2.5.1 version of torch-npu will be installed automatically. "
"[#662](https://github.com/vllm-project/vllm-ascend/pull/662)"
msgstr ""
"升级 PyTorch 至 2.5.1。vLLM Ascend 现在不再依赖于 torch-npu 的开发版本。用户现在无需手动安装 torch-"
"npu，2.5.1 版本的 torch-npu 会被自动安装。[#662](https://github.com/vllm-project"
"/vllm-ascend/pull/662)"

#: ../../source/user_guide/release_notes.md:892
msgid ""
"Integrate MindIE Turbo into vLLM Ascend to improve DeepSeek V3/R1, Qwen 2"
" series performance. [#708](https://github.com/vllm-project/vllm-"
"ascend/pull/708)"
msgstr ""
"将 MindIE Turbo 集成到 vLLM Ascend 以提升 DeepSeek V3/R1、Qwen 2 "
"系列的性能。[#708](https://github.com/vllm-project/vllm-ascend/pull/708)"

#: ../../source/user_guide/release_notes.md:896
msgid ""
"LoRA、Multi-LoRA And Dynamic Serving is supported now. The performance "
"will be improved in the next release. Please follow the official doc for "
"more usage information. Thanks for the contribution from China Merchants "
"Bank. [#700](https://github.com/vllm-project/vllm-ascend/pull/700)"
msgstr ""
"现已支持 LoRA、多LoRA "
"和动态服务。下一个版本中性能将会提升。请参阅官方文档以获取更多用法信息。感谢招商银行的贡献。[#700](https://github.com"
"/vllm-project/vllm-ascend/pull/700)"

#: ../../source/user_guide/release_notes.md:900
msgid ""
"The performance of Qwen2 vl and Qwen2.5 vl is improved. "
"[#702](https://github.com/vllm-project/vllm-ascend/pull/702)"
msgstr ""
"Qwen2 vl 和 Qwen2.5 vl 的性能得到了提升。 [#702](https://github.com/vllm-project"
"/vllm-ascend/pull/702)"

#: ../../source/user_guide/release_notes.md:901
msgid ""
"The performance of `apply_penalties` and `topKtopP` ops are improved. "
"[#525](https://github.com/vllm-project/vllm-ascend/pull/525)"
msgstr ""
"`apply_penalties` 和 `topKtopP` 操作的性能得到了提升。 [#525](https://github.com"
"/vllm-project/vllm-ascend/pull/525)"

#: ../../source/user_guide/release_notes.md:905
msgid ""
"Fixed a issue that may lead CPU memory leak. [#691](https://github.com"
"/vllm-project/vllm-ascend/pull/691) [#712](https://github.com/vllm-"
"project/vllm-ascend/pull/712)"
msgstr ""
"修复了可能导致 CPU 内存泄漏的问题。 [#691](https://github.com/vllm-project/vllm-"
"ascend/pull/691) [#712](https://github.com/vllm-project/vllm-"
"ascend/pull/712)"

#: ../../source/user_guide/release_notes.md:906
msgid ""
"A new environment `SOC_VERSION` is added. If you hit any soc detection "
"error when building with custom ops enabled, please set `SOC_VERSION` to "
"a suitable value. [#606](https://github.com/vllm-project/vllm-"
"ascend/pull/606)"
msgstr ""
"新增了一个环境变量 `SOC_VERSION`。如果在启用自定义算子时构建过程中遇到 SoC 检测错误，请将 `SOC_VERSION` "
"设置为合适的值。[#606](https://github.com/vllm-project/vllm-ascend/pull/606)"

#: ../../source/user_guide/release_notes.md:907
msgid ""
"openEuler container image supported with v0.7.3-openeuler tag. "
"[#665](https://github.com/vllm-project/vllm-ascend/pull/665)"
msgstr ""
"openEuler 容器镜像已支持 v0.7.3-openeuler 标签。[#665](https://github.com/vllm-"
"project/vllm-ascend/pull/665)"

#: ../../source/user_guide/release_notes.md:908
msgid ""
"Prefix cache feature works on V1 engine now. [#559](https://github.com"
"/vllm-project/vllm-ascend/pull/559)"
msgstr ""
"前缀缓存功能现在已在 V1 引擎上工作。[#559](https://github.com/vllm-project/vllm-"
"ascend/pull/559)"

#: ../../source/user_guide/release_notes.md:910
msgid "v0.8.5rc1 - 2025.05.06"
msgstr "v0.8.5rc1 - 2025.05.06"

#: ../../source/user_guide/release_notes.md:912
msgid ""
"This is the 1st release candidate of v0.8.5 for vllm-ascend. Please "
"follow the [official doc](https://docs.vllm.ai/projects/ascend/en/) to "
"start the journey. Now you can enable V1 egnine by setting the "
"environment variable `VLLM_USE_V1=1`, see the feature support status of "
"vLLM Ascend in "
"[here](https://docs.vllm.ai/projects/ascend/en/latest/user_guide/support_matrix/supported_features.html)."
msgstr ""
"这是 vllm-ascend v0.8.5 的第一个候选发布版本。请按照[官方文档](https://docs.vllm.ai/projects/ascend/en/)开始使用。现在，您可以通过设置环境变量 `VLLM_USE_V1=1` 启用 V1 引擎。关于 vLLM Ascend 的特性支持情况，请参见[此处](https://docs.vllm.ai/projects/ascend/en/latest/user_guide/support_matrix/supported_features.html)。"
"ascend.readthedocs.io/en/latest/user_guide/support_matrix/supported_features.html)。"

#: ../../source/user_guide/release_notes.md:916
msgid ""
"Upgrade CANN version to 8.1.RC1 to support chunked prefill and automatic "
"prefix caching (`--enable_prefix_caching`) when V1 is enabled "
"[#747](https://github.com/vllm-project/vllm-ascend/pull/747)"
msgstr ""
"将 CANN 版本升级到 8.1.RC1，以支持在启用 V1 "
"时的分块预填充和自动前缀缓存（`--enable_prefix_caching`）[#747](https://github.com/vllm-"
"project/vllm-ascend/pull/747)"

#: ../../source/user_guide/release_notes.md:917
msgid ""
"Optimize Qwen2 VL and Qwen 2.5 VL [#701](https://github.com/vllm-project"
"/vllm-ascend/pull/701)"
msgstr ""
"优化 Qwen2 VL 和 Qwen 2.5 VL [#701](https://github.com/vllm-project/vllm-"
"ascend/pull/701)"

#: ../../source/user_guide/release_notes.md:918
#, python-brace-format
msgid ""
"Improve Deepseek V3 eager mode and graph mode performance, now you can "
"use --additional_config={'enable_graph_mode': True} to enable graph mode."
" [#598](https://github.com/vllm-project/vllm-ascend/pull/598) "
"[#719](https://github.com/vllm-project/vllm-ascend/pull/719)"
msgstr ""
"改进 Deepseek V3 的 eager 模式和图模式性能，现在你可以使用 "
"--additional_config={'enable_graph_mode': True} "
"来启用图模式。[#598](https://github.com/vllm-project/vllm-ascend/pull/598) "
"[#719](https://github.com/vllm-project/vllm-ascend/pull/719)"

#: ../../source/user_guide/release_notes.md:922
msgid ""
"Upgrade vLLM to 0.8.5.post1 [#715](https://github.com/vllm-project/vllm-"
"ascend/pull/715)"
msgstr ""
"将 vLLM 升级到 0.8.5.post1 [#715](https://github.com/vllm-project/vllm-"
"ascend/pull/715)"

#: ../../source/user_guide/release_notes.md:923
msgid ""
"Fix early return in CustomDeepseekV2MoE.forward during profile_run "
"[#682](https://github.com/vllm-project/vllm-ascend/pull/682)"
msgstr ""
"修复在 profile_run 期间 CustomDeepseekV2MoE.forward 过早返回的问题 "
"[#682](https://github.com/vllm-project/vllm-ascend/pull/682)"

#: ../../source/user_guide/release_notes.md:924
msgid ""
"Adapts for new quant model generated by modelslim "
"[#719](https://github.com/vllm-project/vllm-ascend/pull/719)"
msgstr ""
"适配由 modelslim 生成的新量化模型 [#719](https://github.com/vllm-project/vllm-"
"ascend/pull/719)"

#: ../../source/user_guide/release_notes.md:925
msgid ""
"Initial support on P2P Disaggregated Prefill based on llm_datadist "
"[#694](https://github.com/vllm-project/vllm-ascend/pull/694)"
msgstr ""
"基于 llm_datadist 的 P2P 分布式预填充初步支持 [#694](https://github.com/vllm-"
"project/vllm-ascend/pull/694)"

#: ../../source/user_guide/release_notes.md:926
msgid ""
"Use `/vllm-workspace` as code path and include `.git` in container image "
"to fix issue when start vllm under `/workspace` [#726](https://github.com"
"/vllm-project/vllm-ascend/pull/726)"
msgstr ""
"使用 `/vllm-workspace` 作为代码路径，并在容器镜像中包含 `.git` ，以修复在 `/workspace` 下启动 vllm "
"时的问题 [#726](https://github.com/vllm-project/vllm-ascend/pull/726)"

#: ../../source/user_guide/release_notes.md:927
msgid ""
"Optimize NPU memory usage to make DeepSeek R1 W8A8 32K model len work. "
"[#728](https://github.com/vllm-project/vllm-ascend/pull/728)"
msgstr ""
"优化 NPU 内存使用，以使 DeepSeek R1 W8A8 32K 模型长度能够运行。[#728](https://github.com"
"/vllm-project/vllm-ascend/pull/728)"

#: ../../source/user_guide/release_notes.md:928
msgid ""
"Fix `PYTHON_INCLUDE_PATH` typo in setup.py [#762](https://github.com"
"/vllm-project/vllm-ascend/pull/762)"
msgstr ""
"修复 setup.py 中的 `PYTHON_INCLUDE_PATH` 拼写错误 [#762](https://github.com/vllm-"
"project/vllm-ascend/pull/762)"

#: ../../source/user_guide/release_notes.md:932
msgid ""
"Add Qwen3-0.6B test [#717](https://github.com/vllm-project/vllm-"
"ascend/pull/717)"
msgstr ""
"添加 Qwen3-0.6B 测试 [#717](https://github.com/vllm-project/vllm-"
"ascend/pull/717)"

#: ../../source/user_guide/release_notes.md:933
msgid ""
"Add nightly CI [#668](https://github.com/vllm-project/vllm-"
"ascend/pull/668)"
msgstr "添加夜间持续集成 [#668](https://github.com/vllm-project/vllm-ascend/pull/668)"

#: ../../source/user_guide/release_notes.md:934
msgid ""
"Add accuracy test report [#542](https://github.com/vllm-project/vllm-"
"ascend/pull/542)"
msgstr "添加准确性测试报告 [#542](https://github.com/vllm-project/vllm-ascend/pull/542)"

#: ../../source/user_guide/release_notes.md:936
msgid "v0.8.4rc2 - 2025.04.29"
msgstr "v0.8.4rc2 - 2025.04.29"

#: ../../source/user_guide/release_notes.md:938
msgid ""
"This is the second release candidate of v0.8.4 for vllm-ascend. Please "
"follow the [official doc](https://docs.vllm.ai/projects/ascend/en/) to "
"start the journey. Some experimental features are included in this "
"version, such as W8A8 quantization and EP/DP support. We'll make them "
"stable enough in the next release."
msgstr ""
"这是 vllm-ascend v0.8.4 的第二个候选版本。请按照[官方文档](https://docs.vllm.ai/projects/ascend/en/)开始使用。本版本包含了一些实验性功能，例如 W8A8 量化和 EP/DP 支持。我们将在下一个版本中使其足够稳定。"

#: ../../source/user_guide/release_notes.md:942
msgid ""
"Qwen3 and Qwen3MOE is supported now. Please follow the [official "
"doc](https://docs.vllm.ai/projects/ascend/en/latest/tutorials/single_npu.html)"
" to run the quick demo. [#709](https://github.com/vllm-project/vllm-"
"ascend/pull/709)"
msgstr ""
"现已支持 Qwen3 和 Qwen3MOE。请按照[官方文档](https://docs.vllm.ai/projects/ascend/en/latest/tutorials/single_npu.html)运行快速演示。[#709](https://github.com/vllm-project/vllm-ascend/pull/709)"

#: ../../source/user_guide/release_notes.md:943
msgid ""
"Ascend W8A8 quantization method is supported now. Please take the "
"[official "
"doc](https://docs.vllm.ai/projects/ascend/en/latest/tutorials/multi_npu_quantization.html)"
" for example. Any [feedback](https://github.com/vllm-project/vllm-"
"ascend/issues/619) is welcome. [#580](https://github.com/vllm-project"
"/vllm-ascend/pull/580)"
msgstr ""
"现已支持 Ascend W8A8 量化方法。请参考[官方文档](https://docs.vllm.ai/projects/ascend/en/latest/tutorials/multi_npu_quantization.html)示例。欢迎提供任何[反馈](https://github.com/vllm-project/vllm-ascend/issues/619)。[#580](https://github.com/vllm-project/vllm-ascend/pull/580)"

#: ../../source/user_guide/release_notes.md:944
msgid ""
"DeepSeek V3/R1 works with DP, TP and MTP now. Please note that it's still"
" in experimental status. Let us know if you hit any problem. "
"[#429](https://github.com/vllm-project/vllm-ascend/pull/429) "
"[#585](https://github.com/vllm-project/vllm-ascend/pull/585)  "
"[#626](https://github.com/vllm-project/vllm-ascend/pull/626) "
"[#636](https://github.com/vllm-project/vllm-ascend/pull/636) "
"[#671](https://github.com/vllm-project/vllm-ascend/pull/671)"
msgstr ""
"DeepSeek V3/R1 现已支持 DP、TP 和 MTP。请注意，目前仍处于实验阶段。如果遇到任何问题，请告知我们。 "
"[#429](https://github.com/vllm-project/vllm-ascend/pull/429) "
"[#585](https://github.com/vllm-project/vllm-ascend/pull/585) "
"[#626](https://github.com/vllm-project/vllm-ascend/pull/626) "
"[#636](https://github.com/vllm-project/vllm-ascend/pull/636) "
"[#671](https://github.com/vllm-project/vllm-ascend/pull/671)"

#: ../../source/user_guide/release_notes.md:948
msgid ""
"ACLGraph feature is supported with V1 engine now. It's disabled by "
"default because this feature rely on CANN 8.1 release. We'll make it "
"available by default in the next release [#426](https://github.com/vllm-"
"project/vllm-ascend/pull/426)"
msgstr ""
"ACLGraph 特性现在已被 V1 引擎支持。它默认是禁用的，因为该特性依赖于 CANN 8.1 版本。我们将在下一个版本中默认启用此特性 "
"[#426](https://github.com/vllm-project/vllm-ascend/pull/426)。"

#: ../../source/user_guide/release_notes.md:949
msgid ""
"Upgrade PyTorch to 2.5.1. vLLM Ascend no longer relies on the dev version"
" of torch-npu now. Now users don't need to install the torch-npu by hand."
" The 2.5.1 version of torch-npu will be installed automatically. "
"[#661](https://github.com/vllm-project/vllm-ascend/pull/661)"
msgstr ""
"升级 PyTorch 至 2.5.1。vLLM Ascend 现在不再依赖开发版本的 torch-npu，用户无需手动安装 torch-"
"npu。torch-npu 的 2.5.1 版本将会自动安装。[#661](https://github.com/vllm-project"
"/vllm-ascend/pull/661)"

#: ../../source/user_guide/release_notes.md:953
msgid ""
"MiniCPM model works now. [#645](https://github.com/vllm-project/vllm-"
"ascend/pull/645)"
msgstr ""
"MiniCPM 模型现在可以使用了。[#645](https://github.com/vllm-project/vllm-"
"ascend/pull/645)"

#: ../../source/user_guide/release_notes.md:954
msgid ""
"openEuler container image supported with `v0.8.4-openeuler` tag and "
"customs Ops build is enabled by default for openEuler OS. "
"[#689](https://github.com/vllm-project/vllm-ascend/pull/689)"
msgstr ""
"openEuler 容器镜像已支持 `v0.8.4-openeuler` 标签，并且 openEuler 操作系统默认启用了自定义算子构建。[#689](https://github.com/vllm-project/vllm-ascend/pull/689)"

#: ../../source/user_guide/release_notes.md:955
msgid ""
"Fix ModuleNotFoundError bug to make Lora work [#600](https://github.com"
"/vllm-project/vllm-ascend/pull/600)"
msgstr ""
"修复 ModuleNotFoundError 错误以使 LoRA 正常工作 [#600](https://github.com/vllm-"
"project/vllm-ascend/pull/600)"

#: ../../source/user_guide/release_notes.md:956
msgid ""
"Add \"Using EvalScope evaluation\" doc [#611](https://github.com/vllm-"
"project/vllm-ascend/pull/611)"
msgstr ""
"添加了“使用 EvalScope 评估”文档 [#611](https://github.com/vllm-project/vllm-"
"ascend/pull/611)"

#: ../../source/user_guide/release_notes.md:957
msgid ""
"Add a `VLLM_VERSION` environment to make vLLM version configurable to "
"help developer set correct vLLM version if the code of vLLM is changed by"
" hand locally. [#651](https://github.com/vllm-project/vllm-"
"ascend/pull/651)"
msgstr ""
"新增了一个 `VLLM_VERSION` 环境变量，使 vLLM 版本可以配置，帮助开发者在本地手动修改 vLLM 代码后，设置正确的 vLLM "
"版本。[#651](https://github.com/vllm-project/vllm-ascend/pull/651)"

#: ../../source/user_guide/release_notes.md:959
msgid "v0.8.4rc1 - 2025.04.18"
msgstr "v0.8.4rc1 - 2025.04.18"

#: ../../source/user_guide/release_notes.md:961
msgid ""
"This is the first release candidate of v0.8.4 for vllm-ascend. Please "
"follow the [official doc](https://docs.vllm.ai/projects/ascend/en/) to "
"start the journey. From this version, vllm-ascend will follow the newest "
"version of vllm and release every two weeks. For example, if vllm "
"releases v0.8.5 in the next two weeks, vllm-ascend will release v0.8.5rc1"
" instead of v0.8.4rc2. Please find the detail from the [official "
"documentation](https://docs.vllm.ai/projects/ascend/en/latest/community/versioning_policy.html"
"#release-window)."
msgstr ""
"这是 vllm-ascend v0.8.4 的第一个候选版本。请按照[官方文档](https://docs.vllm.ai/projects/ascend/en/)开始使用。从本版本起，vllm-ascend 将跟随 vllm 的最新版本并每两周发布一次。例如，如果 vllm 在接下来的两周内发布 v0.8.5，vllm-ascend 将发布 v0.8.5rc1，而不是 v0.8.4rc2。详细信息请参考[官方文档](https://docs.vllm.ai/projects/ascend/en/latest/community/versioning_policy.html#release-window)。"

#: ../../source/user_guide/release_notes.md:965
msgid ""
"vLLM V1 engine experimental support is included in this version. You can "
"visit [official "
"guide](https://docs.vllm.ai/en/latest/getting_started/v1_user_guide.html)"
" to get more detail. By default, vLLM will fallback to V0 if V1 doesn't "
"work, please set `VLLM_USE_V1=1` environment if you want to use V1 "
"forcibly."
msgstr ""
"本版本包含了对 vLLM V1 引擎的实验性支持。你可以访问[官方指南](https://docs.vllm.ai/en/latest/getting_started/v1_user_guide.html)获取更多详细信息。默认情况下，如果 V1 不可用，vLLM 会自动回退到 V0。如果你想强制使用 V1，请设置 `VLLM_USE_V1=1` 环境变量。"

#: ../../source/user_guide/release_notes.md:966
msgid ""
"LoRA、Multi-LoRA And Dynamic Serving is supported now. The performance "
"will be improved in the next release. Please follow the [official "
"doc](https://docs.vllm.ai/en/latest/features/lora.html) for more usage "
"information. Thanks for the contribution from China Merchants Bank. "
"[#521](https://github.com/vllm-project/vllm-ascend/pull/521)."
msgstr ""
"现已支持 LoRA、Multi-LoRA 和动态服务。性能将在下一个版本中得到提升。请参阅[官方文档](https://docs.vllm.ai/en/latest/features/lora.html)获取更多使用信息。感谢招商银行的贡献。[#521](https://github.com/vllm-project/vllm-ascend/pull/521)。"

#: ../../source/user_guide/release_notes.md:967
msgid ""
"Sleep Mode feature is supported. Currently it only works on V0 engine. V1"
" engine support will come soon. [#513](https://github.com/vllm-project"
"/vllm-ascend/pull/513)"
msgstr ""
"已支持休眠模式功能。目前它只在 V0 引擎上有效，V1 引擎的支持即将到来。[#513](https://github.com/vllm-project/vllm-ascend/pull/513)"

#: ../../source/user_guide/release_notes.md:971
msgid ""
"The Ascend scheduler is added for V1 engine. This scheduler is more "
"affinity with Ascend hardware. More scheduler policy will be added in the"
" future. [#543](https://github.com/vllm-project/vllm-ascend/pull/543)"
msgstr ""
"为 V1 引擎添加了 Ascend 调度器。此调度器与 Ascend 硬件更具亲和性。未来将添加更多调度策略。[#543](https://github.com/vllm-project/vllm-ascend/pull/543)"
#: ../../source/user_guide/release_notes.md:971
msgid ""
"Add Ascend scheduler for V1 engine. It's more compatible with Ascend "
"hardware. More scheduling policies will be added in the future. "
"[#543](https://github.com/vllm-project/vllm-ascend/pull/543)"
msgstr ""
"为V1引擎新增了Ascend调度器。该调度器与Ascend硬件更加适配。未来还将添加更多调度策略。 "
"[#543](https://github.com/vllm-project/vllm-ascend/pull/543)"

#: ../../source/user_guide/release_notes.md:972
msgid ""
"Disaggregated Prefill feature is supported. Currently only 1P1D works. "
"NPND is under design by vllm team. vllm-ascend will support it once it's "
"ready from vLLM. Follow the [official "
"guide](https://docs.vllm.ai/en/latest/features/disagg_prefill.html) to "
"use. [#432](https://github.com/vllm-project/vllm-ascend/pull/432)"
msgstr ""
"支持分离式预填充（Disaggregated Prefill）功能。目前仅支持1P1D，NPND正在由vllm团队设计中。一旦vLLM支持"
"，vllm-"
"ascend将会支持。请按照[官方指南](https://docs.vllm.ai/en/latest/features/disagg_prefill.html)使用。[#432](https://github.com"
"/vllm-project/vllm-ascend/pull/432)"

#: ../../source/user_guide/release_notes.md:973
msgid ""
"Spec decode feature works now. Currently it only works on V0 engine. V1 "
"engine support will come soon. [#500](https://github.com/vllm-project"
"/vllm-ascend/pull/500)"
msgstr ""
"推测解码（Spec decode）功能现已可用。目前仅支持V0引擎，V1引擎的支持即将推出。[#500](https://github.com"
"/vllm-project/vllm-ascend/pull/500)"

#: ../../source/user_guide/release_notes.md:974
msgid ""
"Structured output feature works now on V1 Engine. Currently it only "
"supports xgrammar backend while using guidance backend may get some "
"errors. [#555](https://github.com/vllm-project/vllm-ascend/pull/555)"
msgstr ""
"结构化输出功能现在已在V1引擎上生效。目前仅支持xgrammar后端，使用guidance后端可能会出现一些错误。[#555](https://github.com"
"/vllm-project/vllm-ascend/pull/555)"

#: ../../source/user_guide/release_notes.md:978
msgid ""
"A new communicator `pyhccl` is added. It's used for call CANN HCCL "
"library directly instead of using `torch.distribute`. More usage of it "
"will be added in the next release [#503](https://github.com/vllm-project"
"/vllm-ascend/pull/503)"
msgstr ""
"新增了一个通信器 `pyhccl`。它用于直接调用 CANN HCCL 库，而不是使用 "
"`torch.distribute`。将在下一个版本中添加更多用法 [#503](https://github.com/vllm-project"
"/vllm-ascend/pull/503)。"

#: ../../source/user_guide/release_notes.md:979
msgid ""
"The custom ops build is enabled by default. You should install the "
"packages like `gcc`, `cmake` first to build `vllm-ascend` from source. "
"Set `COMPILE_CUSTOM_KERNELS=0` environment to disable the compilation if "
"you don't need it. [#466](https://github.com/vllm-project/vllm-"
"ascend/pull/466)"
msgstr ""
"自定义算子的构建默认是启用的。你应该先安装如 `gcc`、`cmake` 等包以便从源码编译 `vllm-"
"ascend`。如果不需要自定义算子的编译，可以设置环境变量 `COMPILE_CUSTOM_KERNELS=0` 来禁用编译。 "
"[#466](https://github.com/vllm-project/vllm-ascend/pull/466)"

#: ../../source/user_guide/release_notes.md:980
msgid ""
"The custom op `rotay embedding` is enabled by default now to improve the "
"performance. [#555](https://github.com/vllm-project/vllm-ascend/pull/555)"
msgstr ""
"自定义算子 `rotary embedding` 现在已默认启用，以提升性能。[#555](https://github.com/vllm-"
"project/vllm-ascend/pull/555)"

#: ../../source/user_guide/release_notes.md:982
msgid "v0.7.3rc2 - 2025.03.29"
msgstr "v0.7.3rc2 - 2025.03.29"

#: ../../source/user_guide/release_notes.md:984
msgid ""
"This is 2nd release candidate of v0.7.3 for vllm-ascend. Please follow "
"the [official doc](https://docs.vllm.ai/projects/ascend/en/v0.7.3) to "
"start the journey."
msgstr ""
"这是 vllm-ascend v0.7.3 的第二个候选发布版本。请根据[官方文档](https://docs.vllm.ai/projects/ascend/en/v0.7.3)开始使用。"

#: ../../source/user_guide/release_notes.md:986
#: ../../source/user_guide/release_notes.md:1012
msgid ""
"Quickstart with container: "
"<https://docs.vllm.ai/projects/ascend/en/v0.7.3/quick_start.html>"
msgstr "容器快速入门： <https://docs.vllm.ai/projects/ascend/en/v0.7.3/quick_start.html>"

#: ../../source/user_guide/release_notes.md:987
#: ../../source/user_guide/release_notes.md:1013
msgid ""
"Installation: "
"<https://docs.vllm.ai/projects/ascend/en/v0.7.3/installation.html>"
msgstr "安装: <https://docs.vllm.ai/projects/ascend/en/v0.7.3/installation.html>"

#: ../../source/user_guide/release_notes.md:991
msgid ""
"Add Ascend Custom Ops framework. Developers now can write customs ops "
"using AscendC. An example ops `rotary_embedding` is added. More tutorials"
" will come soon. The Custom Ops compilation is disabled by default when "
"installing vllm-ascend. Set `COMPILE_CUSTOM_KERNELS=1` to enable it.  "
"[#371](https://github.com/vllm-project/vllm-ascend/pull/371)"
msgstr ""
"新增了Ascend自定义算子框架。开发者现在可以使用AscendC编写自定义算子。新增了一个示例算子 `rotary_embedding` "
"。更多教程即将发布。安装vllm-ascend时，自定义算子的编译默认是关闭的。可通过设置 `COMPILE_CUSTOM_KERNELS=1` "
"启用。[#371](https://github.com/vllm-project/vllm-ascend/pull/371)"

#: ../../source/user_guide/release_notes.md:992
msgid ""
"V1 engine is basic supported in this release. The full support will be "
"done in 0.8.X release. If you hit any issue or have any requirement of V1"
" engine. Please tell us [here](https://github.com/vllm-project/vllm-"
"ascend/issues/414). [#376](https://github.com/vllm-project/vllm-"
"ascend/pull/376)"
msgstr ""
"本版本对 V1 引擎提供了基础支持，全面支持将在 0.8.X 版本中完成。如果您遇到任何问题或有 V1 "
"引擎的相关需求，请在[这里](https://github.com/vllm-project/vllm-"
"ascend/issues/414)告诉我们。[#376](https://github.com/vllm-project/vllm-"
"ascend/pull/376)"

#: ../../source/user_guide/release_notes.md:993
msgid ""
"Prefix cache feature works now. You can set `enable_prefix_caching=True` "
"to enable it. [#282](https://github.com/vllm-project/vllm-"
"ascend/pull/282)"
msgstr ""
"前缀缓存功能现在已经可用。你可以通过设置 `enable_prefix_caching=True` "
"来启用该功能。[#282](https://github.com/vllm-project/vllm-ascend/pull/282)"

#: ../../source/user_guide/release_notes.md:997
msgid ""
"Bump torch_npu version to dev20250320.3 to improve accuracy to fix `!!!` "
"output problem. [#406](https://github.com/vllm-project/vllm-"
"ascend/pull/406)"
msgstr ""
"将 torch_npu 版本升级到 dev20250320.3 以提升精度，修复 `!!!` "
"输出问题。[#406](https://github.com/vllm-project/vllm-ascend/pull/406)"

#: ../../source/user_guide/release_notes.md:1001
msgid ""
"The performance of Qwen2-vl is improved by optimizing patch embedding "
"(Conv3D). [#398](https://github.com/vllm-project/vllm-ascend/pull/398)"
msgstr ""
"通过优化 patch embedding（Conv3D），Qwen2-vl 的性能得到了提升。[#398](https://github.com"
"/vllm-project/vllm-ascend/pull/398)"

#: ../../source/user_guide/release_notes.md:1005
msgid ""
"Fixed a bug to make sure multi step scheduler feature work. "
"[#349](https://github.com/vllm-project/vllm-ascend/pull/349)"
msgstr ""
"修复了一个错误，以确保多步调度器功能正常工作。[#349](https://github.com/vllm-project/vllm-"
"ascend/pull/349)"

#: ../../source/user_guide/release_notes.md:1006
msgid ""
"Fixed a bug to make prefix cache feature works with correct accuracy. "
"[#424](https://github.com/vllm-project/vllm-ascend/pull/424)"
msgstr ""
"修复了一个 bug，使前缀缓存功能能够以正确的准确性运行。[#424](https://github.com/vllm-project/vllm-"
"ascend/pull/424)"

#: ../../source/user_guide/release_notes.md:1008
msgid "v0.7.3rc1 - 2025.03.14"
msgstr "v0.7.3rc1 - 2025.03.14"

#: ../../source/user_guide/release_notes.md:1010
msgid ""
"🎉 Hello, World! This is the first release candidate of v0.7.3 for vllm-"
"ascend. Please follow the [official "
"doc](https://docs.vllm.ai/projects/ascend/en/v0.7.3) to start the "
"journey."
msgstr ""
"🎉 你好，世界！这是 vllm-ascend v0.7.3 的第一个候选发布版本。请按照[官方文档](https://docs.vllm.ai/projects/ascend/en/v0.7.3)开始你的旅程。"

#: ../../source/user_guide/release_notes.md:1017
msgid ""
"DeepSeek V3/R1 works well now. Read the [official "
"guide](https://docs.vllm.ai/projects/ascend/en/v0.7.3/tutorials/multi_node.html)"
" to start! [#242](https://github.com/vllm-project/vllm-ascend/pull/242)"
msgstr ""
"DeepSeek V3/R1 现在运行良好。请阅读[官方指南](https://docs.vllm.ai/projects/ascend/en/v0.7.3/tutorials/multi_node.html)开始！[#242](https://github.com"
"/vllm-project/vllm-ascend/pull/242)"

#: ../../source/user_guide/release_notes.md:1018
msgid ""
"Speculative decoding feature is supported. [#252](https://github.com"
"/vllm-project/vllm-ascend/pull/252)"
msgstr "已支持推测解码功能。[#252](https://github.com/vllm-project/vllm-ascend/pull/252)"

#: ../../source/user_guide/release_notes.md:1019
msgid ""
"Multi step scheduler feature is supported. [#300](https://github.com"
"/vllm-project/vllm-ascend/pull/300)"
msgstr "已支持多步调度器功能。[#300](https://github.com/vllm-project/vllm-ascend/pull/300)"

#: ../../source/user_guide/release_notes.md:1023
msgid "Bump torch_npu version to dev20250308.3 to improve `_exponential` accuracy"
msgstr "将 torch_npu 版本升级到 dev20250308.3，以提升 `_exponential` 的精度"

#: ../../source/user_guide/release_notes.md:1024
msgid ""
"Added initial support for pooling models. Bert based model, such as `BAAI"
"/bge-base-en-v1.5` and `BAAI/bge-reranker-v2-m3` works now. "
"[#229](https://github.com/vllm-project/vllm-ascend/pull/229)"
msgstr ""
"新增了对池化模型的初步支持。现在支持 Bert 基础模型，如 `BAAI/bge-base-en-v1.5` 和 `BAAI/bge-"
"reranker-v2-m3`。 [#229](https://github.com/vllm-project/vllm-"
"ascend/pull/229)"

#: ../../source/user_guide/release_notes.md:1028
msgid ""
"The performance of Qwen2-VL is improved. [#241](https://github.com/vllm-"
"project/vllm-ascend/pull/241)"
msgstr ""
"Qwen2-VL 的性能得到了提升。[#241](https://github.com/vllm-project/vllm-"
"ascend/pull/241)"

#: ../../source/user_guide/release_notes.md:1029
msgid ""
"MiniCPM is now supported [#164](https://github.com/vllm-project/vllm-"
"ascend/pull/164)"
msgstr ""
"MiniCPM 现在已被支持 [#164](https://github.com/vllm-project/vllm-"
"ascend/pull/164)"

#: ../../source/user_guide/release_notes.md:1033
msgid ""
"Support MTP(Multi-Token Prediction) for DeepSeek V3/R1 "
"[#236](https://github.com/vllm-project/vllm-ascend/pull/236)"
msgstr ""
"为 DeepSeek V3/R1 支持 MTP（多标记预测） [#236](https://github.com/vllm-project"
"/vllm-ascend/pull/236)"

#: ../../source/user_guide/release_notes.md:1034
msgid ""
"[Docs] Added more model tutorials, include DeepSeek, QwQ, Qwen and Qwen "
"2.5VL. See the [official "
"doc](https://docs.vllm.ai/projects/ascend/en/v0.7.3/tutorials/index.html)"
" for detail"
msgstr ""
"[文档] 增加了更多的模型教程，包括 DeepSeek、QwQ、Qwen 和 Qwen 2.5VL。详情请参见[官方文档](https"
"://docs.vllm.ai/projects/ascend/en/v0.7.3/tutorials/index.html)。"

#: ../../source/user_guide/release_notes.md:1035
msgid ""
"Pin modelscope<1.23.0 on vLLM v0.7.3 to resolve: <https://github.com"
"/vllm-project/vllm/pull/13807>"
msgstr ""
"在 vLLM v0.7.3 上锁定 modelscope 版本低于 1.23.0，以解决：<https://github.com/vllm-"
"project/vllm/pull/13807>"

#: ../../source/user_guide/release_notes.md:1039
msgid ""
"In [some cases](https://github.com/vllm-project/vllm-ascend/issues/324), "
"especially when the input/output is very long, the accuracy of output may"
" be incorrect. We are working on it. It'll be fixed in the next release."
msgstr ""
"在[某些情况下](https://github.com/vllm-project/vllm-"
"ascend/issues/324)，特别是当输入或输出非常长时，输出的准确性可能会有误。我们正在解决这个问题。将在下一个版本中修复。"

#: ../../source/user_guide/release_notes.md:1040
msgid ""
"Improved and reduced the garbled code in model output. But if you still "
"hit the issue, try to change the generation config value, such as "
"`temperature`, and try again. There is also a known issue shown below. "
"Any [feedback](https://github.com/vllm-project/vllm-ascend/issues/267) is"
" welcome. [#277](https://github.com/vllm-project/vllm-ascend/pull/277)"
msgstr ""
"改进并减少了模型输出中的乱码问题。但如果你仍然遇到该问题，请尝试更改生成配置的参数，例如 "
"`temperature`，然后再试一次。下面还列出了一个已知问题。欢迎提供任何[反馈](https://github.com/vllm-"
"project/vllm-ascend/issues/267)。[#277](https://github.com/vllm-project"
"/vllm-ascend/pull/277)"

#: ../../source/user_guide/release_notes.md:1042
msgid "v0.7.1rc1 - 2025.02.19"
msgstr "v0.7.1rc1 - 2025.02.19"

#: ../../source/user_guide/release_notes.md:1046
msgid ""
"We are excited to announce the first release candidate of v0.7.1 for "
"vllm-ascend."
msgstr "我们很高兴地宣布 vllm-ascend v0.7.1 的第一个候选版本发布。"

#: ../../source/user_guide/release_notes.md:1048
msgid ""
"vLLM Ascend Plugin (vllm-ascend) is a community maintained hardware "
"plugin for running vLLM on the Ascend NPU. With this release, users can "
"now enjoy the latest features and improvements of vLLM on the Ascend NPU."
msgstr ""
"vLLM Ascend 插件（vllm-ascend）是一个由社区维护的硬件插件，用于在 Ascend NPU 上运行 vLLM。通过此版本，用户现在可以在 Ascend NPU 上享受 vLLM 的最新功能和改进。"
#: ../../source/user_guide/release_notes.md:1050
msgid ""
"Please follow the [official "
"doc](https://docs.vllm.ai/projects/ascend/en/v0.7.1) to start the "
"journey. Note that this is a release candidate, and there may be some "
"bugs or issues. We appreciate your feedback and suggestions "
"[here](https://github.com/vllm-project/vllm-ascend/issues/19)"
msgstr ""
"请按照[官方文档](https://docs.vllm.ai/projects/ascend/en/v0.7.1)开始使用。请注意，这是一个候选发布版本，可能存在一些缺陷或问题。我们非常欢迎您通过[此链接](https://github.com/vllm-project/vllm-ascend/issues/19)提供反馈和建议。"

#: ../../source/user_guide/release_notes.md:1054
msgid ""
"Initial supports for Ascend NPU on vLLM. [#3](https://github.com/vllm-"
"project/vllm-ascend/pull/3)"
msgstr ""
"在 vLLM 上初步支持 Ascend NPU。[#3](https://github.com/vllm-project/vllm-ascend/pull/3)"

#: ../../source/user_guide/release_notes.md:1055
msgid ""
"DeepSeek is now supported. [#88](https://github.com/vllm-project/vllm-"
"ascend/pull/88) [#68](https://github.com/vllm-project/vllm-"
"ascend/pull/68)"
msgstr ""
"现已支持 DeepSeek 模型。[#88](https://github.com/vllm-project/vllm-ascend/pull/88) [#68](https://github.com/vllm-project/vllm-ascend/pull/68)"

#: ../../source/user_guide/release_notes.md:1056
msgid ""
"Qwen, Llama series and other popular models are also supported, you can "
"see more details in "
"[here](https://docs.vllm.ai/projects/ascend/en/latest/user_guide/supported_models.html)."
msgstr ""
"Qwen、Llama 系列及其他主流模型也已获得支持，更多详情请参见[此处](https://docs.vllm.ai/projects/ascend/en/latest/user_guide/supported_models.html)。"

#: ../../source/user_guide/release_notes.md:1060
msgid ""
"Added the Ascend quantization config option, the implementation will "
"coming soon. [#7](https://github.com/vllm-project/vllm-ascend/pull/7) "
"[#73](https://github.com/vllm-project/vllm-ascend/pull/73)"
msgstr ""
"新增了 Ascend 量化配置选项，具体实现即将推出。[#7](https://github.com/vllm-project/vllm-ascend/pull/7) [#73](https://github.com/vllm-project/vllm-ascend/pull/73)"

#: ../../source/user_guide/release_notes.md:1061
msgid ""
"Add silu_and_mul and rope ops and add mix ops into attention layer. "
"[#18](https://github.com/vllm-project/vllm-ascend/pull/18)"
msgstr ""
"添加了 silu_and_mul 和 rope 算子，并将混合算子集成到注意力层中。[#18](https://github.com/vllm-project/vllm-ascend/pull/18)"

#: ../../source/user_guide/release_notes.md:1065
msgid ""
"[CI] Enable Ascend CI to actively monitor and improve quality for vLLM on"
" Ascend. [#3](https://github.com/vllm-project/vllm-ascend/pull/3)"
msgstr ""
"[CI] 启用 Ascend CI，以主动监控并提升 vLLM 在 Ascend 平台上的质量。[#3](https://github.com/vllm-project/vllm-ascend/pull/3)"

#: ../../source/user_guide/release_notes.md:1066
msgid ""
"[Docker] Add vllm-ascend container image [#64](https://github.com/vllm-"
"project/vllm-ascend/pull/64)"
msgstr ""
"[Docker] 新增 vllm-ascend 容器镜像 [#64](https://github.com/vllm-project/vllm-ascend/pull/64)"

#: ../../source/user_guide/release_notes.md:1067
msgid ""
"[Docs] Add a [live doc](https://vllm-ascend.readthedocs.org) "
"[#55](https://github.com/vllm-project/vllm-ascend/pull/55)"
msgstr ""
"[文档] 新增了[在线文档](https://vllm-ascend.readthedocs.org) [#55](https://github.com/vllm-project/vllm-ascend/pull/55)"

#: ../../source/user_guide/release_notes.md:1071
msgid ""
"This release relies on an unreleased torch_npu version. It has been "
"installed within official container image already. Please "
"[install](https://docs.vllm.ai/projects/ascend/en/v0.7.1rc1/installation.html)"
" it manually if you are using non-container environment."
msgstr ""
"本版本依赖于一个尚未正式发布的 torch_npu 版本。该版本已预装在官方容器镜像中。如果您使用的是非容器环境，请[手动安装](https://docs.vllm.ai/projects/ascend/en/v0.7.1rc1/installation.html)。"

#: ../../source/user_guide/release_notes.md:1072
msgid ""
"There are logs like `No platform detected, vLLM is running on "
"UnspecifiedPlatform` or `Failed to import from vllm._C with "
"ModuleNotFoundError(\"No module named 'vllm._C'\")` shown when running "
"vllm-ascend. It actually doesn't affect any functionality and "
"performance. You can just ignore it. And it has been fixed in this "
"[PR](https://github.com/vllm-project/vllm/pull/12432) which will be "
"included in v0.7.3 soon."
msgstr ""
"运行 vllm-ascend 时可能会出现类似 `No platform detected, vLLM is running on UnspecifiedPlatform` 或 `Failed to import from vllm._C with ModuleNotFoundError(\"No module named 'vllm._C'\")` 的日志。这实际上不会影响任何功能或性能，您可以忽略它。此问题已在此 [PR](https://github.com/vllm-project/vllm/pull/12432) 中修复，并将很快包含在 v0.7.3 版本中。"

#: ../../source/user_guide/release_notes.md:1073
msgid ""
"There are logs like `# CPU blocks: 35064, # CPU blocks: 2730` shown when "
"running vllm-ascend which should be `# NPU blocks:` . It actually doesn't"
" affect any functionality and performance. You can just ignore it. And it"
" has been fixed in this [PR](https://github.com/vllm-"
"project/vllm/pull/13378) which will be included in v0.7.3 soon."
msgstr ""
"运行 vllm-ascend 时可能会出现类似 `# CPU blocks: 35064, # CPU blocks: 2730` 的日志，其正确显示应为 `# NPU blocks:`。这实际上不会影响任何功能或性能，您可以忽略它。此问题已在此 [PR](https://github.com/vllm-project/vllm/pull/13378) 中修复，并将很快包含在 v0.7.3 版本中。"

#~ msgid "Known issues"
#~ msgstr "已知问题"