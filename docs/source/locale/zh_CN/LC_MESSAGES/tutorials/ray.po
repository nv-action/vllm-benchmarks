# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2025, vllm-ascend team
# This file is distributed under the same license as the vllm-ascend
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2026.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: vllm-ascend \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2026-02-10 09:26+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.18.0\n"

#: ../../source/tutorials/ray.md:1
msgid "Ray Distributed (Qwen3-235B-A22B)"
msgstr ""

#: ../../source/tutorials/ray.md:3
msgid ""
"Multi-node inference is suitable for scenarios where the model cannot be "
"deployed on a single machine. In such cases, the model can be distributed"
" using tensor parallelism or pipeline parallelism. The specific "
"parallelism strategies will be covered in the following sections. To "
"successfully deploy multi-node inference, the following three steps need "
"to be completed:"
msgstr ""

#: ../../source/tutorials/ray.md:5
msgid "**Verify Multi-Node Communication Environment**"
msgstr ""

#: ../../source/tutorials/ray.md:6
msgid "**Set Up and Start the Ray Cluster**"
msgstr ""

#: ../../source/tutorials/ray.md:7
msgid "**Start the Online Inference Service on Multi-node**"
msgstr ""

#: ../../source/tutorials/ray.md:9
msgid "Verify Multi-Node Communication Environment"
msgstr ""

#: ../../source/tutorials/ray.md:11
msgid "Physical Layer Requirements"
msgstr ""

#: ../../source/tutorials/ray.md:13
msgid ""
"The physical machines must be located on the same LAN, with network "
"connectivity."
msgstr ""

#: ../../source/tutorials/ray.md:14
msgid ""
"All NPUs are connected with optical modules, and the connection status "
"must be normal."
msgstr ""

#: ../../source/tutorials/ray.md:16
msgid "Verification Process"
msgstr ""

#: ../../source/tutorials/ray.md:18
msgid ""
"Execute the following commands on each node in sequence. The results must"
" all be `success` and the status must be `UP`:"
msgstr ""

#: ../../source/tutorials/ray.md:35
msgid "NPU Interconnect Verification"
msgstr ""

#: ../../source/tutorials/ray.md:37
msgid "1. Get NPU IP Addresses"
msgstr ""

#: ../../source/tutorials/ray.md:43
msgid "2. Cross-Node PING Test"
msgstr ""

#: ../../source/tutorials/ray.md:50
msgid "Set Up and Start the Ray Cluster"
msgstr ""

#: ../../source/tutorials/ray.md:52
msgid "Setting Up the Basic Container"
msgstr ""

#: ../../source/tutorials/ray.md:54
msgid ""
"To ensure a consistent execution environment across all nodes, including "
"the model path and Python environment, it is advised to use Docker "
"images."
msgstr ""

#: ../../source/tutorials/ray.md:56
msgid ""
"For setting up a multi-node inference cluster with Ray, **containerized "
"deployment** is the preferred approach. Containers should be started on "
"both the primary and secondary nodes, with the `--net=host` option to "
"enable proper network connectivity."
msgstr ""

#: ../../source/tutorials/ray.md:58
msgid ""
"Below is the example container setup command, which should be executed on"
" **all nodes** :"
msgstr ""

#: ../../source/tutorials/ray.md:93
msgid "Start Ray Cluster"
msgstr ""

#: ../../source/tutorials/ray.md:95
msgid ""
"After setting up the containers and installing vllm-ascend on each node, "
"follow the steps below to start the Ray cluster and execute inference "
"tasks."
msgstr ""

#: ../../source/tutorials/ray.md:97
msgid ""
"Choose one machine as the primary node and the others as secondary nodes."
" Before proceeding, use `ip addr` to check your `nic_name` (network "
"interface name)."
msgstr ""

#: ../../source/tutorials/ray.md:99
msgid ""
"Set the `ASCEND_RT_VISIBLE_DEVICES` environment variable to specify the "
"NPU devices to use. For Ray versions above 2.1, also set the "
"`RAY_EXPERIMENTAL_NOSET_ASCEND_RT_VISIBLE_DEVICES` variable to avoid "
"device recognition issues."
msgstr ""

#: ../../source/tutorials/ray.md:101
msgid "Below are the commands for the primary and secondary nodes:"
msgstr ""

#: ../../source/tutorials/ray.md:103
msgid "**Primary node**:"
msgstr ""

#: ../../source/tutorials/ray.md:106 ../../source/tutorials/ray.md:123
msgid ""
"When starting a Ray cluster for multi-node inference, the environment "
"variables on each node must be set **before** starting the Ray cluster "
"for them to take effect. Updating the environment variables requires "
"restarting the Ray cluster."
msgstr ""

#: ../../source/tutorials/ray.md:120
msgid "**Secondary node**:"
msgstr ""

#: ../../source/tutorials/ray.md:136
msgid ""
"Once the cluster is started on multiple nodes, execute `ray status` and "
"`ray list nodes` to verify the Ray cluster's status. You should see the "
"correct number of nodes and NPUs listed."
msgstr ""

#: ../../source/tutorials/ray.md:138
msgid ""
"After Ray is successfully started, the following content will appear: A "
"local Ray instance has started successfully. Dashboard URL: The access "
"address for the Ray Dashboard (default: <http://localhost:8265>); Node "
"status (CPU/memory resources, number of healthy nodes); Cluster "
"connection address (used for adding multiple nodes)."
msgstr ""

#: ../../source/tutorials/ray.md:142
msgid "Start the Online Inference Service on Multi-node scenario"
msgstr ""

#: ../../source/tutorials/ray.md:144
msgid ""
"In the container, you can use vLLM as if all NPUs were on a single node. "
"vLLM will utilize NPU resources across all nodes in the Ray cluster."
msgstr ""

#: ../../source/tutorials/ray.md:146
msgid "**You only need to run the vllm command on one node.**"
msgstr ""

#: ../../source/tutorials/ray.md:148
msgid ""
"To set up parallelism, the common practice is to set the `tensor-"
"parallel-size` to the number of NPUs per node, and the `pipeline-"
"parallel-size` to the number of nodes."
msgstr ""

#: ../../source/tutorials/ray.md:150
msgid ""
"For example, with 16 NPUs across 2 nodes (8 NPUs per node), set the "
"tensor parallel size to 8 and the pipeline parallel size to 2:"
msgstr ""

#: ../../source/tutorials/ray.md:166
msgid ""
"Alternatively, if you want to use only tensor parallelism, set the tensor"
" parallel size to the total number of NPUs in the cluster. For example, "
"with 16 NPUs across 2 nodes, set the tensor parallel size to 16:"
msgstr ""

#: ../../source/tutorials/ray.md:181
msgid "Once your server is started, you can query the model with input prompts:"
msgstr ""

