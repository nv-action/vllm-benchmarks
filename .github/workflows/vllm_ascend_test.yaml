#
# Copyright (c) 2025 Huawei Technologies Co., Ltd. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# This file is a part of the vllm-ascend project.
#

name: 'e2e test'

on:
  workflow_dispatch:

# Bash shells do not use ~/.profile or ~/.bashrc so these shells need to be explicitly
# declared as "shell: bash -el {0}" on steps that need to be properly activated.
# It's used to activate ascend-toolkit environment variables.
defaults:
  run:
    shell: bash -el {0}


jobs:
  test:
    strategy:
      max-parallel: 2
      matrix:
        os: [linux-arm64-npu-1, linux-arm64-npu-4]
        vllm_verison: [main, v0.8.4]

    name: vLLM Ascend test
    runs-on: ${{ matrix.os }}
    container:
      image: quay.io/ascend/cann:8.0.0-910b-ubuntu22.04-py3.10
      env:
        HF_ENDPOINT: https://hf-mirror.com
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
    steps:
    #   - name: Sleep
    #     run: |
    #       if [[ "${{ matrix.os }}" == "linux-arm64-npu-4" ]]; then
    #         sleep 3600
    #       else
    #         sleep 36
    #       fi
      - name: Check npu
        run: |
          count=1  # 初始化尝试次数

          while true; do
            echo "第 $count 次尝试执行 npu-smi info..."
              npu-smi info &> /dev/null  # 执行命令并忽略输出（如需查看输出，可去掉 &> /dev/null）
              if [ $? -eq 0 ]; then  # 检查命令执行状态（0 表示成功）
                  echo "npu-smi info 执行成功，退出循环。"
                  break  # 成功则退出循环
              else
                  echo "执行失败，继续尝试..."
                  ((count++))  # 尝试次数加 1
                  sleep 1  # 等待 1 秒后重试（可根据需要调整间隔）
              fi
          done
      - name: CANN info
        run: cat /usr/local/Ascend/ascend-toolkit/latest/"$(uname -i)"-linux/ascend_toolkit_install.info



    #   - name: Config mirrors
    #     run: |
    #       sed -i 's|ports.ubuntu.com|mirrors.tuna.tsinghua.edu.cn|g' /etc/apt/sources.list
    #       pip config set global.index-url https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple
    #       apt-get update -y
    #       apt install git -y
    #       git config --global url."https://gh-proxy.test.osinfra.cn/https://github.com/".insteadOf https://github.com/

    #   - name: Checkout vllm-project/vllm-ascend repo
    #     uses: actions/checkout@v4
    #     with:
    #       repository: vllm-project/vllm-ascend

    #   - name: Install system dependencies
    #     run: |
    #       sleep 60

    #   - name: Checkout vllm-project/vllm repo
    #     uses: actions/checkout@v4
    #     with:
    #       repository: vllm-project/vllm
    #       ref: ${{ matrix.vllm_verison }}
    #       path: ./vllm-empty

    #   - name: Install vllm-project/vllm from source
    #     working-directory: ./vllm-empty
    #     run: |
    #       VLLM_TARGET_DEVICE=empty pip install -e .

    #   - name: Install pta
    #     run: |
    #       if [ ! -d /root/.cache/pta ]; then
    #         mkdir -p /root/.cache/pta
    #       fi

    #       if [ ! -f /root/.cache/pta/torch_npu-2.5.1.dev20250320-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl ]; then
    #         cd /root/.cache/pta
    #         rm -rf pytorch_v2.5.1_py310*
    #         wget https://pytorch-package.obs.cn-north-4.myhuaweicloud.com/pta/Daily/v2.5.1/20250320.3/pytorch_v2.5.1_py310.tar.gz
    #         tar -zxvf pytorch_v2.5.1_py310.tar.gz
    #       fi

    #       pip install /root/.cache/pta/torch_npu-2.5.1.dev20250320-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl

    #   - name: Install vllm-project/vllm-ascend
    #     run: |
    #       pip install -r requirements-dev.txt
    #       pip install -e .

    #   - name: Run vllm-project/vllm-ascend test on V0 engine
    #     env:
    #       VLLM_USE_V1: 0
    #     run: |
    #       if [[ "${{ matrix.os }}" == "linux-arm64-npu-1" ]]; then
    #         pytest -sv tests/singlecard/test_offline_inference.py
    #         pytest -sv tests/ops
    #       else
    #         pytest -sv tests/multicard/test_offline_inference_distributed.py
    #         pytest -sv tests/ops
    #       fi

    #   - name: Run vllm-project/vllm-ascend test for V1 Engine
    #     env:
    #       VLLM_USE_V1: 1
    #       VLLM_WORKER_MULTIPROC_METHOD: spawn
    #     run: |
    #       if [[ "${{ matrix.os }}" == "linux-arm64-npu-1" ]]; then
    #         pytest -sv tests/singlecard/test_offline_inference.py
    #         pytest -sv tests/ops
    #       else
    #         pytest -sv tests/multicard/test_offline_inference_distributed.py
    #         pytest -sv tests/ops
    #       fi

    #   - name: Check for changes in Speculative Decode
    #     id: filter_spec_decode
    #     uses: dorny/paths-filter@v2
    #     with:
    #       filters: |
    #         speculative_tests_changed:
    #           - "tests/singlecard/spec_decode/**"
    #           - "tests/multicard/spec_decode_e2e/**"
    #           - "vllm_ascend/worker/multi_step_runner.py"
    #           - "vllm_ascend/worker/multi_step_worker.py"
    #           - "vllm_ascend/patch/patch_rejection_sampler.py"
    #           - "vllm_ascend/patch/patch_spec_decode_worker.py"
    #           - "vllm_ascend/patch/patch_multi_step_worker.py"
    #   - name: Run vllm-project/vllm-ascend Speculative Decode test
    #     env:
    #       HF_ENDPOINT: https://hf-mirror.com
    #     if: steps.filter_spec_decode.outputs.speculative_tests_changed
    #     run: |
    #       if [[ "${{ matrix.os }}" == "linux-arm64-npu-1" ]]; then
    #         pytest -sv tests/singlecard/spec_decode
    #       fi

    #   - name: Run vllm-project/vllm test for V0 Engine
    #     env:
    #       VLLM_USE_V1: 0
    #       PYTORCH_NPU_ALLOC_CONF: max_split_size_mb:256
    #     run: |
    #       pytest -sv
