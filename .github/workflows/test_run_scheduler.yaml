name: 'scheduler'

on:
  # schedule:
  # - cron: '00 16 * * *'

  workflow_dispatch:
    inputs:
      runner:
        required: true
        type: choice
        options:
          - linux-arm64-npu-4
          - linux-arm64-npu-1
        default: 'linux-arm64-npu-1'
        description: 'The runner selected to run on'
      image:
        required: true
        type: choice
        options:
          - ascendai/cann:8.0.0-910b-ubuntu22.04-py3.10
        default: 'ascendai/cann:8.0.0-910b-ubuntu22.04-py3.10'
        description: 'The docker image which will be loaded'


# Bash shells do not use ~/.profile or ~/.bashrc so these shells need to be explicitly
# declared as "shell: bash -el {0}" on steps that need to be properly activated.
# It's used to activate ascend-toolkit environment variables.
defaults:
  run:
    shell: bash -el {0}

jobs:
  test:
    name: test scheduler
    runs-on: ${{ github.event.inputs.runner || 'linux-arm64-npu-1' }}
    container:
      image: ${{ github.event.inputs.image || 'ascendai/cann:8.0.0-910b-ubuntu22.04-py3.10' }}
      env:
        HF_ENDPOINT: https://hf-mirror.com
        # VLLM_USE_MODELSCOPE: true
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
        ES_OM_DOMAIN: ${{ secrets.ES_OM_DOMAIN }}
        ES_OM_AUTHORIZATION: ${{ secrets.ES_OM_AUTHORIZATION }}
    steps:
      - name: Check npu and CANN info
        run: |
          npu-smi info
          cat /usr/local/Ascend/ascend-toolkit/latest/"$(uname -i)"-linux/ascend_toolkit_install.info

      - name: Config mirrors
        run: |
          # sed -i 's|ports.ubuntu.com|mirrors.tuna.tsinghua.edu.cn|g' /etc/apt/sources.list
          pip config set global.index-url https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple

      - name: Install system dependencies
        run: |
          apt-get update -y
          apt-get -y install git jq wget curl


      - name: Download dataset
        run: |
          mkdir -p datasets
            if [ ! -d /root/.cache/datasets/sharegpt ]; then
                mkdir -p /root/.cache/datasets/sharegpt
            fi

            if [ ! -f /root/.cache/datasets/sharegpt/ShareGPT_V3_unfiltered_cleaned_split.json ]; then
                cd /root/.cache/datasets/sharegpt
                # rm -rf ShareGPT_V3_unfiltered_cleaned_split.json
                wget https://hf-mirror.com/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json
            fi
      
      - name: Prepare dataset
        working-directory: ./vllm-benchmarks
        run: |
          cp /root/.cache/datasets/sharegpt/ShareGPT_V3_unfiltered_cleaned_split.json ./benchmarks

    #   - name: Run benchmark iteration
    #     working-directory: ./vllm-ascend
    #     run: |
    #       while IFS= read -r line || [[ -n "$line" ]]; do
    #         commit_id=$(awk '{print $1}' <<< "$line")
    #         commit_title=$(awk '{$1=""; print substr($0, 2)}' <<< "$line")
    #         commit_time=$(git show -s --format=%cd $commit_hash --date=iso-strict)
    #         commit_time_no_tz=$(echo "$commit_time" | sed 's/[+-][0-9][0-9]:[0-9][0-9]$//')
    #         cur_tag=$(git branch --show-current)

    #         git checkout $commit_id
    #         pip install -e .
    #         echo "------------------------"
    #         echo "commit_id: $commit_id"
    #         echo "commit_title: $commit_title"
    #         echo "commit_time: $commit_time_no_tz"
    #         echo "cur_tag: $cur_tag"
    #         echo "------------------------"
    #         # for now we use current repo script to have a test
    #         cd ../vllm-benchmarks
    #         bash benchmarks/scripts/run-performance-benchmarks.sh
    #         # send the result to es
    #         escli add --tag $cur_tag \
    #         --commit_id $commit_id \
    #         --commit_title "$commit_title" \
    #         --created_at "$commit_time_no_tz"
    #         --res_dir ./benchmarks/results 
    #         cd -
    #       done < commit_log.txt
